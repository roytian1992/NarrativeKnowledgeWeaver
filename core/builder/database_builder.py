# kag/builder/database_builder.py

import os
import json
import sqlite3
import asyncio
import time
from typing import List, Dict, Any, Tuple
import pandas as pd
from tqdm import tqdm

from core.utils.config import KAGConfig
from core.model_providers.openai_llm import OpenAILLM
from core.utils.prompt_loader import PromptLoader
from core.agent.cmp_extraction_agent import CMPExtractionAgent


def clean_df(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # ç»Ÿä¸€åŽ»ç©ºæ ¼ï¼ˆåªå¤„ç†ä¼šç”¨åˆ°çš„åˆ—ï¼‰
    norm_cols = ['åç§°','ç±»åˆ«','ç›¸å…³è§’è‰²','chunk_id','åœºæ¬¡','å­ç±»åˆ«','å¤–è§‚','çŠ¶æ€','æ–‡ä¸­çº¿ç´¢','è¡¥å……ä¿¡æ¯','å­åœºæ¬¡']
    for c in norm_cols:
        if c in df.columns:
            df[c] = df[c].astype('string').str.strip()

    # 1) åˆ é™¤åç§°ç¼ºå¤±/ç©ºç™½çš„è¡Œ
    df = df[ df['åç§°'].notna() & (df['åç§°'] != '') ]

    # 2) ç±»åˆ«=propitem -> propï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
    if 'ç±»åˆ«' in df.columns:
        df.loc[df['ç±»åˆ«'].str.lower() == 'propitem', 'ç±»åˆ«'] = 'prop'

    # 3) æŒ‰å…³é”®åˆ—åŽ»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¡ï¼‰
    keys = [c for c in ['åç§°','ç±»åˆ«','ç›¸å…³è§’è‰²','chunk_id','åœºæ¬¡'] if c in df.columns]
    df = df.drop_duplicates(subset=keys, keep='first').reset_index(drop=True)

    return df



class RelationalDatabaseBuilder:
    """
    è¯»å– all_document_chunks.json -> æœåŒ–é“æŠ½å– -> å†™å…¥ SQLite
    æ”¯æŒå¤±è´¥ç®¡ç† + å¤šè½®é‡è¯•ï¼ˆé»˜è®¤2è½®ï¼šé¦–è½®+1æ¬¡é‡è¯•ï¼‰ã€‚
    """

    def __init__(self, config: KAGConfig, max_retries: int = 2):
        self.config = config
        self.llm = OpenAILLM(config)
        self.max_retries = max_retries  # å¯é…ç½®æœ€å¤§å°è¯•æ¬¡æ•°ï¼ŒåŒ…å«é¦–è½®
        self.prompt_loader = PromptLoader(self.config.knowledge_graph_builder.prompt_dir)
        self.system_prompt = self._init_system_prompt()
        self.agent = CMPExtractionAgent(config, self.llm, self.system_prompt)

    # ---------------- system prompt ----------------
    def _init_system_prompt(self) -> str:
        base = self.config.storage.graph_schema_path
        settings_path = os.path.join(base, "settings.json")
        if os.path.exists(settings_path):
            settings = json.load(open(settings_path, "r", encoding="utf-8"))
        elif os.path.exists(self.config.probing.default_background_path):
            settings = json.load(open(self.config.probing.default_background_path, "r", encoding="utf-8"))
        else:
            settings = {"background": "", "abbreviations": []}

        background_info = self.get_background_info(
            background=settings.get("background", ""),
            abbreviations=settings.get("abbreviations", []),
        )
        # doc_type = self.config.knowledge_graph_builder.doc_type
        system_prompt_id = "agent_prompt_cmp" 
        return self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})

    def get_background_info(self, background: str, abbreviations: List[dict]) -> str:
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" if background else ""

        def fmt(item: dict) -> str:
            if not isinstance(item, dict):
                return ""
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )
            parts = [v.strip() for k, v in item.items() if k not in ("abbr", "full") and isinstance(v, str) and v.strip()]
            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(x) for x in abbreviations if isinstance(x, dict))
        return f"{bg_block}\n{abbr_block}" if (background and abbr_block) else (bg_block or abbr_block)

    # ---------------- æœåŒ–é“æŠ½å– ----------------
    def _rows_from_result(self, chunk: Dict[str, Any], merged_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        md = chunk.get("metadata", {}) or {}
        rows = []
        for r in merged_results:
            rows.append({
                "name": r.get("name", ""),
                "category": r.get("category", ""),
                "subcategory": r.get("subcategory", ""),
                "appearance": r.get("appearance", ""),
                "status": r.get("status", ""),
                "character": r.get("character", ""),
                "evidence": r.get("evidence", ""),
                "notes": r.get("notes", ""),
                "chunk_id": chunk.get("id", ""),
                "title": md.get("title", ""),
                "subtitle": md.get("subtitle", ""),
                "scene_id": md.get("scene_id", "")
            })
        return rows

    async def _extract_chunk(self, chunk: Dict[str, Any]) -> Dict[str, Any]:
        content = (chunk.get("content") or "").strip()
        if not content:
            return {"chunk": chunk, "rows": [], "error": None}
        try:
            result = await self.agent.arun(content, timeout=self.config.agent.async_timeout)
            merged = result.get("results", []) if isinstance(result, dict) else []
            return {"chunk": chunk, "rows": self._rows_from_result(chunk, merged), "error": None}
        except Exception as e:
            return {"chunk": chunk, "rows": [], "error": f"{e.__class__.__name__}: {e}"}

    async def _gather_once(self, chunks: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        sem = asyncio.Semaphore(self.config.knowledge_graph_builder.max_workers)

        async def _guarded(ch):
            async with sem:
                return await self._extract_chunk(ch)

        rows, failures = [], []
        for coro in tqdm(asyncio.as_completed([_guarded(ch) for ch in chunks]), total=len(chunks), desc="æœåŒ–é“æŠ½å–ä¸­"):
            res = await coro
            if res["error"]:
                failures.append({"chunk": res["chunk"], "error": res["error"]})
            else:
                rows.extend(res["rows"])
        return rows, failures

    async def _gather_with_retries(self, chunks: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        all_rows, failures = [], []
        current_chunks = chunks

        for attempt in range(1, self.max_retries + 1):
            rows, failures = await self._gather_once(current_chunks)
            all_rows.extend(rows)
            if not failures:
                break
            if attempt < self.max_retries:
                backoff = getattr(self.config.agent, "async_backoff_seconds", 2)
                print(f"ðŸ”„ ç¬¬ {attempt} è½®åŽä»æœ‰ {len(failures)} ä¸ªå¤±è´¥ï¼Œç­‰å¾… {backoff}s åŽé‡è¯•...")
                await asyncio.sleep(backoff)
                current_chunks = [f["chunk"] for f in failures]

        return all_rows, failures

    # ---------------- ä¸»æµç¨‹ ----------------
    def extract_cmp_information(self):
        base = self.config.storage.knowledge_graph_path
        input_json_path = os.path.join(base, "all_document_chunks.json")

        with open(input_json_path, "r", encoding="utf-8") as fr:
            chunks = json.load(fr)

        all_rows, still_failed = asyncio.run(self._gather_with_retries(chunks))
        os.makedirs(self.config.storage.sql_database_path, exist_ok=True)
        with open(os.path.join(self.config.storage.sql_database_path, "extraction_results.json"), "w") as f:
            json.dump(all_rows, f, ensure_ascii=False, indent=2)

        print(f"âœ… æœåŒ–é“æŠ½å–å®Œæˆ: æˆåŠŸ {len(all_rows)} è¡Œ, å¤±è´¥ {len(still_failed)} è¡Œ (æœ€å¤§å°è¯• {self.max_retries} è½®)")

    def build_relational_database(self):
        with open(os.path.join(self.config.storage.sql_database_path, "extraction_results.json"), "r") as f:
            all_rows = json.load(f)
        df_cmp = pd.DataFrame(all_rows)

        # scene_id_list = []
        # for i in range(df_cmp.shape[0]):
        #     row = df_cmp.iloc[0]
        #     if row["subtitle"]:
        #         scene_id = row["subtitle"].split("ã€")[0]
        #     else:
        #         scene_id = row["title"].split("ã€")[0]
        #     scene_id_list.append(scene_id)
        # df_cmp["scene_id"] = scene_id_list
        
        df_cmp = df_cmp.rename(columns={
            "name": "åç§°",
            "category": "ç±»åˆ«",
            "subcategory": "å­ç±»åˆ«",
            "appearance": "å¤–è§‚",
            "status": "çŠ¶æ€",
            "character": "ç›¸å…³è§’è‰²",
            "evidence": "æ–‡ä¸­çº¿ç´¢",
            "chunk_id": "chunk_id",
            "scene_id": "åœºæ¬¡",
            "title": "åœºæ¬¡å",
            "subtitle": "å­åœºæ¬¡å",
            "notes": "è¡¥å……ä¿¡æ¯"
        })

        db_path = os.path.join(self.config.storage.sql_database_path, "CMP.db")
        os.makedirs(self.config.storage.sql_database_path, exist_ok=True)
        if os.path.exists(db_path):
            os.remove(db_path)
        conn = sqlite3.connect(db_path)
        df_cmp = clean_df(df_cmp)
        df_cmp.to_sql("CMP_info", conn, if_exists="replace", index=False)
        conn.commit()
        print(f"âœ… æž„å»ºSQLæ•°æ®åº“æˆåŠŸ: {db_path}")
        df_cmp.to_csv(os.path.join(self.config.storage.sql_database_path, "CMP_info.csv"), index=False)
        conn.close()

    def build_scene_info(self):
        db_path = os.path.join(self.config.storage.sql_database_path, "CMP.db")
        os.makedirs(self.config.storage.sql_database_path, exist_ok=True)
        if os.path.exists(db_path):
            os.remove(db_path)
        conn = sqlite3.connect(db_path)

        rows = []
        base = self.config.storage.knowledge_graph_path
        input_json_path = os.path.join(base, "all_document_chunks.json")
        with open(input_json_path, "r", encoding="utf-8") as fr:
            chunks = json.load(fr)

        for chunk in chunks:
            metadata = chunk.get("metadata", {})
            rows.append({
                "chunk_id": chunk.get("id"),
                "scene_id": metadata.get("scene_id", metadata.get("title").split("ã€")[0]),
                "title": metadata.get("title"),
                "subtitle": metadata.get("subtitle")
            })

        df_scene = pd.DataFrame(rows)
        df_scene = df_scene.rename(columns={
            "chunk_id": "chunk_id",
            "scene_id": "åœºæ¬¡",
            "title": "åœºæ¬¡å",
            "subtitle": "å­åœºæ¬¡å",
        }).drop_duplicates()
        df_scene.to_sql("Scene_info", conn, if_exists="replace", index=False)
        conn.commit()
        df_scene.to_csv(os.path.join(self.config.storage.sql_database_path, "Scene_info.csv"), index=False)
        conn.close()
