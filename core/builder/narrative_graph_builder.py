"""
äº‹ä»¶å› æœå›¾æ„å»ºå™¨
è´Ÿè´£æ„å»ºäº‹ä»¶å› æœå…³ç³»çš„æœ‰å‘å¸¦æƒå›¾å’Œæƒ…èŠ‚å•å…ƒå›¾è°±
æ”¯æŒï¼šæ¯ä¸€æ­¥å•ç‹¬è¿è¡Œã€æ–­ç‚¹ç»­è·‘ã€EPGè·¯å¾„æŒä¹…åŒ–
"""

import json
import pickle
import networkx as nx
import hashlib
from typing import List, Dict, Tuple, Optional, Any, Set
from tqdm import tqdm
import time
from collections import Counter
from pathlib import Path
from core.model_providers.openai_llm import OpenAILLM
from core.utils.neo4j_utils import Neo4jUtils
from core.models.data import Entity
from core.builder.manager.graph_manager import GraphManager
from core.storage.graph_store import GraphStore
from core.storage.vector_store import VectorStore
from core.utils.prompt_loader import PromptLoader
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError, wait, FIRST_COMPLETED
from core.utils.format import correct_json_format, format_event_card
import logging
from collections import defaultdict
import os
from core.builder.graph_builder import DOC_TYPE_META


# -----------------------------
# å·¥å…·å‡½æ•°ï¼šé“¾å»é‡/ç›¸ä¼¼åº¦/é«˜é¢‘å­é“¾
# -----------------------------
def remove_subset_paths(chains: List[List[str]]) -> List[List[str]]:
    """
    åˆ é™¤æ‰€æœ‰äº‹ä»¶é›†åˆæ˜¯å…¶ä»–é“¾äº‹ä»¶é›†åˆå­é›†çš„é“¾ï¼ˆå¿½ç•¥é¡ºåºã€è¿ç»­æ€§ï¼‰
    """
    filtered = []
    for i, chain in enumerate(chains):
        set_chain = set(chain)
        remove = False
        for j, other in enumerate(chains):
            if i == j:
                continue
            if set_chain.issubset(set(other)) and len(set_chain) < len(set(other)):
                remove = True
                break
        if not remove:
            filtered.append(chain)
    return filtered


def jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:
    """è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ Jaccard ç›¸ä¼¼åº¦ï¼ˆ|Aâˆ©B| / max(|A|, |B|)ï¼‰"""
    if not set1 and not set2:
        return 1.0
    return len(set1 & set2) / max([len(set1), len(set2)])


def remove_similar_paths(chains: List[List[str]], threshold: float = 0.8) -> List[List[str]]:
    """
    åˆ é™¤ä¸å·²ä¿ç•™é“¾ Jaccard ç›¸ä¼¼åº¦ >= threshold çš„é“¾
    """
    filtered: List[List[str]] = []
    for chain in chains:
        set_chain = set(chain)
        keep = True
        for kept in filtered:
            sim = jaccard_similarity(set_chain, set(kept))
            if sim >= threshold:
                keep = False
                break
        if keep:
            filtered.append(chain)
    return filtered


def get_frequent_subchains(chains: List[List[str]], min_length: int = 2, min_count: int = 2):
    """
    ç»Ÿè®¡äº‹ä»¶é“¾ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„è¿ç»­å­é“¾
    Args:
        chains: äº‹ä»¶é“¾åˆ—è¡¨
        min_length: æœ€çŸ­å­é“¾é•¿åº¦
        min_count: æœ€å°‘å‡ºç°æ¬¡æ•°ï¼ˆé¢‘ç‡é˜ˆå€¼ï¼‰
    Returns:
        List[List[str]]  å­é“¾åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡ä¸é•¿åº¦é™åºï¼‰
    """
    counter = Counter()
    for chain in chains:
        n = len(chain)
        # æšä¸¾æ‰€æœ‰è¿ç»­å­é“¾
        for i in range(n):
            for j in range(i + min_length, n + 1):
                sub = tuple(chain[i:j])
                counter[sub] += 1
    # è¿‡æ»¤ä½é¢‘
    results = [(list(sub), cnt) for sub, cnt in counter.items() if cnt >= min_count]
    # æŒ‰é¢‘ç‡ã€é•¿åº¦æ’åº
    results.sort(key=lambda x: (-x[1], -len(x[0]), x[0]))
    return [pair[0] for pair in results]


# -----------------------------
# ä¸»ç±»ï¼šEventCausalityBuilder
# -----------------------------
class EventCausalityBuilder:
    """
    äº‹ä»¶å› æœå›¾æ„å»ºå™¨

    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä»Neo4jåŠ è½½å’Œæ’åºäº‹ä»¶
    2. é€šè¿‡è¿é€šä½“å’Œç¤¾åŒºè¿‡æ»¤äº‹ä»¶å¯¹
    3. ä½¿ç”¨extractoræ£€æŸ¥å› æœå…³ç³»
    4. æ„å»ºæœ‰å‘å¸¦æƒNetworkXå›¾
    5. ä¿å­˜å’ŒåŠ è½½å›¾æ•°æ®ï¼ˆå…¨éƒ¨å†™å…¥ EPG è·¯å¾„ï¼‰
    6. æ„å»ºPlotæƒ…èŠ‚å•å…ƒå›¾è°±
    """

    def __init__(self, config):
        """
        åˆå§‹åŒ–äº‹ä»¶å› æœå›¾æ„å»ºå™¨

        Args:
            config: KAGé…ç½®å¯¹è±¡
        """
        self.config = config
        self.llm = OpenAILLM(config)
        self.graph_store = GraphStore(config)
        self.vector_store = VectorStore(config, "documents")
        self.event_fallback = []  # å¯ä»¥åŠ å…¥Goalå’ŒAction

        self.doc_type = config.knowledge_graph_builder.doc_type
        if self.doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {self.doc_type}")
        self.meta = DOC_TYPE_META[self.doc_type]

        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, self.doc_type)
        self.neo4j_utils.load_embedding_model(config.graph_embedding)

        # åˆå§‹åŒ–Plotç›¸å…³ç»„ä»¶
        prompt_dir = config.knowledge_graph_builder.prompt_dir
        self.prompt_loader = PromptLoader(prompt_dir)
        settings_path = os.path.join(self.config.storage.graph_schema_path, "settings.json")
        if not os.path.exists(settings_path):
            settings_path = self.config.probing.default_background_path

        settings = json.load(open(settings_path, "r", encoding="utf-8"))

        self.system_prompt_text = self.construct_system_prompt(
            background=settings.get("background"),
            abbreviations=settings.get("abbreviations", [])
        )

        self.graph_analyzer = GraphManager(config, self.llm)

        # Plotæ„å»ºé…ç½®å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
        self.causality_threshold = "Medium"
        self.logger = logging.getLogger(__name__)
        self.sorted_scenes = []
        self.event_list = []
        self.event2section_map = {}
        self.max_depth = config.event_plot_graph_builder.max_depth
        self.check_weakly_connected_components = True
        self.min_component_size = config.event_plot_graph_builder.min_connected_component_size
        self.max_workers = config.event_plot_graph_builder.max_workers
        self.max_iteration = config.event_plot_graph_builder.max_iterations
        self.check_weakly_connected_components = config.event_plot_graph_builder.check_weakly_connected_components
        self.max_num_triangles = config.event_plot_graph_builder.max_num_triangles

        # å› æœå…³ç³»å¼ºåº¦åˆ°æƒé‡çš„æ˜ å°„
        self.event_cards: Dict[str, Dict[str, Any]] = {}

        self.logger.info("EventCausalityBuilderåˆå§‹åŒ–å®Œæˆ")

    # -----------------------------
    # Prompt æ„å»º
    # -----------------------------
    def construct_system_prompt(self, background, abbreviations):
        background_info = self.get_background_info(background, abbreviations)
        if self.doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel"
        system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})
        return system_prompt_text

    def get_background_info(self, background, abbreviations):
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" if background else ""

        def fmt(item: dict) -> str:
            if not isinstance(item, dict):
                return ""
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())
            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(item) for item in abbreviations if isinstance(item, dict))
        if background and abbr_block:
            background_info = f"{bg_block}\n{abbr_block}"
        else:
            background_info = bg_block or abbr_block
        return background_info

    # -----------------------------
    # äº‹ä»¶åˆ—è¡¨æ„å»º
    # -----------------------------
    def build_event_list(self) -> List[Entity]:
        """
        æ„å»ºæ’åºåçš„äº‹ä»¶åˆ—è¡¨
        Returns:
            æ’åºåçš„äº‹ä»¶åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æ„å»ºäº‹ä»¶åˆ—è¡¨...")

        # 1. è·å–æ‰€æœ‰ section å¹¶æ’åº
        section_entities = self.neo4j_utils.search_entities_by_type(
            entity_type=self.meta["section_label"]
        )
        self.sorted_sections = sorted(
            section_entities,
            key=lambda e: int(e.properties.get("order", 99999))
        )
        print(f"âœ… æ‰¾åˆ° {len(self.sorted_sections)} ä¸ªsection")

        # 2. ä»åœºæ™¯ä¸­æå–äº‹ä»¶
        event_list = []
        event2section_map = {}
        for section in tqdm(self.sorted_sections, desc="æå–åœºæ™¯ä¸­çš„äº‹ä»¶"):
            results = self.neo4j_utils.search_related_entities(
                source_id=section.id,
                predicate=self.meta["contains_pred"],
                entity_types=["Event"],
                return_relations=False
            )
            # fallbackï¼ˆå¯é€‰ï¼‰
            if not results and self.event_fallback:
                results = self.neo4j_utils.search_related_entities(
                    source_id=section.id,
                    relation_type=self.meta["contains_pred"],
                    entity_types=self.event_fallback,
                    return_relations=False
                )
            for result in results:
                if result.id not in event2section_map:
                    event2section_map[result.id] = section.id
                    event_list.append(result)

        self.event_list = event_list
        self.event2section_map = event2section_map

        print(f"âœ… æ„å»ºå®Œæˆï¼Œå…±æ‰¾åˆ° {len(event_list)} ä¸ªäº‹ä»¶")
        return event_list

    # -----------------------------
    # äº‹ä»¶å¡ç‰‡å¹¶å‘é¢„ç”Ÿæˆï¼ˆæ”¯æŒEPGè·¯å¾„ç¼“å­˜ï¼‰
    # -----------------------------
    def precompute_event_cards(
        self,
        events: List[Entity],
        per_task_timeout: float = 180,
        max_retries: int = 3,
        retry_timeout: float = 60.0,
    ) -> Dict[str, Dict[str, Any]]:
        """
        å¹¶å‘ä¸ºæ‰€æœ‰äº‹ä»¶ç”Ÿæˆ event_cardï¼š
        - å…ˆè¯» EPG è·¯å¾„ä¸‹çš„ event_cards.jsonï¼ˆå¦‚å­˜åœ¨ï¼‰
        - åªä¸ºç¼ºå¤±çš„äº‹ä»¶è¡¥é½å¡ç‰‡
        - ç»“æŸåå†™å› EPG è·¯å¾„
        äº§ç‰©ï¼ševent_cards.json
        """
        import time
        from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED

        # â€”â€” EPG è·¯å¾„
        base = self.config.storage.event_plot_graph_path
        os.makedirs(base, exist_ok=True)
        cache_path = os.path.join(base, "event_cards.json")

        # â€”â€” è¯»å–å·²å­˜åœ¨çš„ç¼“å­˜
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    self.event_cards = json.load(f)
                    if not isinstance(self.event_cards, dict):
                        self.event_cards = {}
            except Exception:
                self.event_cards = {}
        else:
            self.event_cards = {}

        # â€”â€” åªå¤„ç†ç¼ºå¤±çš„äº‹ä»¶
        existing = set(self.event_cards.keys())
        pending_events = [e for e in events if e.id not in existing]
        if not pending_events:
            print(f"ğŸ—‚ï¸ äº‹ä»¶å¡ç‰‡å·²å­˜åœ¨ï¼š{len(self.event_cards)} ä¸ªï¼Œè·³è¿‡ç”Ÿæˆã€‚")
            return self.event_cards

        def _collect_related_context_by_section(ev: Entity) -> str:
            ctx_set = set()
            sec_id = self.event2section_map.get(ev.id)
            if sec_id:
                sec = self.neo4j_utils.get_entity_by_id(sec_id)
                titles = sec.properties.get(self.meta['title'], [])
                if isinstance(titles, str):
                    titles = [titles]
                for t in titles or []:
                    try:
                        docs = self.vector_store.search_by_metadata({"title": t})
                        for d in docs:
                            if getattr(d, "content", None):
                                ctx_set.add(d.content)
                    except Exception:
                        pass
            if not ctx_set:
                try:
                    node = self.neo4j_utils.get_entity_by_id(ev.id)
                    chunk_ids = set((node.source_chunks or [])[:50])
                    if chunk_ids:
                        docs = self.vector_store.search_by_ids(list(chunk_ids))
                        for d in docs:
                            if getattr(d, "content", None):
                                ctx_set.add(d.content)
                except Exception:
                    pass
            return "\n".join(ctx_set)

        def _build_one(ev: Entity):
            info = self.neo4j_utils.get_entity_info(ev.id, "äº‹ä»¶", True, True)
            related_ctx = _collect_related_context_by_section(ev)
            out = self.graph_analyzer.generate_event_context(info, related_ctx)
            card = json.loads(correct_json_format(out))["event_card"]
            card = format_event_card(card)
            return ev.id, card

        def _run_batch(evts: List[Entity], timeout: float, allow_placeholder: bool, desc: str):
            results, failed = {}, set()
            executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="card")
            try:
                fut_info = {}
                for ev in evts:
                    f = executor.submit(_build_one, ev)
                    fut_info[f] = {"start": time.monotonic(), "event": ev}

                pbar = tqdm(total=len(fut_info), desc=desc, ncols=100)
                pending = set(fut_info.keys())

                while pending:
                    done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)

                    for f in done:
                        ev = fut_info[f]["event"]
                        try:
                            eid, card = f.result()
                            results[eid] = card
                        except Exception:
                            failed.add(ev.id)
                            if allow_placeholder:
                                skeleton = {
                                    "name": ev.properties.get("name") or ev.name or f"event_{ev.id}",
                                    "summary": "",
                                    "time_hint": "unknown",
                                    "locations": [],
                                    "participants": [],
                                    "action": "",
                                    "outcomes": [],
                                    "evidence": ""
                                }
                                results[ev.id] = json.dumps(skeleton, ensure_ascii=False)
                        pbar.update(1)
                        fut_info.pop(f, None)

                    now = time.monotonic()
                    to_forget = []
                    for f in list(pending):
                        start = fut_info[f]["start"]
                        if now - start >= timeout:
                            ev = fut_info[f]["event"]
                            f.cancel()
                            failed.add(ev.id)
                            if allow_placeholder:
                                skeleton = {
                                    "name": ev.properties.get("name") or ev.name or f"event_{ev.id}",
                                    "summary": "",
                                    "time_hint": "unknown",
                                    "locations": [],
                                    "participants": [],
                                    "action": "",
                                    "outcomes": [],
                                    "evidence": ""
                                }
                                results[ev.id] = json.dumps(skeleton, ensure_ascii=False)
                            pbar.update(1)
                            to_forget.append(f)

                    for f in to_forget:
                        pending.remove(f)
                        fut_info.pop(f, None)

                pbar.close()
            finally:
                executor.shutdown(wait=False, cancel_futures=True)

            return results, failed

        # â€” é¦–è½®ï¼šå…è®¸å ä½
        head_map, failed_ids = _run_batch(
            pending_events, timeout=per_task_timeout, allow_placeholder=True, desc="é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡ï¼ˆé¦–è½®ï¼‰"
        )
        for k, v in head_map.items():
            self.event_cards[k] = v

        # â€” é‡è¯•è½®ï¼šä»…å¤±è´¥é¡¹ï¼Œä¸å†å†™å ä½
        need_ids = list(failed_ids)
        for attempt in range(1, max_retries + 1):
            if not need_ids:
                break
            try:
                time.sleep(min(2 ** (attempt - 1), 5.0))
            except Exception:
                pass
            id2evt = {e.id: e for e in pending_events}
            retry_evts = [id2evt[i] for i in need_ids if i in id2evt]
            retry_map, retry_failed = _run_batch(
                retry_evts, timeout=retry_timeout, allow_placeholder=False, desc=f"é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡ï¼ˆé‡è¯• {attempt}/{max_retries}ï¼‰"
            )
            for k, v in retry_map.items():
                self.event_cards[k] = v
            need_ids = list(retry_failed)

        # â€”â€” å†™å›ç¼“å­˜ï¼ˆEPGï¼‰
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(self.event_cards, f, ensure_ascii=False, indent=2)

        print(f"ğŸ—‚ï¸ äº‹ä»¶å¡ç‰‡ç”Ÿæˆå®Œæˆï¼šæ€»è®¡ {len(self.event_cards)}ï¼Œæœ¬æ¬¡ç¼ºå£ä½™ {len(need_ids)}")
        return self.event_cards

    # -----------------------------
    # å€™é€‰äº‹ä»¶å¯¹è¿‡æ»¤
    # -----------------------------
    def filter_event_pairs_by_community(
        self,
        events: List[Entity],
        max_depth: int = 3
    ) -> List[Tuple[Entity, Entity]]:
        """
        åˆ©ç”¨ Neo4j ä¸­ Louvain ç»“æœç›´æ¥ç­›é€‰åŒç¤¾åŒºä¸” max_depth å†…å¯è¾¾çš„äº‹ä»¶å¯¹
        """
        id2entity = {e.id: e for e in events}
        pairs = self.neo4j_utils.fetch_event_pairs_same_community()
        filtered_pairs = []
        for row in pairs:
            src_id, dst_id = row["srcId"], row["dstId"]
            if src_id in id2entity and dst_id in id2entity:
                filtered_pairs.append((id2entity[src_id], id2entity[dst_id]))
        print(f"[âœ“] åŒç¤¾åŒºäº‹ä»¶å¯¹: {len(filtered_pairs)}")
        return filtered_pairs

    def sort_event_pairs_by_section_order(
        self, pairs: List[Tuple[Entity, Entity]]
    ) -> List[Tuple[Entity, Entity]]:
        def get_order(evt: Entity) -> int:
            sec_id = self.event2section_map.get(evt.id)
            if not sec_id:
                return 99999
            sec = self.neo4j_utils.get_entity_by_id(sec_id)
            return int(sec.properties.get("order", 99999))
        ordered = []
        for e1, e2 in pairs:
            ordered.append((e1, e2) if get_order(e1) <= get_order(e2) else (e2, e1))
        return ordered

    def filter_pair_by_distance_and_similarity(self, pairs):
        filtered_pairs = []
        for pair in tqdm(pairs, desc="ç­›é€‰èŠ‚ç‚¹å¯¹"):
            src_id, tgt_id = pair[0].id, pair[1].id
            reachable = self.neo4j_utils.check_nodes_reachable(
                src_id, tgt_id,
                excluded_rels=[self.meta["contains_pred"], "EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"],
                max_depth=self.max_depth
            )
            if reachable:
                filtered_pairs.append(pair)
            else:
                score = self.neo4j_utils.compute_semantic_similarity(src_id, tgt_id)
                if score >= 0.5:
                    filtered_pairs.append(pair)
        return filtered_pairs

    # -----------------------------
    # å› æœæ€§åˆ¤å®šï¼ˆå¹¶å‘ + æ–­ç‚¹ç»­è·‘ï¼‰
    # -----------------------------
    def check_causality_batch(
        self,
        pairs: List[Tuple[Entity, Entity]]
    ) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        å¹¶å‘æ£€æŸ¥äº‹ä»¶å¯¹çš„å› æœå…³ç³»ï¼ˆè½¯è¶…æ—¶ + å¤±è´¥æ”¶é›† + æœ«å°¾å¤šè½®é‡è¯•ï¼‰
        - ä¾èµ– self.event_cardsï¼ˆåœ¨ä¸»æµç¨‹é¢„ç”Ÿæˆï¼‰ï¼Œç¼ºå¤±é¡¹ä¼šå…œåº•å³æ—¶è¡¥å»ºä¸€æ¬¡
        - é¦–è½®ï¼šå®Œæˆå³æ”¶é›†ï¼›è¶…æ—¶/å¼‚å¸¸ -> å…ˆå ä½ + è®°å…¥é‡è¯•é˜Ÿåˆ—
        - æœ«å°¾ï¼šä»…å¯¹å¤±è´¥é¡¹åš N è½®é‡è¯•ï¼›æˆåŠŸå³è¦†ç›–æ—§ç»“æœ
        - è¿”å›ï¼š{(src_id, tgt_id): result_dict}
        """
        PER_TASK_TIMEOUT = 1800
        MAX_RETRIES = 2
        RETRY_BACKOFF = 2.0
        RETRY_TIMEOUT = 600

        def _make_result(src_event, tgt_event,
                         relation="NONE",
                         reason="",
                         temporal_order="Unknown",
                         confidence=0.0,
                         raw_result="",
                         timeout=False) -> Dict[str, Any]:
            res = {
                "src_event": src_event,
                "tgt_event": tgt_event,
                "relation": relation,
                "reason": reason,
                "temporal_order": temporal_order,
                "confidence": float(confidence) if confidence is not None else 0.0,
                "raw_result": raw_result
            }
            if timeout:
                res["causality_timeout"] = True
            return res

        def _get_common_neighbor_info(src_id, tgt_id):
            commons = self.neo4j_utils.get_common_neighbors(src_id, tgt_id, limit=50)
            info = "ä¸¤ä¸ªäº‹ä»¶å…·æœ‰çš„å…±åŒé‚»å±…çš„ä¿¡æ¯ä¸ºï¼š\n"
            if not commons:
                return info + "æ— "
            for ent_ in commons:
                try:
                    ent_type = "/".join(ent_.type) if isinstance(ent_.type, (list, set, tuple)) else str(ent_.type)
                except Exception:
                    ent_type = "Unknown"
                info += f"- å®ä½“åç§°ï¼š{ent_.name}ï¼Œå®ä½“ç±»å‹ï¼š{ent_type}ï¼Œç›¸å…³æè¿°ä¸ºï¼š{ent_.description}\n"
            return info

        def _ensure_card(e: Entity, info_text: str) -> Dict[str, Any]:
            if e.id in self.event_cards:
                return self.event_cards[e.id]
            out = self.graph_analyzer.generate_event_context(info_text, "")
            card = json.loads(correct_json_format(out))["event_card"]
            card = format_event_card(card)
            self.event_cards[e.id] = card
            return card

        def _process_pair(pair: Tuple[Entity, Entity]):
            src_event, tgt_event = pair
            pair_key = (src_event.id, tgt_event.id)
            try:
                info_1 = self.neo4j_utils.get_entity_info(
                    src_event.id, entity_type="äº‹ä»¶",
                    contain_properties=True, contain_relations=True
                )
                info_2 = self.neo4j_utils.get_entity_info(
                    tgt_event.id, entity_type="äº‹ä»¶",
                    contain_properties=True, contain_relations=True
                )
                related_context = info_1 + "\n" + info_2 + "\n" + _get_common_neighbor_info(src_event.id, tgt_event.id)
                src_event_card = _ensure_card(src_event, info_1)
                tgt_event_card = _ensure_card(tgt_event, info_2)

                result_json = self.graph_analyzer.check_event_causality(
                    src_event_card, tgt_event_card,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )
                if isinstance(result_json, dict):
                    result_dict = result_json
                    raw_str = json.dumps(result_json, ensure_ascii=False)
                else:
                    result_dict = json.loads(correct_json_format(result_json))
                    raw_str = result_json

                relation = result_dict.get("relation", "NONE")
                reason = result_dict.get("reason", "")
                temporal_order = result_dict.get("temporal_order", "Unknown")
                confidence = result_dict.get("confidence", 0.3)

                return pair_key, _make_result(
                    src_event, tgt_event,
                    relation=relation,
                    reason=reason,
                    temporal_order=temporal_order,
                    confidence=confidence,
                    raw_result=raw_str,
                    timeout=False
                )

            except Exception as e:
                return pair_key, _make_result(
                    src_event, tgt_event,
                    relation="NONE",
                    reason=f"æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}",
                    temporal_order="Unknown",
                    confidence=0.0,
                    raw_result="",
                    timeout=True
                )

        def _run_batch(pairs_to_run: List[Tuple[Entity, Entity]], per_task_timeout: float,
                       allow_placeholders: bool, desc: str):
            results_batch: Dict[Tuple[str, str], Dict[str, Any]] = {}
            failed_keys: set = set()
            executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="causal")
            try:
                fut_info: Dict[Any, Dict[str, Any]] = {}
                for pair in pairs_to_run:
                    f = executor.submit(_process_pair, pair)
                    fut_info[f] = {"start": time.monotonic(), "pair": pair}

                pbar = tqdm(total=len(fut_info), desc=desc, ncols=100)
                pending = set(fut_info.keys())

                while pending:
                    done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)

                    for f in done:
                        pair = fut_info[f]["pair"]
                        src_event, tgt_event = pair
                        key = (src_event.id, tgt_event.id)
                        try:
                            k2, res = f.result()
                            results_batch[k2] = res
                            if res.get("causality_timeout"):
                                failed_keys.add(k2)
                        except Exception as e:
                            res = _make_result(
                                src_event, tgt_event,
                                relation="NONE",
                                reason=f"ç»“æœæ”¶é›†å‡ºé”™: {e}",
                                temporal_order="Unknown",
                                confidence=0.0,
                                raw_result="",
                                timeout=True
                            )
                            results_batch[key] = res
                            failed_keys.add(key)
                        pbar.update(1)
                        fut_info.pop(f, None)

                    now = time.monotonic()
                    to_forget = []
                    for f in list(pending):
                        start = fut_info[f]["start"]
                        if now - start >= per_task_timeout:
                            pair = fut_info[f]["pair"]
                            src_event, tgt_event = pair
                            key = (src_event.id, tgt_event.id)
                            f.cancel()
                            if allow_placeholders:
                                res = _make_result(
                                    src_event, tgt_event,
                                    relation="NONE",
                                    reason="è½¯è¶…æ—¶ï¼Œå ä½è¿”å›",
                                    temporal_order="Unknown",
                                    confidence=0.0,
                                    raw_result="",
                                    timeout=True
                                )
                                results_batch[key] = res
                            failed_keys.add(key)
                            pbar.update(1)
                            to_forget.append(f)

                    for f in to_forget:
                        pending.remove(f)
                        fut_info.pop(f, None)

                pbar.close()
            finally:
                executor.shutdown(wait=False, cancel_futures=True)

            return results_batch, failed_keys

        print(f"ğŸ” å¼€å§‹å¹¶å‘æ£€æŸ¥ {len(pairs)} å¯¹äº‹ä»¶çš„å› æœå…³ç³»...")

        key2pair: Dict[Tuple[str, str], Tuple[Entity, Entity]] = {
            (src.id, tgt.id): (src, tgt) for (src, tgt) in pairs
        }

        head_results, failed_keys = _run_batch(
            pairs_to_run=pairs,
            per_task_timeout=PER_TASK_TIMEOUT,
            allow_placeholders=True,
            desc="å¹¶å‘æ£€æŸ¥å› æœå…³ç³»ï¼ˆé¦–è½®ï¼‰"
        )
        results: Dict[Tuple[str, str], Dict[str, Any]] = dict(head_results)

        def _needs_retry(key: Tuple[str, str], res: Dict[str, Any]) -> bool:
            if res.get("causality_timeout"):
                return True
            reason = (res.get("reason") or "").strip()
            return ("å‡ºé”™" in reason)

        needs_retry = [k for k in failed_keys if k in results and _needs_retry(k, results[k])]
        print(f"â© é¦–è½®åå‡†å¤‡é‡è¯•ï¼š{len(needs_retry)} / {len(pairs)}")

        for attempt in range(1, MAX_RETRIES + 1):
            if not needs_retry:
                break
            backoff = (RETRY_BACKOFF ** (attempt - 1))
            try:
                time.sleep(min(backoff, 5.0))
            except Exception:
                pass

            pairs_for_retry = [key2pair[k] for k in needs_retry if k in key2pair]
            batch_desc = f"å¹¶å‘æ£€æŸ¥å› æœå…³ç³»ï¼ˆé‡è¯•ç¬¬ {attempt}/{MAX_RETRIES} è½®ï¼‰"
            retry_results, retry_failed = _run_batch(
                pairs_to_run=pairs_for_retry,
                per_task_timeout=RETRY_TIMEOUT,
                allow_placeholders=False,
                desc=batch_desc
            )

            improved = 0
            for k, v in retry_results.items():
                if not _needs_retry(k, v):
                    results[k] = v
                    improved += 1
            print(f"ğŸ” é‡è¯•ç¬¬ {attempt} è½®ï¼šæˆåŠŸè¦†ç›– {improved} é¡¹ï¼Œä»éœ€é‡è¯• {len(retry_failed)} é¡¹")

            needs_retry = [k for k in retry_failed]

        print(f"âœ… å› æœå…³ç³»å¹¶å‘æ£€æŸ¥å®Œæˆï¼ˆæˆåŠŸ {len(results) - len(needs_retry)} / {len(pairs)}ï¼Œä»å¤±è´¥ {len(needs_retry)}ï¼‰")

        for k in needs_retry:
            if k in results:
                r = results[k]
                r["final_fallback"] = True
                r["retries"] = MAX_RETRIES

        return results

    # -----------------------------
    # åˆå§‹åŒ–ï¼šå­å›¾ + Louvain
    # -----------------------------
    def initialize(self):
        for relation_type in ["EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"]:
            self.neo4j_utils.delete_relation_type(relation_type)

        self.neo4j_utils.create_subgraph(
            graph_name="knowledge_graph",
            exclude_entity_types=[self.meta["section_label"]],
            exclude_relation_types=[self.meta["contains_pred"]],
            force_refresh=True
        )
        self.neo4j_utils.run_louvain(
            graph_name="knowledge_graph",
            write_property="community",
            force_run=True
        )

    # -----------------------------
    # ä¸»æµç¨‹1ï¼šæ„å»ºäº‹ä»¶å› æœå›¾ï¼ˆæŒä¹…åŒ–ï¼‰
    # -----------------------------
    def build_event_causality_graph(
        self,
        limit_events: Optional[int] = None
    ) -> None:
        """
        å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹ï¼ˆæ¯æ­¥ç»“æŸéƒ½æŠŠäº§ç‰©å†™å…¥ EPG è·¯å¾„ï¼‰
        äº§ç‰©ï¼š
          - event_cards.json
          - event_causality_results.pkl
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹...")

        base = self.config.storage.event_plot_graph_path
        os.makedirs(base, exist_ok=True)

        # 2. æ„å»ºäº‹ä»¶åˆ—è¡¨
        print("\nğŸ” æ„å»ºäº‹ä»¶åˆ—è¡¨...")
        event_list = self.build_event_list()

        # 3. é™åˆ¶äº‹ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if limit_events and limit_events < len(event_list):
            event_list = event_list[:limit_events]
            print(f"âš ï¸ é™åˆ¶å¤„ç†äº‹ä»¶æ•°é‡ä¸º: {limit_events}")

        # è¯»å–å·²æœ‰ event_cardsï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ç”Ÿæˆ
        event_cards_path = os.path.join(base, "event_cards.json")
        if os.path.exists(event_cards_path):
            try:
                with open(event_cards_path, "r", encoding="utf-8") as f:
                    self.event_cards = json.load(f)
                    if not isinstance(self.event_cards, dict):
                        self.event_cards = {}
                print(f"ğŸ—‚ï¸ å¤ç”¨å·²æœ‰äº‹ä»¶å¡ç‰‡ï¼š{len(self.event_cards)}")
            except Exception:
                self.event_cards = {}
        else:
            print("\nğŸ§© å¹¶å‘é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡...")
            self.precompute_event_cards(event_list)

        # 4. è¿‡æ»¤äº‹ä»¶å¯¹
        print("\nğŸ” è¿‡æ»¤äº‹ä»¶å¯¹...")
        filtered_pairs = self.filter_event_pairs_by_community(event_list, max_depth=self.max_depth)
        filtered_pairs = self.filter_pair_by_distance_and_similarity(filtered_pairs)
        filtered_pairs = self.sort_event_pairs_by_section_order(filtered_pairs)
        print("     æœ€ç»ˆå€™é€‰äº‹ä»¶å¯¹æ•°é‡ï¼š ", len(filtered_pairs))

        # 5. æ£€æŸ¥å› æœå…³ç³»ï¼ˆè¯»å– self.event_cardsï¼‰
        print("\nğŸ” æ£€æŸ¥å› æœå…³ç³»...")
        causality_results = self.check_causality_batch(filtered_pairs)

        # â€”â€” ä¿å­˜äº§ç‰©åˆ° EPG è·¯å¾„
        with open(os.path.join(base, "event_causality_results.pkl"), "wb") as f:
            pickle.dump(causality_results, f)
        with open(event_cards_path, "w", encoding="utf-8") as f:
            json.dump(self.event_cards, f, ensure_ascii=False, indent=2)

        # 6. å†™å› EVENT å…³ç³»
        print("\nğŸ”— å†™å›Eventé—´å…³ç³»...")
        self.write_event_cause_edges(causality_results)
        self.neo4j_utils.create_event_causality_graph("event_causality_graph", force_refresh=True)

    # -----------------------------
    # å†™å›äº‹ä»¶å› æœè¾¹
    # -----------------------------
    def write_event_cause_edges(self, causality_results):
        rows = []
        for (src_id, dst_id), res in causality_results.items():
            rel = (res.get("relation") or "").upper()
            if rel != "NONE":
                confidence = float(res.get("confidence", 0.3) or 0.0)
                rows.append({
                    "srcId": src_id,
                    "dstId": dst_id,
                    "confidence": confidence,
                    "reason": res.get("reason", ""),
                    "predicate": res.get("relation", "NONE")
                })
        self.neo4j_utils.write_event_causes(rows)

    # -----------------------------
    # SABERï¼šç»“æ„çº¦æŸçš„è¾¹ç²¾ç®€ï¼ˆæŒä¹…åŒ–æ¯è½®åˆ é™¤ï¼‰
    # -----------------------------
    def detect_flattened_causal_patterns(self, edges: List[Dict]) -> List[Dict]:
        forward_graph = defaultdict(set)
        edge_set = set()
        for edge in edges:
            sid = edge["sid"]
            tid = edge["tid"]
            forward_graph[sid].add(tid)
            edge_set.add((sid, tid))

        patterns = []
        for a, a_children in forward_graph.items():
            a_children = list(a_children)
            if len(a_children) < 2:
                continue
            internal_links = []
            for i in range(len(a_children)):
                for j in range(len(a_children)):
                    if i == j:
                        continue
                    u, v = a_children[i], a_children[j]
                    if (u, v) in edge_set:
                        internal_links.append((u, v))
            if internal_links:
                patterns.append({
                    "source": a,
                    "targets": a_children,
                    "internal_links": internal_links
                })
        return patterns

    def filter_weak_edges_in_patterns(
        self,
        patterns: List[Dict],
        edge_map: Dict[Tuple[str, str], Dict],
        conf_threshold: float = 0.5
    ) -> List[Dict]:
        cleaned_patterns = []
        for pat in patterns:
            src = pat["source"]
            targets = pat["targets"]
            internals = pat["internal_links"]

            new_targets = []
            for t in targets:
                info = edge_map.get((src, t))
                confidence = info.get("confidence", 0) if info else 0
                if not info:
                    continue
                if confidence < conf_threshold:
                    new_targets.append(t)

            new_internals = []
            for u, v in internals:
                info = edge_map.get((u, v))
                confidence = info.get("confidence", 0) if info else 0
                if not info:
                    continue
                if not confidence < conf_threshold:
                    new_internals.append((u, v))

            if len(new_targets) >= 2 and new_internals:
                cleaned_patterns.append({
                    "source": src,
                    "targets": new_targets,
                    "internal_links": new_internals
                })
        return cleaned_patterns

    def collect_removed_edges(self,
                              original_patterns: List[Dict],
                              filtered_patterns: List[Dict]
                              ) -> Set[Tuple[str, str]]:
        def extract_edges(patterns: List[Dict]) -> Set[Tuple[str, str]]:
            edge_set = set()
            for pat in patterns:
                src = pat["source"]
                for tgt in pat["targets"]:
                    edge_set.add((src, tgt))
                edge_set.update(pat["internal_links"])
            return edge_set

        origin_edges = extract_edges(original_patterns)
        filtered_edges = extract_edges(filtered_patterns)
        removed_edges = origin_edges - filtered_edges
        print(f"[+] Found {len(removed_edges)} candidate edges removed due to pattern collapse")
        return list(removed_edges)

    def filter_pattern(self, pattern, edge_map):
        source = pattern["source"]
        targets = pattern["targets"]
        internal_links = pattern["internal_links"]
        context_to_check = []
        for link in internal_links:
            mid_tgt_sim = self.neo4j_utils.compute_semantic_similarity(link[0], link[1])
            src_mid_sim = self.neo4j_utils.compute_semantic_similarity(source, link[0])
            src_tgt_sim = self.neo4j_utils.compute_semantic_similarity(source, link[1])

            mid_tgt_conf = edge_map.get((link[0], link[1]))["confidence"]
            src_mid_conf = edge_map.get((source, link[0]))["confidence"]
            src_tgt_conf = edge_map.get((source, link[1]))["confidence"]

            if (src_mid_sim > src_tgt_sim and mid_tgt_sim > src_tgt_sim) or (src_mid_conf > src_tgt_conf and mid_tgt_conf > src_tgt_conf):
                context_to_check.append({
                    "entities": [source, link[0], link[1]],
                    "details": [
                        {"edge": [source, link[0]], "similarity": src_mid_sim, "confidence": src_mid_conf},
                        {"edge": [source, link[1]], "similarity": src_tgt_sim, "confidence": src_tgt_conf},
                        {"edge": [link[0], link[1]], "similarity": mid_tgt_sim, "confidence": mid_tgt_conf},
                    ]
                })
        return context_to_check

    def prepare_context(self, pattern_detail):
        def _safe_str(x: Any) -> str:
            return x if isinstance(x, str) else ("" if x is None else str(x))

        event_details = self.neo4j_utils.get_event_details(pattern_detail["entities"])
        full_event_details = "ä¸‰ä¸ªäº‹ä»¶å®ä½“çš„æè¿°å¦‚ä¸‹ï¼š\n"

        for i, event_info in enumerate(event_details):
            event_id = event_info["event_id"]
            full_event_details += f"**äº‹ä»¶{i+1}çš„ç›¸å…³æè¿°å¦‚ä¸‹ï¼š**\näº‹ä»¶idï¼š{event_id}\n"

            background = self.neo4j_utils.get_entity_info(event_id, "äº‹ä»¶", True, True)
            background = _safe_str(background)

            props_raw = event_info.get("event_properties")
            if isinstance(props_raw, dict):
                event_props = props_raw
            elif isinstance(props_raw, str) and props_raw.strip():
                try:
                    event_props = json.loads(props_raw)
                    if not isinstance(event_props, dict):
                        event_props = {}
                except Exception:
                    event_props = {}
            else:
                event_props = {}

            non_empty_props = {k: v for k, v in event_props.items() if isinstance(v, str) and v.strip()}

            if non_empty_props:
                background += "\näº‹ä»¶çš„å±æ€§å¦‚ä¸‹ï¼š\n"
                for k, v in non_empty_props.items():
                    background += f"- {k}ï¼š{v}\n"

            if i + 1 != len(event_details):
                background += "\n"

            full_event_details += background

        full_relation_details = "å®ƒä»¬ä¹‹é—´å·²ç»å­˜åœ¨çš„å› æœå…³ç³»æœ‰ï¼š\n"
        relation_details = pattern_detail["details"]
        for i, relation_info in enumerate(relation_details):
            src, tgt = relation_info["edge"]
            rel_summary = self.neo4j_utils.get_relation_summary(src, tgt, "EVENT_CAUSES")
            rel_summary = _safe_str(rel_summary)
            background = f"{i+1}. " + rel_summary
            background += f"\nå…³ç³»çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºï¼š{round(relation_info['similarity'], 4)}ï¼Œç½®ä¿¡åº¦ä¸ºï¼š{relation_info['confidence']}ã€‚"
            if i + 1 != len(relation_details):
                background += "\n\n"
            full_relation_details += background

        return full_event_details, full_relation_details

    def run_SABER(self):
        """
        æ‰§è¡ŒåŸºäºç»“æ„+LLMçš„å› æœè¾¹ç²¾ç®€ä¼˜åŒ–è¿‡ç¨‹
        äº§ç‰©ï¼š
          - saber_removed_edges_round_{i}.jsonï¼ˆæ¯è½®ä¸€ä»½ï¼Œå†™å…¥ EPGï¼‰
        """
        loop_count = 0
        while True:
            print(f"\n===== [ç¬¬ {loop_count + 1} è½®ä¼˜åŒ–] =====")

            scc_components = self.neo4j_utils.fetch_scc_components("event_causality_graph", 2)
            wcc_components = []
            if self.check_weakly_connected_components:
                wcc_components = self.neo4j_utils.fetch_wcc_components("event_causality_graph", self.min_component_size)

            connected_components = scc_components + wcc_components
            print(f"ğŸ“Œ å½“å‰è¿é€šä½“æ•°é‡ï¼šSCC={len(scc_components)}ï¼ŒWCC={len(wcc_components)}")

            all_triangles = []
            edge_map_global = {}

            for cc in connected_components:
                node_map, edges = self.neo4j_utils.load_connected_components_subgraph(cc)
                edge_map = {
                    (e["sid"], e["tid"]): {"confidence": e.get("confidence", 0.0)}
                    for e in edges
                }
                edge_map_global.update(edge_map)

                old_patterns = self.detect_flattened_causal_patterns(edges)
                new_patterns = self.filter_weak_edges_in_patterns(old_patterns, edge_map, conf_threshold=0.5)
                for pattern in new_patterns:
                    all_triangles += self.filter_pattern(pattern, edge_map)

            print(f"ğŸ”º æœ¬è½®éœ€åˆ¤æ–­çš„ä¸‰å…ƒå› æœç»“æ„æ•°é‡ï¼š{len(all_triangles)}")
            if len(all_triangles) >= self.max_num_triangles:
                print(f"âš ï¸ æ£€æµ‹åˆ°ä¸‰å…ƒç»“æ„æ•°é‡è¿‡å¤šï¼Œåªé€‰æ‹©å‰{self.max_num_triangles}ä¸ªè¿›è¡Œå¤„ç†ã€‚")
                all_triangles = all_triangles[:self.max_num_triangles]
                return

            if loop_count >= 1:
                if len(scc_components) == 0 and len(all_triangles) == 0:
                    print("âœ… å›¾ç»“æ„å·²æ— å¼ºè¿é€šä½“ï¼Œä¸”æ— å¾…åˆ¤å®šä¸‰å…ƒç»“æ„ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
                    break
            elif loop_count >= self.max_iteration:
                break

            removed_edges = []

            def process_triangle(triangle_):
                try:
                    event_details, relation_details = self.prepare_context(triangle_)
                    chunks = [self.neo4j_utils.get_entity_by_id(ent_id).source_chunks[0] for ent_id in triangle_["entities"]]
                    chunks = list(set(chunks))
                    documents = self.vector_store.search_by_ids(chunks)
                    results = {doc.content for doc in documents}
                    related_context = "\n".join(list(results))

                    output = self.graph_analyzer.evaluate_event_redundancy(
                        event_details, relation_details, self.system_prompt_text, related_context
                    )
                    output = json.loads(correct_json_format(output))
                    if output.get("remove_edge", False):
                        return (triangle_["entities"][0], triangle_["entities"][2])
                except Exception as e:
                    print(f"[âš ï¸ é”™è¯¯] Triangle åˆ¤æ–­å¤±è´¥: {triangle_['entities']}, é”™è¯¯ä¿¡æ¯: {str(e)}")
                return None

            print(f"ğŸ§  æ­£åœ¨å¹¶å‘åˆ¤æ–­ä¸‰å…ƒç»“æ„...")
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(process_triangle, tri) for tri in all_triangles]
                for f in tqdm(as_completed(futures), total=len(futures), desc="LLMåˆ¤æ–­"):
                    res = f.result()
                    if res:
                        removed_edges.append(res)

            print(f"âŒ æœ¬è½®å¾…å®šç§»é™¤è¾¹æ•°é‡ï¼š{len(set(removed_edges))}")

            # â€”â€” åˆ é™¤è¾¹
            for edge in removed_edges:
                self.neo4j_utils.delete_relation_by_ids(edge[0], edge[1], "EVENT_CAUSES")

            # â€”â€” æŒä¹…åŒ–æœ¬è½®åˆ é™¤æ—¥å¿—ï¼ˆEPGï¼‰
            base = self.config.storage.event_plot_graph_path
            os.makedirs(base, exist_ok=True)
            saber_log_path = os.path.join(base, f"saber_removed_edges_round_{loop_count+1}.json")
            try:
                with open(saber_log_path, "w", encoding="utf-8") as f:
                    json.dump([list(e) for e in set(removed_edges)], f, ensure_ascii=False, indent=2)
            except Exception:
                pass

            # â€”â€” åˆ·æ–° GDS å›¾
            self.neo4j_utils.create_event_causality_graph("event_causality_graph", min_confidence=0, force_refresh=True)
            loop_count += 1

    # -----------------------------
    # å·¥å…·ï¼šæå–æ‰€æœ‰äº‹ä»¶é“¾ & æ–‡æœ¬ä¸Šä¸‹æ–‡
    # -----------------------------
    def get_all_event_chains(self, min_confidence: float = 0.0):
        """
        è·å–æ‰€æœ‰å¯èƒ½çš„äº‹ä»¶é“¾ï¼ˆä»èµ·ç‚¹åˆ°æ²¡æœ‰å‡ºè¾¹çš„ç»ˆç‚¹ï¼‰
        """
        starting_events = self.neo4j_utils.get_starting_events()
        chains = []
        for event in starting_events:
            all_chains = self.neo4j_utils.find_event_chain(event, min_confidence)
            chains.extend([chain for chain in all_chains if len(chain) >= 2])
        return chains

    def prepare_chain_context(self, chain):
        if len(chain) > 1:
            context = "äº‹ä»¶é“¾ï¼š" + "->".join(chain) + "\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        else:
            context = f"äº‹ä»¶ï¼š{chain[0]}" + "\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        for i, event in enumerate(chain):
            # è¿™é‡Œç›´æ¥ä½¿ç”¨ event_cardsï¼Œé¿å…å†æ¬¡æ‹¼è£…é•¿æ–‡æœ¬
            context += f"äº‹ä»¶{i+1}ï¼š{event}\n" + self.event_cards[event] + "\n"
        return context

    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_embedding_model(self.config.graph_embedding)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            entity_types=["Event", "Plot"]
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… äº‹ä»¶æƒ…èŠ‚å›¾å‘é‡æ„å»ºå®Œæˆ")

    # -----------------------------
    # ä¸»æµç¨‹2ï¼šæ„å»ºæƒ…èŠ‚-äº‹ä»¶å›¾ï¼ˆè¯»å†™EPGï¼‰
    # -----------------------------
    def build_event_plot_graph(self):
        """
        æ„å»ºæƒ…èŠ‚-äº‹ä»¶å›¾
        è¯»ï¼šEPG/event_cards.json
        å†™ï¼šEPG/filtered_event_chains.json
        """
        # æ¸…ç©ºæ—§çš„ Plot å›¾ä¸å…³ç³»ï¼ˆå·²é€‚é…æ–°å…­ç§ Plot å…³ç³» + HAS_EVENTï¼‰
        self.neo4j_utils.reset_event_plot_graph()

        base = self.config.storage.event_plot_graph_path
        os.makedirs(base, exist_ok=True)

        # è¯»å–äº‹ä»¶å¡ç‰‡ï¼ˆEPGï¼‰
        cards_path = os.path.join(base, "event_cards.json")
        with open(cards_path, "r", encoding="utf-8") as f:
            self.event_cards = json.load(f)

        all_chains = self.get_all_event_chains(min_confidence=0.0)
        print("[âœ“] å½“å‰äº‹ä»¶é“¾æ€»æ•°ï¼š", len(all_chains))

        filtered_chains = get_frequent_subchains(all_chains, 2, 1)
        filtered_chains = remove_subset_paths(filtered_chains)
        filtered_chains = remove_similar_paths(filtered_chains, 0.7)
        print("[âœ“] è¿‡æ»¤åäº‹ä»¶é“¾æ€»æ•°ï¼š", len(filtered_chains))

        # â€”â€” ä¿å­˜ç­›åé“¾æ¡åˆ° EPG
        with open(os.path.join(base, "filtered_event_chains.json"), "w", encoding="utf-8") as f:
            json.dump(filtered_chains, f, ensure_ascii=False, indent=2)

        def _stable_plot_id(title: str, chain: list[str]) -> str:
            key = f"{title}||{'->'.join(chain)}"
            return "plot_" + hashlib.sha1(key.encode("utf-8")).hexdigest()[:16]

        def _to_bool(v) -> bool:
            if isinstance(v, bool):
                return v
            if v is None:
                return False
            return str(v).strip().lower() in ("true", "yes", "1")

        def process_chain(chain):
            try:
                event_chain_info = self.prepare_chain_context(chain)

                chunk_ids = []
                for ent_id in chain:
                    ent = self.neo4j_utils.get_entity_by_id(ent_id)
                    if not ent:
                        continue
                    sc = ent.source_chunks or []
                    if sc:
                        chunk_ids.append(sc[0])
                chunk_ids = list(set(chunk_ids))

                related_context = ""
                if chunk_ids:
                    documents = self.vector_store.search_by_ids(chunk_ids)
                    contents = {getattr(doc, "content", "") for doc in documents if getattr(doc, "content", "")}
                    related_context = "\n".join(list(contents))

                raw = self.graph_analyzer.generate_event_plot(
                    event_chain_info=event_chain_info,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )
                result = json.loads(correct_json_format(raw))

                if not _to_bool(result.get("is_plot")):
                    return False

                plot_info = result.get("plot_info") or {}
                title = (plot_info.get("title") or "").strip()
                if not title:
                    title = f"æƒ…èŠ‚é“¾ï¼š{chain[0]}â†’{chain[-1]}"

                plot_info["id"] = _stable_plot_id(title, chain)
                plot_info["event_ids"] = chain
                plot_info["reason"] = result.get("reason", "")

                self.neo4j_utils.write_plot_to_neo4j(plot_data=plot_info)
                return True

            except Exception as e:
                print(f"[!] å¤„ç†äº‹ä»¶é“¾ {chain} æ—¶å‡ºé”™: {e}")
                return False

        success_count = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_chain, chain) for chain in filtered_chains]
            for future in tqdm(as_completed(futures), total=len(futures), desc="å¹¶å‘ç”Ÿæˆæƒ…èŠ‚å›¾è°±"):
                try:
                    if future.result():
                        success_count += 1
                except Exception as e:
                    print(f"[!] å­ä»»åŠ¡å¼‚å¸¸ï¼š{e}")

        print(f"[âœ“] æˆåŠŸç”Ÿæˆæƒ…èŠ‚æ•°é‡ï¼š{success_count}/{len(filtered_chains)}")
        return

    # -----------------------------
    # ä¸»æµç¨‹3ï¼šæŠ½å–æƒ…èŠ‚é—´å…³ç³»ï¼ˆå†™EPGï¼‰
    # -----------------------------
    def generate_plot_relations(self):
        """
        åŸºäºå€™é€‰æƒ…èŠ‚å¯¹ï¼Œåˆ¤å®šå¹¶å†™å…¥æƒ…èŠ‚é—´å…³ç³»ã€‚
        å…³ç³»é›†ï¼ˆæœ€ç»ˆç‰ˆï¼‰ï¼š
        - æœ‰å‘ï¼šPLOT_PREREQUISITE_FOR, PLOT_ADVANCES, PLOT_BLOCKS, PLOT_RESOLVES
        - æ— å‘ï¼šPLOT_CONFLICTS_WITH, PLOT_PARALLELS
        å…¼å®¹æ—§ç±»å‹ï¼šPLOT_CONTRIBUTES_TO / PLOT_SETS_UP â†’ ç»Ÿä¸€æ˜ å°„ä¸º PLOT_ADVANCES
        äº§ç‰©ï¼š
          - EPG/plot_relations_created.json
        """
        # é¢„å¤„ç†ï¼šå‘é‡ã€GDS å›¾ä¸åµŒå…¥
        self.neo4j_utils.process_all_embeddings(entity_types=[self.meta["section_label"]])
        self.neo4j_utils.create_event_plot_graph()
        self.neo4j_utils.run_node2vec()

        # å¬å›å€™é€‰æƒ…èŠ‚å¯¹
        all_plot_pairs = self.neo4j_utils.get_plot_pairs(threshold=0)
        print("[âœ“] å¾…åˆ¤å®šæƒ…èŠ‚å…³ç³»æ•°é‡ï¼š", len(all_plot_pairs))

        LEGACY_MAP = {
            "PLOT_CONTRIBUTES_TO": "PLOT_ADVANCES",
            "PLOT_SETS_UP": "PLOT_ADVANCES"
        }
        DIRECTED = {
            "PLOT_PREREQUISITE_FOR",
            "PLOT_ADVANCES",
            "PLOT_BLOCKS",
            "PLOT_RESOLVES",
        }
        UNDIRECTED = {
            "PLOT_CONFLICTS_WITH",
            "PLOT_PARALLELS",
        }
        VALID_TYPES = DIRECTED | UNDIRECTED | {"None", None}

        edges_to_add = []

        def _make_edge(src_id, tgt_id, rtype, confidence, reason):
            return {
                "src": src_id,
                "tgt": tgt_id,
                "relation_type": rtype,
                "confidence": float(confidence) if confidence is not None else 0.0,
                "reason": reason or ""
            }

        def _parse_direction_to_edge(pair, direction_str, rtype, confidence, reason):
            """
            å°† A/B æ–¹å‘æ˜ å°„ä¸ºçœŸå® src/tgt è¾¹ï¼›è¿”å› [edge] æˆ– []ã€‚
            direction_str: "A->B" / "B->A"
            """
            if direction_str == "A->B":
                return [_make_edge(pair["src"], pair["tgt"], rtype, confidence, reason)]
            elif direction_str == "B->A":
                return [_make_edge(pair["tgt"], pair["src"], rtype, confidence, reason)]
            else:
                print(f"[!] è·³è¿‡ï¼šæœ‰å‘å…³ç³»ç¼ºå°‘æœ‰æ•ˆæ–¹å‘ direction={direction_str} pair={pair}")
                return []

        def process_pair(pair):
            try:
                plot_A_info = self.neo4j_utils.get_entity_info(pair["src"], "æƒ…èŠ‚", contain_properties=True, contain_relations=True)
                plot_B_info = self.neo4j_utils.get_entity_info(pair["tgt"], "æƒ…èŠ‚", contain_properties=True, contain_relations=True)

                # è°ƒç”¨å…³ç³»åˆ¤å®šï¼ˆLLM/è§„åˆ™ï¼‰
                result = self.graph_analyzer.extract_plot_relation(plot_A_info, plot_B_info, self.system_prompt_text)

                # å°è¯•ä¿®æ­£/è§£æ JSON
                try:
                    result = json.loads(correct_json_format(result))
                except Exception:
                    if isinstance(result, dict):
                        pass
                    else:
                        raise

                # è¯»å–å­—æ®µ
                rtype = result.get("relation_type")
                direction = result.get("direction", None)  # æœ‰å‘æ—¶åº”ä¸º "A->B" / "B->A"ï¼Œæ— å‘æˆ– None ç”¨ null
                confidence = result.get("confidence", 0.0)
                reason = result.get("reason", "")

                # å…¼å®¹æ—§æšä¸¾
                if rtype in LEGACY_MAP:
                    rtype = LEGACY_MAP[rtype]

                # è¿‡æ»¤æ— æ•ˆç±»å‹
                if rtype not in VALID_TYPES:
                    print(f"[!] æœªçŸ¥ relation_type={rtype}ï¼Œè·³è¿‡ pair={pair}")
                    return []

                # None æˆ–æ— å…³ç³»
                if rtype in {"None", None}:
                    return []

                pair_edges = []

                # æœ‰å‘å…³ç³»
                if rtype in DIRECTED:
                    pair_edges.extend(_parse_direction_to_edge(pair, direction, rtype, confidence, reason))

                # æ— å‘å…³ç³»ï¼šå†™åŒå‘è¾¹
                elif rtype in UNDIRECTED:
                    pair_edges.append(_make_edge(pair["src"], pair["tgt"], rtype, confidence, reason))
                    pair_edges.append(_make_edge(pair["tgt"], pair["src"], rtype, confidence, reason))

                return pair_edges

            except Exception as e:
                print(f"[âš ] å¤„ç†æƒ…èŠ‚å¯¹ {pair} å‡ºé”™: {e}")
                return []

        # å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_pair, pair) for pair in all_plot_pairs]
            for future in tqdm(as_completed(futures), total=len(futures), desc="æŠ½å–æƒ…èŠ‚å…³ç³»"):
                try:
                    res = future.result()
                    if res:
                        edges_to_add.extend(res)
                except Exception as e:
                    print(f"[âš ] future ç»“æœå¤„ç†å‡ºé”™: {e}")

        # æ‰¹é‡å†™å…¥ Neo4j + EPG
        if edges_to_add:
            self.neo4j_utils.create_plot_relations(edges_to_add)
            print(f"[âœ“] å·²åˆ›å»ºæƒ…èŠ‚å…³ç³» {len(edges_to_add)} æ¡")

            base = self.config.storage.event_plot_graph_path
            os.makedirs(base, exist_ok=True)
            with open(os.path.join(base, "plot_relations_created.json"), "w", encoding="utf-8") as f:
                json.dump(edges_to_add, f, ensure_ascii=False, indent=2)
        else:
            print("[!] æ²¡æœ‰ç”Ÿæˆä»»ä½•æƒ…èŠ‚å…³ç³»")
