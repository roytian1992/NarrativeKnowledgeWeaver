import json
import pickle
import glob
import networkx as nx
import hashlib
from typing import List, Dict, Tuple, Optional, Any, Set
from tqdm import tqdm
import time
from collections import Counter
from pathlib import Path
from core.model_providers.openai_llm import OpenAILLM
from core.utils.neo4j_utils import Neo4jUtils
from core.models.data import Entity
from core.builder.manager.graph_manager import GraphManager
from core.storage.graph_store import GraphStore
from core.storage.vector_store import VectorStore
from core.utils.prompt_loader import PromptLoader
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError, wait, FIRST_COMPLETED
from core.utils.format import correct_json_format, format_event_card
import logging
from collections import defaultdict
import os
from core.builder.graph_builder import DOC_TYPE_META


def _run_with_soft_timeout_and_retries(
    self,
    items: List[Any],
    *,
    work_fn,                      # (item) -> result
    key_fn,                       # (item) -> hashable key
    desc_label: str,
    per_task_timeout: float = 600.0,
    retries: int = 2,
    retry_backoff: float = 1.0,
    allow_placeholder_first_round: bool = False,
    placeholder_fn=None,          # (item, exc=None) -> placeholder_result
    should_retry=None             # (result) -> bool  Trueè¡¨ç¤ºéœ€è¦è¿›å…¥ä¸‹ä¸€è½®
) -> Tuple[Dict[Any, Any], Set[Any]]:
    """
    é€šç”¨å¹¶å‘æ‰§è¡Œå™¨ï¼šé€è½®å¹¶å‘ + è½¯è¶…æ—¶ + å¤±è´¥/è¶…æ—¶ä»…é‡è¯•æœªæˆåŠŸé¡¹ã€‚
    è¿”å›ï¼š(results_map, still_failed_keys)
    """
    from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, TimeoutError as FuturesTimeoutError
    import time
    from tqdm import tqdm

    total = len(items)
    if total == 0:
        return {}, set()

    results: Dict[Any, Any] = {}
    remaining_items = items[:]  # æ¯è½®ä»…æäº¤æœªæˆåŠŸé¡¹
    still_failed_keys: Set[Any] = set()

    max_rounds = max(1, retries)
    for round_id in range(1, max_rounds + 1):
        if not remaining_items:
            break

        round_failures: Set[Any] = set()
        round_timeouts: Set[Any] = set()

        with ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix=f"concur-r{round_id}") as executor:
            fut_info: Dict[Any, Dict[str, Any]] = {}
            for it in remaining_items:
                f = executor.submit(work_fn, it)
                fut_info[f] = {"start": time.monotonic(), "item": it, "key": key_fn(it)}

            pbar = tqdm(total=len(fut_info), desc=f"{desc_label}ï¼ˆç¬¬{round_id}/{max_rounds}è½®ï¼‰", ncols=100)
            pending = set(fut_info.keys())

            while pending:
                done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)

                # æ”¶é›†å·²å®Œæˆ
                for f in done:
                    meta = fut_info.pop(f, None)
                    key = meta["key"]
                    item = meta["item"]
                    try:
                        res = f.result()
                        results[key] = res
                        # è‹¥å®šä¹‰äº† should_retryï¼Œå¹¶åˆ¤æ–­éœ€è¦é‡è¯•ï¼Œåˆ™åŠ å…¥å¤±è´¥é›†åˆ
                        if callable(should_retry) and should_retry(res):
                            round_failures.add(key)
                    except Exception as e:
                        # å¼‚å¸¸ï¼šé¦–è½®å¯å†™å ä½
                        if allow_placeholder_first_round and round_id == 1 and callable(placeholder_fn):
                            try:
                                results[key] = placeholder_fn(item, exc=e)
                            except Exception:
                                pass
                        round_failures.add(key)
                    pbar.update(1)

                # å¤„ç†è½¯è¶…æ—¶ï¼šå–æ¶ˆã€å¯å ä½ã€è®°å¤±è´¥
                now = time.monotonic()
                to_forget = []
                for f in list(pending):
                    meta = fut_info[f]
                    if now - meta["start"] >= per_task_timeout:
                        key = meta["key"]
                        item = meta["item"]
                        try:
                            f.cancel()
                        except Exception:
                            pass
                        if allow_placeholder_first_round and round_id == 1 and callable(placeholder_fn):
                            try:
                                results[key] = placeholder_fn(item, exc=FuturesTimeoutError(f"soft-timeout {per_task_timeout}s"))
                            except Exception:
                                pass
                        round_timeouts.add(key)
                        pbar.update(1)
                        to_forget.append(f)

                for f in to_forget:
                    pending.remove(f)
                    fut_info.pop(f, None)

            pbar.close()

        # ä¸‹ä¸€è½®ä»…é‡è¯•å½“è½®å¤±è´¥/è¶…æ—¶é¡¹
        keys_to_retry = round_failures | round_timeouts
        still_failed_keys |= keys_to_retry
        # å°† keys æ˜ å°„å› item
        key_set = set(keys_to_retry)
        remaining_items = [it for it in items if key_fn(it) in key_set]

        # è½®é—´é€€é¿
        if remaining_items and round_id < max_rounds and retry_backoff > 0:
            try:
                time.sleep(retry_backoff)
            except Exception:
                pass

    return results, still_failed_keys


# -----------------------------
# å·¥å…·å‡½æ•°ï¼šé“¾å»é‡/ç›¸ä¼¼åº¦/é«˜é¢‘å­é“¾
# -----------------------------
def remove_subset_paths(chains: List[List[str]]) -> List[List[str]]:
    """
    åˆ é™¤æ‰€æœ‰äº‹ä»¶é›†åˆæ˜¯å…¶ä»–é“¾äº‹ä»¶é›†åˆå­é›†çš„é“¾ï¼ˆå¿½ç•¥é¡ºåºã€è¿ç»­æ€§ï¼‰
    """
    filtered = []
    for i, chain in enumerate(chains):
        set_chain = set(chain)
        remove = False
        for j, other in enumerate(chains):
            if i == j:
                continue
            if set_chain.issubset(set(other)) and len(set_chain) < len(set(other)):
                remove = True
                break
        if not remove:
            filtered.append(chain)
    return filtered


def overlapping_similarity(set1: Set[str], set2: Set[str]) -> float:
    """è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ç›¸ä¼¼åº¦ï¼ˆ|Aâˆ©B| / min(|A|, |B|)ï¼‰"""
    if not set1 and not set2:
        return 1.0
    return len(set1 & set2) / min([len(set1), len(set2)])


def remove_similar_paths(chains: List[List[str]], threshold: float = 0.8) -> List[List[str]]:
    """
    åˆ é™¤ä¸å·²ä¿ç•™é“¾ç›¸ä¼¼åº¦ >= threshold çš„é“¾
    """
    filtered: List[List[str]] = []
    for chain in chains:
        set_chain = set(chain)
        keep = True
        for kept in filtered:
            sim = overlapping_similarity(set_chain, set(kept))
            if sim >= threshold:
                keep = False
                break
        if keep:
            filtered.append(chain)
    return filtered


from typing import List, Set, Tuple, Dict
from collections import defaultdict, Counter

def _maximal_chain_indices_by_set(chains: List[List[str]], min_length: int = 2) -> List[int]:
    """
    è¿”å›â€œäº‹ä»¶é›†åˆä¸¥æ ¼æå¤§â€çš„åŸå§‹é“¾ä¸‹æ ‡é›†åˆï¼ˆä»…è€ƒè™‘é•¿åº¦ >= min_length çš„é“¾ï¼‰ï¼Œ
    å³ï¼šset(chain_i) ä¸æ˜¯ä»»ä½• set(chain_j) çš„ä¸¥æ ¼å­é›†ã€‚
    ä½¿ç”¨å€’æ’ç´¢å¼•åŠ äº¤é›†åšå€™é€‰å‰ªæï¼Œé¿å… O(N^2) å…¨æ¯”è¾ƒã€‚
    """
    # åªä¿ç•™èƒ½äº§ç”Ÿå­é“¾çš„åŸå§‹é“¾ï¼ˆé•¿åº¦ >= min_lengthï¼‰
    idxs = [i for i, ch in enumerate(chains) if len(ch) >= min_length]
    sets = [set(chains[i]) for i in idxs]

    # å€’æ’ç´¢å¼•ï¼šå…ƒç´  -> å«æœ‰è¯¥å…ƒç´ çš„â€œå·²ä¿ç•™æå¤§é›†åˆâ€çš„ä¸‹æ ‡ï¼ˆåœ¨ kept_idxs åºï¼‰
    inv = defaultdict(set)

    # å…ˆæŒ‰â€œé›†åˆå¤§å°â€é™åºï¼Œè¿™æ ·æ›´å¤§çš„é›†åˆå…ˆè¿›å…¥â€œå·²ä¿ç•™é›†åˆæ± â€ï¼Œ
    # åˆ¤æ–­æŸé›†åˆæ˜¯å¦æœ‰â€œä¸¥æ ¼è¶…é›†â€æ—¶ï¼Œåªéœ€åœ¨æ± é‡ŒæŸ¥ã€‚
    order = sorted(range(len(idxs)), key=lambda k: -len(sets[k]))

    kept_local = []         # åœ¨æœ¬å‡½æ•°å†…éƒ¨çš„å±€éƒ¨ä¸‹æ ‡ï¼ˆæŒ‡å‘ idxs/sets çš„ä¸‹æ ‡ï¼‰
    kept_sets = []          # å·²ä¿ç•™çš„é›†åˆæœ¬ä½“ï¼ˆä¸ kept_local å¯¹é½ï¼‰

    for k in order:
        s = sets[k]
        if not s:
            # ç©ºé›†ï¼šè‹¥æ± ä¸­å·²æœ‰ä»»ä¸€éç©ºé›†åˆï¼Œåˆ™ç©ºé›†æ˜¯ä¸¥æ ¼å­é›†ï¼Œä¸¢å¼ƒï¼›å¦åˆ™ä¿ç•™ç¬¬ä¸€ä¸ªç©ºé›†ï¼ˆä¸åŸé€»è¾‘ç­‰ä»·ï¼‰
            if any(len(ts) > 0 for ts in kept_sets):
                continue
            kept_idx = len(kept_sets)
            kept_local.append(k)
            kept_sets.append(s)
            # ç©ºé›†ä¸è¿›å€’æ’
            continue

        # ç”¨å€’æ’ç´¢å¼•åšå€™é€‰â€œä¸¥æ ¼è¶…é›†â€å¬å›ï¼šäº¤æ‰€æœ‰å…ƒç´ çš„æ¡¶
        sig = sorted(s, key=lambda e: len(inv[e]))  # ç¨€æœ‰å…ƒç´ ä¼˜å…ˆï¼Œäº¤é›†æ›´å°
        cand = inv[sig[0]].copy() if sig else set()
        for e in sig[1:]:
            cand &= inv[e]
            if not cand:
                break

        # å€™é€‰é‡Œåªè¦å­˜åœ¨ size æ›´å¤§çš„å·²ä¿ç•™é›†åˆï¼Œå³ä¸ºä¸¥æ ¼è¶…é›†
        has_strict_superset = any(len(kept_sets[j]) > len(s) for j in cand)
        if has_strict_superset:
            continue

        # ä¿ç•™ï¼Œå¹¶æ›´æ–°å€’æ’
        kept_idx = len(kept_sets)
        kept_local.append(k)
        kept_sets.append(s)
        for e in s:
            inv[e].add(kept_idx)

    # æŠŠå±€éƒ¨ä¸‹æ ‡è½¬æ¢ä¸ºåŸå§‹é“¾ä¸‹æ ‡
    return [idxs[k] for k in kept_local]


def _all_covering_windows(chain: List[str], universe: Set[str], min_length: int) -> List[List[str]]:
    """
    è¿”å› chain ä¸­æ‰€æœ‰â€œè¦†ç›–äº† universe å…¨éƒ¨å…ƒç´ â€çš„è¿ç»­å­é“¾ï¼ˆé•¿åº¦>=min_lengthï¼‰ã€‚
    è¿™äº›çª—å£çš„å…ƒç´ é›†åˆæ°å¥½ç­‰äº universeã€‚
    ç”¨æ»‘åŠ¨çª—å£ä¸ºæ¯ä¸ªå·¦ç«¯ç‚¹æ‰¾åˆ°æœ€çŸ­å³ç«¯ç‚¹ï¼Œç„¶åå³ç«¯ç‚¹å‘å³æ‰©å±•éƒ½åˆæ³•ã€‚
    """
    need = {x: 1 for x in universe}
    have = Counter()
    covered = 0
    need_total = len(need)

    res = []
    n = len(chain)
    r = 0

    for l in range(n):
        # æ‰©å³ç›´åˆ°è¦†ç›–å…¨éƒ¨å…ƒç´ æˆ–åˆ°å°¾
        while r < n and covered < need_total:
            x = chain[r]
            if x in need:
                have[x] += 1
                if have[x] == 1:  # ç¬¬ä¸€æ¬¡è¦†ç›–è¯¥å…ƒç´ 
                    covered += 1
            r += 1

        if covered == need_total:
            # æ‰¾åˆ°æœ€çŸ­è¦†ç›–çª—å£ [l, r-1]ï¼Œä»»ä½• r' >= r-1 ä¹Ÿè¦†ç›–
            min_r = r - 1
            start_r = max(min_r, l + min_length - 1)
            if start_r < n:
                for rr in range(start_r, n):
                    # æ‰©å±•ä¸ä¼šå¼•å…¥ universe ä¹‹å¤–çš„æ–°å…ƒç´ ï¼ˆå› ä¸º universe=è¯¥é“¾çš„å…¨é›†ï¼‰
                    res.append(chain[l:rr+1])

        # å·¦ç«¯ç‚¹å³ç§»å‰ï¼Œç§»é™¤å®ƒçš„è´¡çŒ®
        x = chain[l]
        if x in need:
            have[x] -= 1
            if have[x] == 0:
                covered -= 1

    return res


def get_frequent_subchains_with_subset_removal(
    chains: List[List[str]],
    min_length: int = 2,
) -> List[List[str]]:
    """
    ç»„åˆç‰ˆæœ¬ï¼ˆä»…é’ˆå¯¹ min_count=1 çš„åœºæ™¯ï¼‰ï¼š
    ç­‰ä»·äºï¼š
      A = get_frequent_subchains(chains, min_length=min_length, min_count=1)
      B = remove_subset_paths(A)
    ä½†æ— éœ€æšä¸¾æ‰€æœ‰å­é“¾ï¼Œåªç”Ÿæˆæœ€ç»ˆä¼šå­˜æ´»çš„é‚£äº›å­é“¾ã€‚

    è¯´æ˜ï¼š
    - å…ˆæ‰¾å‡ºâ€œäº‹ä»¶é›†åˆä¸¥æ ¼æå¤§â€çš„åŸå§‹é“¾ï¼ˆé•¿åº¦>=min_lengthæ‰å¯èƒ½äº§ç”Ÿå­é“¾ï¼‰ï¼›
    - å¯¹è¿™äº›é“¾ï¼Œæšä¸¾æ‰€æœ‰â€œè¦†ç›–å…¶å…¨é›†çš„å­é“¾çª—å£â€ï¼ˆå…¶å…ƒç´ é›†åˆ == è¯¥é“¾å»é‡åçš„å…¨é›†ï¼‰ã€‚
    - è¾“å‡ºå³ä¸ºä¸¤æ­¥ç®¡çº¿çš„æœ€ç»ˆå­˜æ´»é›†åˆï¼ˆé¡ºåºå¯èƒ½ä¸åŒï¼Œä½†å†…å®¹ä¸€è‡´ï¼‰ã€‚
    """
    # 1) å–ä¸¥æ ¼æå¤§é›†åˆå¯¹åº”çš„åŸå§‹é“¾
    maximal_idxs = _maximal_chain_indices_by_set(chains, min_length=min_length)

    # 2) å¯¹æ¯æ¡æå¤§é“¾ï¼Œæšä¸¾æ‰€æœ‰è¦†ç›–å…¨é›†çš„çª—å£ï¼ˆé•¿åº¦>=min_lengthï¼‰
    out: List[List[str]] = []
    for i in maximal_idxs:
        ch = chains[i]
        uni = set(ch)
        out.extend(_all_covering_windows(ch, uni, min_length))

    # å¯é€‰ï¼šåšä¸€ä¸ªç¨³å®šæ’åºï¼ˆçº¯ä¸ºå¯è¯»æ€§ï¼›ä¸åŸä¸¤æ­¥çš„â€œæœ€ç»ˆä¿ç•™é›†åˆâ€å†…å®¹ä¸€è‡´ï¼‰
    # æ’åºè§„åˆ™ä¸åŸ get_frequent_subchains çš„æ¬¡åºæ¥è¿‘ï¼šé•¿åº¦é™åºï¼Œè¯å…¸åº
    out.sort(key=lambda seq: (-len(seq), tuple(seq)))
    return out



def get_frequent_subchains(chains: List[List[str]], min_length: int = 2, min_count: int = 2):
    """
    ç»Ÿè®¡äº‹ä»¶é“¾ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„è¿ç»­å­é“¾
    Args:
        chains: äº‹ä»¶é“¾åˆ—è¡¨
        min_length: æœ€çŸ­å­é“¾é•¿åº¦
        min_count: æœ€å°‘å‡ºç°æ¬¡æ•°ï¼ˆé¢‘ç‡é˜ˆå€¼ï¼‰
    Returns:
        List[List[str]]  å­é“¾åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡ä¸é•¿åº¦é™åºï¼‰
    """
    counter = Counter()
    for chain in chains:
        n = len(chain)
        # æšä¸¾æ‰€æœ‰è¿ç»­å­é“¾
        for i in range(n):
            for j in range(i + min_length, n + 1):
                sub = tuple(chain[i:j])
                counter[sub] += 1
    # è¿‡æ»¤ä½é¢‘
    results = [(list(sub), cnt) for sub, cnt in counter.items() if cnt >= min_count]
    # æŒ‰é¢‘ç‡ã€é•¿åº¦æ’åº
    results.sort(key=lambda x: (-x[1], -len(x[0]), x[0]))
    return [pair[0] for pair in results]


# -----------------------------
# ä¸»ç±»ï¼šEventCausalityBuilder
# -----------------------------
class EventCausalityBuilder:
    """
    äº‹ä»¶å› æœå›¾æ„å»ºå™¨

    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä»Neo4jåŠ è½½å’Œæ’åºäº‹ä»¶
    2. é€šè¿‡è¿é€šä½“å’Œç¤¾åŒºè¿‡æ»¤äº‹ä»¶å¯¹
    3. ä½¿ç”¨extractoræ£€æŸ¥å› æœå…³ç³»
    4. æ„å»ºæœ‰å‘å¸¦æƒNetworkXå›¾
    5. ä¿å­˜å’ŒåŠ è½½å›¾æ•°æ®ï¼ˆå…¨éƒ¨å†™å…¥ EPG è·¯å¾„ï¼‰
    6. æ„å»ºPlotæƒ…èŠ‚å•å…ƒå›¾è°±
    """

    def __init__(self, config):
        """
        åˆå§‹åŒ–äº‹ä»¶å› æœå›¾æ„å»ºå™¨

        Args:
            config: KAGé…ç½®å¯¹è±¡
        """
        self.config = config
        self.llm = OpenAILLM(config)
        self.graph_store = GraphStore(config)
        self.vector_store = VectorStore(config, "documents")
        self.event_fallback = []  # å¯ä»¥åŠ å…¥Goalå’ŒAction

        self.doc_type = config.knowledge_graph_builder.doc_type
        if self.doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {self.doc_type}")
        self.meta = DOC_TYPE_META[self.doc_type]

        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, self.doc_type)
        self.neo4j_utils.load_embedding_model(config.graph_embedding)

        # åˆå§‹åŒ–Plotç›¸å…³ç»„ä»¶
        prompt_dir = config.knowledge_graph_builder.prompt_dir
        self.prompt_loader = PromptLoader(prompt_dir)
        settings_path = os.path.join(self.config.storage.graph_schema_path, "settings.json")
        if not os.path.exists(settings_path):
            settings_path = self.config.probing.default_background_path

        settings = json.load(open(settings_path, "r", encoding="utf-8"))

        self.system_prompt_text = self.construct_system_prompt(
            background=settings.get("background"),
            abbreviations=settings.get("abbreviations", [])
        )

        self.graph_analyzer = GraphManager(config, self.llm)

        # Plotæ„å»ºé…ç½®å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
        self.causality_threshold = "Medium"
        self.logger = logging.getLogger(__name__)
        self.sorted_scenes = []
        self.event_list = []
        self.event2section_map = {}
        self.max_depth = config.event_plot_graph_builder.max_depth
        self.check_weakly_connected_components = True
        self.min_component_size = config.event_plot_graph_builder.min_connected_component_size
        self.max_workers = config.event_plot_graph_builder.max_workers
        self.max_iteration = config.event_plot_graph_builder.max_iterations
        self.check_weakly_connected_components = config.event_plot_graph_builder.check_weakly_connected_components
        self.max_num_triangles = config.event_plot_graph_builder.max_num_triangles

        # å› æœå…³ç³»å¼ºåº¦åˆ°æƒé‡çš„æ˜ å°„
        self.event_cards: Dict[str, Dict[str, Any]] = {}

        self.logger.info("EventCausalityBuilderåˆå§‹åŒ–å®Œæˆ")

    # -----------------------------
    # Prompt æ„å»º
    # -----------------------------
    def construct_system_prompt(self, background, abbreviations):
        background_info = self.get_background_info(background, abbreviations)
        if self.doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel"
        system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})
        return system_prompt_text

    def get_background_info(self, background, abbreviations):
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" if background else ""

        def fmt(item: dict) -> str:
            if not isinstance(item, dict):
                return ""
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())
            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(item) for item in abbreviations if isinstance(item, dict))
        if background and abbr_block:
            background_info = f"{bg_block}\n{abbr_block}"
        else:
            background_info = bg_block or abbr_block
        return background_info

    # -----------------------------
    # äº‹ä»¶åˆ—è¡¨æ„å»º
    # -----------------------------
    def build_event_list(self) -> List[Entity]:
        """
        æ„å»ºæ’åºåçš„äº‹ä»¶åˆ—è¡¨
        Returns:
            æ’åºåçš„äº‹ä»¶åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æ„å»ºäº‹ä»¶åˆ—è¡¨...")

        # 1. è·å–æ‰€æœ‰ section å¹¶æ’åº
        section_entities = self.neo4j_utils.search_entities_by_type(
            entity_type=self.meta["section_label"]
        )
        self.sorted_sections = sorted(
            section_entities,
            key=lambda e: int(e.properties.get("order", 99999))
        )
        print(f"âœ… æ‰¾åˆ° {len(self.sorted_sections)} ä¸ªsection")

        # 2. ä»åœºæ™¯ä¸­æå–äº‹ä»¶
        event_list = []
        event2section_map = {}
        for section in tqdm(self.sorted_sections, desc="æå–åœºæ™¯ä¸­çš„äº‹ä»¶"):
            results = self.neo4j_utils.search_related_entities(
                source_id=section.id,
                predicate=self.meta["contains_pred"],
                entity_types=["Event"],
                return_relations=False
            )
            # fallbackï¼ˆå¯é€‰ï¼‰
            if not results and self.event_fallback:
                results = self.neo4j_utils.search_related_entities(
                    source_id=section.id,
                    relation_type=self.meta["contains_pred"],
                    entity_types=self.event_fallback,
                    return_relations=False
                )
            for result in results:
                if result.id not in event2section_map:
                    event2section_map[result.id] = section.id
                    event_list.append(result)

        self.event_list = event_list
        self.event2section_map = event2section_map

        print(f"âœ… æ„å»ºå®Œæˆï¼Œå…±æ‰¾åˆ° {len(event_list)} ä¸ªäº‹ä»¶")
        return event_list

    # -----------------------------
    # äº‹ä»¶å¡ç‰‡å¹¶å‘é¢„ç”Ÿæˆï¼ˆæ”¯æŒEPGè·¯å¾„ç¼“å­˜ï¼‰
    # -----------------------------
    def precompute_event_cards(
        self,
        events: List[Entity],
        per_task_timeout: float = 300,
        max_retries: int = 3,
        retry_timeout: float = 60.0,   # å…¼å®¹åŸå‚ï¼›ç»Ÿä¸€ç­–ç•¥é‡Œç”¨ä¸åˆ°è¿™ä¸ªå•ç‹¬è¶…æ—¶
    ) -> Dict[str, Dict[str, Any]]:
        """
        å¹¶å‘ä¸ºæ‰€æœ‰äº‹ä»¶ç”Ÿæˆ event_cardï¼š
        - è¯»/å†™ EPG/event_cards.json
        - é¦–è½®å¤±è´¥/è¶…æ—¶å†™å ä½ï¼›åç»­è½®ä»…é‡è¯•å¤±è´¥é¡¹
        """
        base = self.config.storage.event_plot_graph_path
        os.makedirs(base, exist_ok=True)
        cache_path = os.path.join(base, "event_cards.json")

        # è¯»å–ç¼“å­˜
        if os.path.exists(cache_path):
            try:
                with open(cache_path, "r", encoding="utf-8") as f:
                    self.event_cards = json.load(f)
                    if not isinstance(self.event_cards, dict):
                        self.event_cards = {}
            except Exception:
                self.event_cards = {}
        else:
            self.event_cards = {}

        existing = set(self.event_cards.keys())
        pending_events = [e for e in events if e.id not in existing]
        if not pending_events:
            print(f"ğŸ—‚ï¸ äº‹ä»¶å¡ç‰‡å·²å­˜åœ¨ï¼š{len(self.event_cards)} ä¸ªï¼Œè·³è¿‡ç”Ÿæˆã€‚")
            return self.event_cards

        def _collect_related_context_by_section(ev: Entity) -> str:
            ctx_set = set()
            sec_id = self.event2section_map.get(ev.id)
            if sec_id:
                sec = self.neo4j_utils.get_entity_by_id(sec_id)
                titles = sec.properties.get(self.meta['title'], [])
                if isinstance(titles, str):
                    titles = [titles]
                for t in titles or []:
                    try:
                        docs = self.vector_store.search_by_metadata({"title": t})
                        for d in docs:
                            if getattr(d, "content", None):
                                ctx_set.add(d.content)
                    except Exception:
                        pass
            if not ctx_set:
                try:
                    node = self.neo4j_utils.get_entity_by_id(ev.id)
                    chunk_ids = set((node.source_chunks or [])[:50])
                    if chunk_ids:
                        docs = self.vector_store.search_by_ids(list(chunk_ids))
                        for d in docs:
                            if getattr(d, "content", None):
                                ctx_set.add(d.content)
                except Exception:
                    pass
            return "\n".join(ctx_set)

        def _build_one(ev: Entity) -> str:
            info = self.neo4j_utils.get_entity_info(ev.id, "äº‹ä»¶", True, True)
            related_ctx = _collect_related_context_by_section(ev)
            out = self.graph_analyzer.generate_event_context(info, related_ctx)
            card = json.loads(correct_json_format(out))["event_card"]
            card = format_event_card(card)  # å¤§å¤šæ•°å®ç°ä¼šè¿”å› strï¼›å¦‚è¿”å› dict å¯ json.dumps
            return card

        def _placeholder(ev: Entity, exc=None) -> str:
            skeleton = {
                "name": ev.properties.get("name") or ev.name or f"event_{ev.id}",
                "summary": "",
                "time_hint": "unknown",
                "locations": [],
                "participants": [],
                "action": "",
                "outcomes": [],
                "evidence": ""
            }
            return json.dumps(skeleton, ensure_ascii=False)

        # ç»Ÿä¸€å¹¶å‘æ‰§è¡Œ
        res_map, still_failed = self._run_with_soft_timeout_and_retries(
            pending_events,
            work_fn=_build_one,
            key_fn=lambda e: e.id,
            desc_label="é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡",
            per_task_timeout=per_task_timeout,
            retries=max_retries,
            retry_backoff=1.0,
            allow_placeholder_first_round=True,
            placeholder_fn=_placeholder,
            should_retry=None  # å¯¹äºå¡ç‰‡ï¼Œåªåœ¨å¼‚å¸¸/è¶…æ—¶é‡è¯•ï¼›æ­£å¸¸ç»“æœä¸é‡è¯•
        )

        # å†™å›
        for k, v in res_map.items():
            self.event_cards[k] = v
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(self.event_cards, f, ensure_ascii=False, indent=2)

        print(f"ğŸ—‚ï¸ äº‹ä»¶å¡ç‰‡ç”Ÿæˆå®Œæˆï¼šæ€»è®¡ {len(self.event_cards)}ï¼Œæœ¬æ¬¡ç¼ºå£ä½™ {len(still_failed)}")
        return self.event_cards

    # -----------------------------
    # å€™é€‰äº‹ä»¶å¯¹è¿‡æ»¤
    # -----------------------------
    def filter_event_pairs_by_community(
        self,
        events: List[Entity],
        max_depth: int = 3
    ) -> List[Tuple[Entity, Entity]]:
        """
        åˆ©ç”¨ Neo4j ä¸­ Louvain ç»“æœç›´æ¥ç­›é€‰åŒç¤¾åŒºä¸” max_depth å†…å¯è¾¾çš„äº‹ä»¶å¯¹
        """
        id2entity = {e.id: e for e in events}
        pairs = self.neo4j_utils.fetch_event_pairs_same_community()
        filtered_pairs = []
        for row in pairs:
            src_id, dst_id = row["srcId"], row["dstId"]
            if src_id in id2entity and dst_id in id2entity:
                filtered_pairs.append((id2entity[src_id], id2entity[dst_id]))
        print(f"[âœ“] åŒç¤¾åŒºäº‹ä»¶å¯¹: {len(filtered_pairs)}")
        return filtered_pairs

    def sort_event_pairs_by_section_order(
        self, pairs: List[Tuple[Entity, Entity]]
    ) -> List[Tuple[Entity, Entity]]:
        def get_order(evt: Entity) -> int:
            sec_id = self.event2section_map.get(evt.id)
            if not sec_id:
                return 99999
            sec = self.neo4j_utils.get_entity_by_id(sec_id)
            return int(sec.properties.get("order", 99999))
        ordered = []
        for e1, e2 in pairs:
            ordered.append((e1, e2) if get_order(e1) <= get_order(e2) else (e2, e1))
        return ordered

    def filter_pair_by_distance_and_similarity(self, pairs):
        filtered_pairs = []
        for pair in tqdm(pairs, desc="ç­›é€‰èŠ‚ç‚¹å¯¹"):
            src_id, tgt_id = pair[0].id, pair[1].id
            reachable = self.neo4j_utils.check_nodes_reachable(
                src_id, tgt_id,
                excluded_rels=[self.meta["contains_pred"], "EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"],
                max_depth=self.max_depth
            )
            if reachable:
                filtered_pairs.append(pair)
            else:
                score = self.neo4j_utils.compute_semantic_similarity(src_id, tgt_id)
                if score >= 0.5:
                    filtered_pairs.append(pair)
        return filtered_pairs

    # -----------------------------
    # å› æœæ€§åˆ¤å®šï¼ˆå¹¶å‘ + æ–­ç‚¹ç»­è·‘ï¼‰
    # -----------------------------
    def check_causality_batch(
        self,
        pairs: List[Tuple[Entity, Entity]]
    ) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        å¹¶å‘æ£€æŸ¥äº‹ä»¶å¯¹çš„å› æœå…³ç³»ï¼ˆç»Ÿä¸€å¹¶å‘/è½¯è¶…æ—¶/é‡è¯•ï¼‰
        - é¦–è½®ï¼šå¤±è´¥/è¶…æ—¶å†™å ä½
        - ä»…å¯¹å¤±è´¥é¡¹é‡è¯•ï¼›æˆåŠŸè¦†ç›–æ—§ç»“æœ
        """
        PER_TASK_TIMEOUT = 1800
        MAX_RETRIES = 2
        RETRY_BACKOFF = 2.0

        def _make_result(src_event, tgt_event,
                        relation="NONE",
                        reason="",
                        temporal_order="Unknown",
                        confidence=0.0,
                        raw_result="",
                        timeout=False) -> Dict[str, Any]:
            res = {
                "src_event": src_event,
                "tgt_event": tgt_event,
                "relation": relation,
                "reason": reason,
                "temporal_order": temporal_order,
                "confidence": float(confidence) if confidence is not None else 0.0,
                "raw_result": raw_result
            }
            if timeout:
                res["causality_timeout"] = True
            return res

        def _get_common_neighbor_info(src_id, tgt_id):
            commons = self.neo4j_utils.get_common_neighbors(src_id, tgt_id, limit=50)
            info = "ä¸¤ä¸ªäº‹ä»¶å…·æœ‰çš„å…±åŒé‚»å±…çš„ä¿¡æ¯ä¸ºï¼š\n"
            if not commons:
                return info + "æ— "
            for ent_ in commons:
                try:
                    ent_type = "/".join(ent_.type) if isinstance(ent_.type, (list, set, tuple)) else str(ent_.type)
                except Exception:
                    ent_type = "Unknown"
                info += f"- å®ä½“åç§°ï¼š{ent_.name}ï¼Œå®ä½“ç±»å‹ï¼š{ent_type}ï¼Œç›¸å…³æè¿°ä¸ºï¼š{ent_.description}\n"
            return info

        def _ensure_card(e: Entity, info_text: str):
            if e.id in self.event_cards:
                return self.event_cards[e.id]
            out = self.graph_analyzer.generate_event_context(info_text, "")
            card = json.loads(correct_json_format(out))["event_card"]
            card = format_event_card(card)
            self.event_cards[e.id] = card
            return card

        def _work(pair: Tuple[Entity, Entity]) -> Dict[str, Any]:
            src_event, tgt_event = pair
            try:
                info_1 = self.neo4j_utils.get_entity_info(src_event.id, "äº‹ä»¶", True, True)
                info_2 = self.neo4j_utils.get_entity_info(tgt_event.id, "äº‹ä»¶", True, True)
                related_context = info_1 + "\n" + info_2 + "\n" + _get_common_neighbor_info(src_event.id, tgt_event.id)
                src_card = _ensure_card(src_event, info_1)
                tgt_card = _ensure_card(tgt_event, info_2)

                result_json = self.graph_analyzer.check_event_causality(
                    src_card, tgt_card,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )
                if isinstance(result_json, dict):
                    result_dict = result_json
                    raw_str = json.dumps(result_json, ensure_ascii=False)
                else:
                    result_dict = json.loads(correct_json_format(result_json))
                    raw_str = result_json

                relation = result_dict.get("relation", "NONE")
                reason = result_dict.get("reason", "")
                temporal_order = result_dict.get("temporal_order", "Unknown")
                confidence = result_dict.get("confidence", 0.3)

                return _make_result(
                    src_event, tgt_event,
                    relation=relation,
                    reason=reason,
                    temporal_order=temporal_order,
                    confidence=confidence,
                    raw_result=raw_str,
                    timeout=False
                )
            except Exception as e:
                return _make_result(
                    src_event, tgt_event,
                    relation="NONE",
                    reason=f"æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}",
                    temporal_order="Unknown",
                    confidence=0.0,
                    raw_result="",
                    timeout=True
                    )

        def _placeholder(pair: Tuple[Entity, Entity], exc=None) -> Dict[str, Any]:
            src_event, tgt_event = pair
            return _make_result(
                src_event, tgt_event,
                relation="NONE",
                reason="è½¯è¶…æ—¶/å¼‚å¸¸ï¼Œå ä½è¿”å›",
                temporal_order="Unknown",
                confidence=0.0,
                raw_result="",
                timeout=True
            )

        def _should_retry(res: Dict[str, Any]) -> bool:
            if res.get("causality_timeout"):
                return True
            reason = (res.get("reason") or "").strip()
            return ("å‡ºé”™" in reason)

        print(f"ğŸ” å¼€å§‹å¹¶å‘æ£€æŸ¥ {len(pairs)} å¯¹äº‹ä»¶çš„å› æœå…³ç³»...")

        res_map, still_failed = self._run_with_soft_timeout_and_retries(
            pairs,
            work_fn=_work,
            key_fn=lambda p: (p[0].id, p[1].id),
            desc_label="å¹¶å‘æ£€æŸ¥å› æœå…³ç³»",
            per_task_timeout=PER_TASK_TIMEOUT,
            retries=MAX_RETRIES,
            retry_backoff=RETRY_BACKOFF,
            allow_placeholder_first_round=True,
            placeholder_fn=_placeholder,
            should_retry=_should_retry
        )

        # æ ‡æ³¨æœ€ç»ˆä»å¤±è´¥çš„é”®
        for k in still_failed:
            if k in res_map:
                res_map[k]["final_fallback"] = True
                res_map[k]["retries"] = MAX_RETRIES

        print(f"âœ… å› æœå…³ç³»å¹¶å‘æ£€æŸ¥å®Œæˆï¼ˆæˆåŠŸ {len(pairs) - len(still_failed)} / {len(pairs)}ï¼Œä»å¤±è´¥ {len(still_failed)}ï¼‰")
        return res_map

    # -----------------------------
    # åˆå§‹åŒ–ï¼šå­å›¾ + Louvain
    # -----------------------------
    def initialize(self):
        for relation_type in ["EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"]:
            self.neo4j_utils.delete_relation_type(relation_type)

        self.neo4j_utils.create_subgraph(
            graph_name="knowledge_graph",
            exclude_entity_types=[self.meta["section_label"]],
            exclude_relation_types=[self.meta["contains_pred"]],
            force_refresh=True
        )
        self.neo4j_utils.run_louvain(
            graph_name="knowledge_graph",
            write_property="community",
            force_run=True
        )

        self.clear_directory(self.config.event_plot_graph_path)

    def clear_directory(self, path):
        for file in glob.glob(os.path.join(path, "*.json")):
            try:
                os.remove(file)
            except Exception as e:
                print(f"åˆ é™¤å¤±è´¥: {file} -> {e}")

    # -----------------------------
    # ä¸»æµç¨‹1ï¼šæ„å»ºäº‹ä»¶å› æœå›¾ï¼ˆæŒä¹…åŒ–ï¼‰
    # -----------------------------
    def build_event_causality_graph(
        self,
        limit_events: Optional[int] = None
    ) -> None:
        """
        å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹ï¼ˆæ¯æ­¥ç»“æŸéƒ½æŠŠäº§ç‰©å†™å…¥ EPG è·¯å¾„ï¼‰
        äº§ç‰©ï¼š
          - event_cards.json
          - event_causality_results.pkl
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹...")

        base = self.config.storage.event_plot_graph_path
        os.makedirs(base, exist_ok=True)

        # 2. æ„å»ºäº‹ä»¶åˆ—è¡¨
        print("\nğŸ” æ„å»ºäº‹ä»¶åˆ—è¡¨...")
        event_list = self.build_event_list()

        # 3. é™åˆ¶äº‹ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if limit_events and limit_events < len(event_list):
            event_list = event_list[:limit_events]
            print(f"âš ï¸ é™åˆ¶å¤„ç†äº‹ä»¶æ•°é‡ä¸º: {limit_events}")

        # è¯»å–å·²æœ‰ event_cardsï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™ç”Ÿæˆ
        event_cards_path = os.path.join(base, "event_cards.json")
        if os.path.exists(event_cards_path):
            try:
                with open(event_cards_path, "r", encoding="utf-8") as f:
                    self.event_cards = json.load(f)
                    if not isinstance(self.event_cards, dict):
                        self.event_cards = {}
                print(f"ğŸ—‚ï¸ å¤ç”¨å·²æœ‰äº‹ä»¶å¡ç‰‡ï¼š{len(self.event_cards)}")
            except Exception:
                self.event_cards = {}
        else:
            print("\nğŸ§© å¹¶å‘é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡...")
            self.precompute_event_cards(event_list)

        # 4. è¿‡æ»¤äº‹ä»¶å¯¹
        print("\nğŸ” è¿‡æ»¤äº‹ä»¶å¯¹...")
        filtered_pairs = self.filter_event_pairs_by_community(event_list, max_depth=self.max_depth)
        filtered_pairs = self.filter_pair_by_distance_and_similarity(filtered_pairs)
        filtered_pairs = self.sort_event_pairs_by_section_order(filtered_pairs)
        print("     æœ€ç»ˆå€™é€‰äº‹ä»¶å¯¹æ•°é‡ï¼š ", len(filtered_pairs))

        # 5. æ£€æŸ¥å› æœå…³ç³»ï¼ˆè¯»å– self.event_cardsï¼‰
        print("\nğŸ” æ£€æŸ¥å› æœå…³ç³»...")
        causality_results = self.check_causality_batch(filtered_pairs)

        # â€”â€” ä¿å­˜äº§ç‰©åˆ° EPG è·¯å¾„
        with open(os.path.join(base, "event_causality_results.pkl"), "wb") as f:
            pickle.dump(causality_results, f)
        with open(event_cards_path, "w", encoding="utf-8") as f:
            json.dump(self.event_cards, f, ensure_ascii=False, indent=2)

        # 6. å†™å› EVENT å…³ç³»
        print("\nğŸ”— å†™å›Eventé—´å…³ç³»...")
        self.write_event_cause_edges(causality_results)
        self.neo4j_utils.create_event_causality_graph("event_causality_graph", force_refresh=True)

    # -----------------------------
    # å†™å›äº‹ä»¶å› æœè¾¹
    # -----------------------------
    def write_event_cause_edges(self, causality_results):
        rows = []
        for (src_id, dst_id), res in causality_results.items():
            rel = (res.get("relation") or "").upper()
            if rel != "NONE":
                confidence = float(res.get("confidence", 0.3) or 0.0)
                rows.append({
                    "srcId": src_id,
                    "dstId": dst_id,
                    "confidence": confidence,
                    "reason": res.get("reason", ""),
                    "predicate": res.get("relation", "NONE")
                })
        self.neo4j_utils.write_event_causes(rows)

    # -----------------------------
    # SABERï¼šç»“æ„çº¦æŸçš„è¾¹ç²¾ç®€ï¼ˆæŒä¹…åŒ–æ¯è½®åˆ é™¤ï¼‰
    # -----------------------------
    def detect_flattened_causal_patterns(self, edges: List[Dict]) -> List[Dict]:
        forward_graph = defaultdict(set)
        edge_set = set()
        for edge in edges:
            sid = edge["sid"]
            tid = edge["tid"]
            forward_graph[sid].add(tid)
            edge_set.add((sid, tid))

        patterns = []
        for a, a_children in forward_graph.items():
            a_children = list(a_children)
            if len(a_children) < 2:
                continue
            internal_links = []
            for i in range(len(a_children)):
                for j in range(len(a_children)):
                    if i == j:
                        continue
                    u, v = a_children[i], a_children[j]
                    if (u, v) in edge_set:
                        internal_links.append((u, v))
            if internal_links:
                patterns.append({
                    "source": a,
                    "targets": a_children,
                    "internal_links": internal_links
                })
        return patterns

    def filter_weak_edges_in_patterns(
        self,
        patterns: List[Dict],
        edge_map: Dict[Tuple[str, str], Dict],
        conf_threshold: float = 0.5
    ) -> List[Dict]:
        cleaned_patterns = []
        for pat in patterns:
            src = pat["source"]
            targets = pat["targets"]
            internals = pat["internal_links"]

            new_targets = []
            for t in targets:
                info = edge_map.get((src, t))
                confidence = info.get("confidence", 0) if info else 0
                if not info:
                    continue
                if confidence < conf_threshold:
                    new_targets.append(t)

            new_internals = []
            for u, v in internals:
                info = edge_map.get((u, v))
                confidence = info.get("confidence", 0) if info else 0
                if not info:
                    continue
                if not confidence < conf_threshold:
                    new_internals.append((u, v))

            if len(new_targets) >= 2 and new_internals:
                cleaned_patterns.append({
                    "source": src,
                    "targets": new_targets,
                    "internal_links": new_internals
                })
        return cleaned_patterns

    def collect_removed_edges(self,
                              original_patterns: List[Dict],
                              filtered_patterns: List[Dict]
                              ) -> Set[Tuple[str, str]]:
        def extract_edges(patterns: List[Dict]) -> Set[Tuple[str, str]]:
            edge_set = set()
            for pat in patterns:
                src = pat["source"]
                for tgt in pat["targets"]:
                    edge_set.add((src, tgt))
                edge_set.update(pat["internal_links"])
            return edge_set

        origin_edges = extract_edges(original_patterns)
        filtered_edges = extract_edges(filtered_patterns)
        removed_edges = origin_edges - filtered_edges
        print(f"[+] Found {len(removed_edges)} candidate edges removed due to pattern collapse")
        return list(removed_edges)

    def filter_pattern(self, pattern, edge_map):
        source = pattern["source"]
        targets = pattern["targets"]
        internal_links = pattern["internal_links"]
        context_to_check = []
        for link in internal_links:
            mid_tgt_sim = self.neo4j_utils.compute_semantic_similarity(link[0], link[1])
            src_mid_sim = self.neo4j_utils.compute_semantic_similarity(source, link[0])
            src_tgt_sim = self.neo4j_utils.compute_semantic_similarity(source, link[1])

            mid_tgt_conf = edge_map.get((link[0], link[1]))["confidence"]
            src_mid_conf = edge_map.get((source, link[0]))["confidence"]
            src_tgt_conf = edge_map.get((source, link[1]))["confidence"]

            # if (src_mid_sim > src_tgt_sim and mid_tgt_sim > src_tgt_sim) or (src_mid_conf > src_tgt_conf and mid_tgt_conf > src_tgt_conf):
            context_to_check.append({
                "entities": [source, link[0], link[1]],
                "details": [
                    {"edge": [source, link[0]], "similarity": src_mid_sim, "confidence": src_mid_conf},
                    {"edge": [source, link[1]], "similarity": src_tgt_sim, "confidence": src_tgt_conf},
                    {"edge": [link[0], link[1]], "similarity": mid_tgt_sim, "confidence": mid_tgt_conf},
                ]
            })
        return context_to_check

    def prepare_context(self, pattern_detail):
        def _safe_str(x: Any) -> str:
            return x if isinstance(x, str) else ("" if x is None else str(x))

        event_details = self.neo4j_utils.get_event_details(pattern_detail["entities"])
        full_event_details = "ä¸‰ä¸ªäº‹ä»¶å®ä½“çš„æè¿°å¦‚ä¸‹ï¼š\n"

        for i, event_info in enumerate(event_details):
            event_id = event_info["event_id"]
            full_event_details += f"**äº‹ä»¶{i+1}çš„ç›¸å…³æè¿°å¦‚ä¸‹ï¼š**\näº‹ä»¶idï¼š{event_id}\n"

            background = self.neo4j_utils.get_entity_info(event_id, "äº‹ä»¶", True, True)
            background = _safe_str(background)

            props_raw = event_info.get("event_properties")
            if isinstance(props_raw, dict):
                event_props = props_raw
            elif isinstance(props_raw, str) and props_raw.strip():
                try:
                    event_props = json.loads(props_raw)
                    if not isinstance(event_props, dict):
                        event_props = {}
                except Exception:
                    event_props = {}
            else:
                event_props = {}

            non_empty_props = {k: v for k, v in event_props.items() if isinstance(v, str) and v.strip()}

            if non_empty_props:
                background += "\näº‹ä»¶çš„å±æ€§å¦‚ä¸‹ï¼š\n"
                for k, v in non_empty_props.items():
                    background += f"- {k}ï¼š{v}\n"

            if i + 1 != len(event_details):
                background += "\n"

            full_event_details += background

        full_relation_details = "å®ƒä»¬ä¹‹é—´å·²ç»å­˜åœ¨çš„å› æœå…³ç³»æœ‰ï¼š\n"
        relation_details = pattern_detail["details"]
        for i, relation_info in enumerate(relation_details):
            src, tgt = relation_info["edge"]
            rel_summary = self.neo4j_utils.get_relation_summary(src, tgt, "EVENT_CAUSES")
            rel_summary = _safe_str(rel_summary)
            background = f"{i+1}. " + rel_summary
            background += f"\nå…³ç³»çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºï¼š{round(relation_info['similarity'], 4)}ï¼Œç½®ä¿¡åº¦ä¸ºï¼š{relation_info['confidence']}ã€‚"
            if i + 1 != len(relation_details):
                background += "\n\n"
            full_relation_details += background

        return full_event_details, full_relation_details

    def run_SABER(self):
        """
        æ‰§è¡ŒåŸºäºç»“æ„+LLMçš„å› æœè¾¹ç²¾ç®€ä¼˜åŒ–è¿‡ç¨‹
        äº§ç‰©ï¼š
          - saber_removed_edges_round_{i}.jsonï¼ˆæ¯è½®ä¸€ä»½ï¼Œå†™å…¥ EPGï¼‰
        """
        loop_count = 0
        while True:
            print(f"\n===== [ç¬¬ {loop_count + 1} è½®ä¼˜åŒ–] =====")

            scc_components = self.neo4j_utils.fetch_scc_components("event_causality_graph", 2)
            wcc_components = []
            if self.check_weakly_connected_components:
                wcc_components = self.neo4j_utils.fetch_wcc_components("event_causality_graph", self.min_component_size)

            connected_components = scc_components + wcc_components
            print(f"ğŸ“Œ å½“å‰è¿é€šä½“æ•°é‡ï¼šSCC={len(scc_components)}ï¼ŒWCC={len(wcc_components)}")

            all_triangles = []
            edge_map_global = {}

            for cc in connected_components:
                node_map, edges = self.neo4j_utils.load_connected_components_subgraph(cc)
                edge_map = {
                    (e["sid"], e["tid"]): {"confidence": e.get("confidence", 0.0)}
                    for e in edges
                }
                edge_map_global.update(edge_map)

                old_patterns = self.detect_flattened_causal_patterns(edges)
                new_patterns = self.filter_weak_edges_in_patterns(old_patterns, edge_map, conf_threshold=0.5)
                for pattern in new_patterns:
                    all_triangles += self.filter_pattern(pattern, edge_map)

            print(f"ğŸ”º æœ¬è½®éœ€åˆ¤æ–­çš„ä¸‰å…ƒå› æœç»“æ„æ•°é‡ï¼š{len(all_triangles)}")
            if len(all_triangles) >= self.max_num_triangles:
                print(f"âš ï¸ æ£€æµ‹åˆ°ä¸‰å…ƒç»“æ„æ•°é‡è¿‡å¤šï¼Œåªé€‰æ‹©å‰{self.max_num_triangles}ä¸ªè¿›è¡Œå¤„ç†ã€‚")
                all_triangles = all_triangles[:self.max_num_triangles]
                return

            if loop_count >= 1:
                if len(scc_components) == 0 and len(all_triangles) == 0:
                    print("âœ… å›¾ç»“æ„å·²æ— å¼ºè¿é€šä½“ï¼Œä¸”æ— å¾…åˆ¤å®šä¸‰å…ƒç»“æ„ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
                    break
            elif loop_count >= self.max_iteration:
                break

            removed_edges = []

            def process_triangle(triangle_):
                try:
                    event_details, relation_details = self.prepare_context(triangle_)
                    chunks = [self.neo4j_utils.get_entity_by_id(ent_id).source_chunks[0] for ent_id in triangle_["entities"]]
                    chunks = list(set(chunks))
                    documents = self.vector_store.search_by_ids(chunks)
                    results = {doc.content for doc in documents}
                    related_context = "\n".join(list(results))

                    output = self.graph_analyzer.evaluate_event_redundancy(
                        event_details, relation_details, self.system_prompt_text, related_context
                    )
                    output = json.loads(correct_json_format(output))
                    if output.get("remove_edge", False):
                        return (triangle_["entities"][0], triangle_["entities"][2])
                except Exception as e:
                    print(f"[âš ï¸ é”™è¯¯] Triangle åˆ¤æ–­å¤±è´¥: {triangle_['entities']}, é”™è¯¯ä¿¡æ¯: {str(e)}")
                return None

            print(f"ğŸ§  æ­£åœ¨å¹¶å‘åˆ¤æ–­ä¸‰å…ƒç»“æ„...")
            tri_map, tri_failed = self._run_with_soft_timeout_and_retries(
                all_triangles,
                work_fn=process_triangle,                 # (tri) -> Optional[Tuple[src, tgt]]
                key_fn=lambda tri: tuple(tri["entities"]),
                desc_label="LLMåˆ¤æ–­ä¸‰å…ƒç»“æ„",
                per_task_timeout=600.0,
                retries=1,
                retry_backoff=1.0,
                allow_placeholder_first_round=False,
                placeholder_fn=None,
                should_retry=lambda r: r is None          # None ä»£è¡¨æœªèƒ½ç»™å‡ºè£å†³
            )

            removed_edges = [edge for edge in tri_map.values() if edge]
            print(f"âŒ æœ¬è½®å¾…å®šç§»é™¤è¾¹æ•°é‡ï¼š{len(set(removed_edges))}")

            # â€”â€” åˆ é™¤è¾¹
            for edge in removed_edges:
                self.neo4j_utils.delete_relation_by_ids(edge[0], edge[1], ["EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"])

            # â€”â€” æŒä¹…åŒ–æœ¬è½®åˆ é™¤æ—¥å¿—ï¼ˆEPGï¼‰
            base = self.config.storage.event_plot_graph_path
            os.makedirs(base, exist_ok=True)
            saber_log_path = os.path.join(base, f"saber_removed_edges_round_{loop_count+1}.json")
            try:
                with open(saber_log_path, "w", encoding="utf-8") as f:
                    json.dump([list(e) for e in set(removed_edges)], f, ensure_ascii=False, indent=2)
            except Exception:
                pass

            # â€”â€” åˆ·æ–° GDS å›¾
            self.neo4j_utils.create_event_causality_graph("event_causality_graph", min_confidence=0, force_refresh=True)
            loop_count += 1

    # -----------------------------
    # å·¥å…·ï¼šæå–æ‰€æœ‰äº‹ä»¶é“¾ & æ–‡æœ¬ä¸Šä¸‹æ–‡
    # -----------------------------
    def get_all_event_chains(self, min_confidence: float = 0.0):
        """
        è·å–æ‰€æœ‰å¯èƒ½çš„äº‹ä»¶é“¾ï¼ˆä»èµ·ç‚¹åˆ°æ²¡æœ‰å‡ºè¾¹çš„ç»ˆç‚¹ï¼‰
        """
        starting_events = self.neo4j_utils.get_starting_events()
        chains = []
        for event in starting_events:
            all_chains = self.neo4j_utils.find_event_chain(event, min_confidence)
            chains.extend([chain for chain in all_chains if len(chain) >= 2])
        return chains

    def prepare_chain_context(self, chain):
        if len(chain) > 1:
            context = "äº‹ä»¶é“¾ï¼š" + "->".join(chain) + "\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        else:
            context = f"äº‹ä»¶ï¼š{chain[0]}" + "\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        for i, event in enumerate(chain):
            # è¿™é‡Œç›´æ¥ä½¿ç”¨ event_cardsï¼Œé¿å…å†æ¬¡æ‹¼è£…é•¿æ–‡æœ¬
            context += f"äº‹ä»¶{i+1}ï¼š{event}\n" + self.event_cards[event] + "\n"
        return context

    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_embedding_model(self.config.graph_embedding)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            entity_types=["Event", "Plot"]
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… äº‹ä»¶æƒ…èŠ‚å›¾å‘é‡æ„å»ºå®Œæˆ")

    # -----------------------------
    # ä¸»æµç¨‹2ï¼šæ„å»ºæƒ…èŠ‚-äº‹ä»¶å›¾ï¼ˆè¯»å†™EPGï¼‰
    # -----------------------------
    def build_event_plot_graph(self):
        """
        æ„å»ºæƒ…èŠ‚-äº‹ä»¶å›¾
        è¯»ï¼šEPG/event_cards.json
        å†™ï¼šEPG/filtered_event_chains.json
        """
        # æ¸…ç©ºæ—§çš„ Plot å›¾ä¸å…³ç³»ï¼ˆå·²é€‚é…æ–°å…­ç§ Plot å…³ç³» + HAS_EVENTï¼‰
        self.neo4j_utils.reset_event_plot_graph()

        base = self.config.storage.event_plot_graph_path
        os.makedirs(base, exist_ok=True)

        # è¯»å–äº‹ä»¶å¡ç‰‡ï¼ˆEPGï¼‰
        cards_path = os.path.join(base, "event_cards.json")
        with open(cards_path, "r", encoding="utf-8") as f:
            self.event_cards = json.load(f)

        all_chains = self.get_all_event_chains(min_confidence=0.0)
        print("[âœ“] å½“å‰äº‹ä»¶é“¾æ€»æ•°ï¼š", len(all_chains))

        # filtered_chains = get_frequent_subchains(all_chains, 2, 1)
        # filtered_chains = remove_subset_paths(filtered_chains)
        filtered_chains = get_frequent_subchains_with_subset_removal(all_chains, 2)
        filtered_chains = remove_similar_paths(filtered_chains, 0.75)
        print("[âœ“] è¿‡æ»¤åäº‹ä»¶é“¾æ€»æ•°ï¼š", len(filtered_chains))

        # â€”â€” ä¿å­˜ç­›åé“¾æ¡åˆ° EPG
        with open(os.path.join(base, "filtered_event_chains.json"), "w", encoding="utf-8") as f:
            json.dump(filtered_chains, f, ensure_ascii=False, indent=2)

        def _stable_plot_id(title: str, chain: list[str]) -> str:
            key = f"{title}||{'->'.join(chain)}"
            return "plot_" + hashlib.sha1(key.encode("utf-8")).hexdigest()[:16]

        def _to_bool(v) -> bool:
            if isinstance(v, bool):
                return v
            if v is None:
                return False
            return str(v).strip().lower() in ("true", "yes", "1")

        def process_chain(chain):
            try:
                event_chain_info = self.prepare_chain_context(chain)

                chunk_ids = []
                for ent_id in chain:
                    ent = self.neo4j_utils.get_entity_by_id(ent_id)
                    if not ent:
                        continue
                    sc = ent.source_chunks or []
                    if sc:
                        chunk_ids.append(sc[0])
                chunk_ids = list(set(chunk_ids))

                related_context = ""
                if chunk_ids:
                    documents = self.vector_store.search_by_ids(chunk_ids)
                    contents = {getattr(doc, "content", "") for doc in documents if getattr(doc, "content", "")}
                    related_context = "\n".join(list(contents))

                raw = self.graph_analyzer.generate_event_plot(
                    event_chain_info=event_chain_info,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )
                result = json.loads(correct_json_format(raw))

                if not _to_bool(result.get("is_plot")):
                    return False

                plot_info = result.get("plot_info") or {}
                title = (plot_info.get("title") or "").strip()
                if not title:
                    title = f"æƒ…èŠ‚é“¾ï¼š{chain[0]}â†’{chain[-1]}"

                plot_info["id"] = _stable_plot_id(title, chain)
                plot_info["event_ids"] = chain
                plot_info["reason"] = result.get("reason", "")

                self.neo4j_utils.write_plot_to_neo4j(plot_data=plot_info)
                return True

            except Exception as e:
                print(f"[!] å¤„ç†äº‹ä»¶é“¾ {chain} æ—¶å‡ºé”™: {e}")
                return False

        chain_map, chain_failed = self._run_with_soft_timeout_and_retries(
            filtered_chains,
            work_fn=process_chain,                 # (chain) -> bool
            key_fn=lambda ch: tuple(ch),
            desc_label="å¹¶å‘ç”Ÿæˆæƒ…èŠ‚å›¾è°±",
            per_task_timeout=900.0,
            retries=2,
            retry_backoff=1.0,
            allow_placeholder_first_round=False,
            placeholder_fn=None,
            should_retry=lambda ok: not bool(ok)   # False/å¼‚å¸¸/è¶…æ—¶ -> é‡è¯•
        )

        success_count = sum(1 for v in chain_map.values() if v)
        print(f"[âœ“] æˆåŠŸç”Ÿæˆæƒ…èŠ‚æ•°é‡ï¼š{success_count}/{len(filtered_chains)} ï¼›ä»å¤±è´¥ {len(chain_failed)}")

        return

    # -----------------------------
    # ä¸»æµç¨‹3ï¼šæŠ½å–æƒ…èŠ‚é—´å…³ç³»ï¼ˆå†™EPGï¼‰
    # -----------------------------
    def generate_plot_relations(self):
        """
        åŸºäºå€™é€‰æƒ…èŠ‚å¯¹ï¼Œåˆ¤å®šå¹¶å†™å…¥æƒ…èŠ‚é—´å…³ç³»ã€‚
        å…³ç³»é›†ï¼š
        - æœ‰å‘ï¼šPLOT_PREREQUISITE_FOR, PLOT_ADVANCES, PLOT_BLOCKS, PLOT_RESOLVES
        - æ— å‘ï¼šPLOT_CONFLICTS_WITH, PLOT_PARALLELS
        äº§ç‰©ï¼šEPG/plot_relations_created.json
        """
        # é¢„å¤„ç†ï¼šå‘é‡ã€GDS å›¾ä¸åµŒå…¥
        self.neo4j_utils.process_all_embeddings(entity_types=[self.meta["section_label"]])
        self.neo4j_utils.create_event_plot_graph()
        self.neo4j_utils.run_node2vec()

        # å¬å›å€™é€‰æƒ…èŠ‚å¯¹
        all_plot_pairs = self.neo4j_utils.get_plot_pairs(threshold=0)
        print("[âœ“] å¾…åˆ¤å®šæƒ…èŠ‚å…³ç³»æ•°é‡ï¼š", len(all_plot_pairs))

        DIRECTED = {
            "PLOT_PREREQUISITE_FOR",
            "PLOT_ADVANCES",
            "PLOT_BLOCKS",
            "PLOT_RESOLVES",
        }
        UNDIRECTED = {
            "PLOT_CONFLICTS_WITH",
            "PLOT_PARALLELS",
        }
        VALID_TYPES = DIRECTED | UNDIRECTED | {"None", None}

        # å¹¶å‘å‚æ•°
        PER_TASK_TIMEOUT = 900.0
        MAX_RETRIES = 2
        RETRY_BACKOFF = 1.0

        def _make_edge(src_id, tgt_id, rtype, confidence, reason):
            return {
                "src": src_id,
                "tgt": tgt_id,
                "relation_type": rtype,
                "confidence": float(confidence) if confidence is not None else 0.0,
                "reason": reason or ""
            }

        # å·¥ä½œå‡½æ•°ï¼šè¿”å› {"status": "ok"|"none"|"error", "edges": List[edge], "reason": str}
        def _work(pair: dict) -> dict:
            try:
                plot_A_info = self.neo4j_utils.get_entity_info(
                    pair["src"], "æƒ…èŠ‚", contain_properties=True, contain_relations=True
                )
                plot_B_info = self.neo4j_utils.get_entity_info(
                    pair["tgt"], "æƒ…èŠ‚", contain_properties=True, contain_relations=True
                )

                # LLM/è§„åˆ™åˆ¤å®š
                result = self.graph_analyzer.extract_plot_relation(
                    plot_A_info, plot_B_info, self.system_prompt_text
                )

                # å°è¯•ä¿®æ­£/è§£æ JSON
                try:
                    result = json.loads(correct_json_format(result))
                except Exception:
                    if not isinstance(result, dict):
                        raise

                rtype = result.get("relation_type")
                direction = result.get("direction", None)  # "A->B"/"B->A"ï¼ˆæœ‰å‘ï¼‰
                confidence = result.get("confidence", 0.0)
                reason = result.get("reason", "")

                # è¿‡æ»¤éæ³•ç±»å‹
                if rtype not in VALID_TYPES:
                    return {"status": "error", "edges": [], "reason": f"unknown relation_type: {rtype}"}

                # æ— å…³ç³»
                if rtype in {"None", None}:
                    return {"status": "none", "edges": [], "reason": "no-relation"}

                # ç”Ÿæˆè¾¹
                if rtype in DIRECTED:
                    if direction == "A->B":
                        edges = [_make_edge(pair["src"], pair["tgt"], rtype, confidence, reason)]
                    elif direction == "B->A":
                        edges = [_make_edge(pair["tgt"], pair["src"], rtype, confidence, reason)]
                    else:
                        # ç¼ºå°‘æˆ–éæ³•æ–¹å‘ï¼šä½œä¸ºé”™è¯¯ä»¥ä¾¿é‡è¯•
                        return {"status": "error", "edges": [], "reason": f"missing/invalid direction: {direction}"}
                else:  # æ— å‘ï¼šå†™åŒå‘è¾¹
                    edges = [
                        _make_edge(pair["src"], pair["tgt"], rtype, confidence, reason),
                        _make_edge(pair["tgt"], pair["src"], rtype, confidence, reason),
                    ]

                return {"status": "ok", "edges": edges, "reason": ""}

            except Exception as e:
                return {"status": "error", "edges": [], "reason": str(e)}

        # ä»…å¯¹ "error" é‡è¯•ï¼›"none" ä¸é‡è¯•ï¼›"ok" ä¸é‡è¯•
        def _should_retry(res: dict) -> bool:
            return isinstance(res, dict) and res.get("status") == "error"

        # ç»Ÿä¸€å¹¶å‘æ‰§è¡Œ
        res_map, still_failed = self._run_with_soft_timeout_and_retries(
            all_plot_pairs,
            work_fn=_work,
            key_fn=lambda p: (p["src"], p["tgt"]),
            desc_label="æŠ½å–æƒ…èŠ‚å…³ç³»",
            per_task_timeout=PER_TASK_TIMEOUT,
            retries=MAX_RETRIES,
            retry_backoff=RETRY_BACKOFF,
            allow_placeholder_first_round=False,
            placeholder_fn=None,
            should_retry=_should_retry
        )

        # æ±‡æ€»è¾¹
        edges_to_add = []
        for out in res_map.values():
            if isinstance(out, dict) and out.get("status") == "ok" and out.get("edges"):
                edges_to_add.extend(out["edges"])

        # æ‰¹é‡å†™å…¥ Neo4j + EPG
        if edges_to_add:
            self.neo4j_utils.create_plot_relations(edges_to_add)
            print(f"[âœ“] å·²åˆ›å»ºæƒ…èŠ‚å…³ç³» {len(edges_to_add)} æ¡")

            base = self.config.storage.event_plot_graph_path
            os.makedirs(base, exist_ok=True)
            with open(os.path.join(base, "plot_relations_created.json"), "w", encoding="utf-8") as f:
                json.dump(edges_to_add, f, ensure_ascii=False, indent=2)
        else:
            print("[!] æ²¡æœ‰ç”Ÿæˆä»»ä½•æƒ…èŠ‚å…³ç³»")

        if still_failed:
            print(f"[!] ä»å¤±è´¥çš„æƒ…èŠ‚å¯¹æ•°é‡ï¼š{len(still_failed)}")
