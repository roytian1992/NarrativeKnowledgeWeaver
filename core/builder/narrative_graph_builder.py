"""
äº‹ä»¶å› æœå›¾æ„å»ºå™¨
è´Ÿè´£æ„å»ºäº‹ä»¶å› æœå…³ç³»çš„æœ‰å‘å¸¦æƒå›¾å’Œæƒ…èŠ‚å•å…ƒå›¾è°±
"""

import json
import pickle
import networkx as nx
import hashlib
from typing import List, Dict, Tuple, Optional, Any, Set
from tqdm import tqdm
import time
from collections import Counter
from pathlib import Path
from core.model_providers.openai_llm import OpenAILLM
from core.utils.neo4j_utils import Neo4jUtils
from core.models.data import Entity
from core.builder.manager.graph_manager import GraphManager
from core.storage.graph_store import GraphStore
from core.storage.vector_store import VectorStore
from core.utils.prompt_loader import PromptLoader
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError, wait, FIRST_COMPLETED
from core.utils.format import correct_json_format, format_event_card
import logging
from collections import defaultdict
import os
from core.builder.graph_builder import DOC_TYPE_META


def remove_subset_paths(chains: List[List[str]]) -> List[List[str]]:
    """
    åˆ é™¤æ‰€æœ‰äº‹ä»¶é›†åˆæ˜¯å…¶ä»–é“¾äº‹ä»¶é›†åˆå­é›†çš„é“¾ï¼ˆå¿½ç•¥é¡ºåºã€è¿ç»­æ€§ï¼‰
    """
    filtered = []
    for i, chain in enumerate(chains):
        set_chain = set(chain)
        remove = False
        for j, other in enumerate(chains):
            if i == j:
                continue
            if set_chain.issubset(set(other)) and len(set_chain) < len(set(other)):
                remove = True
                break
        if not remove:
            filtered.append(chain)
    return filtered


def jaccard_similarity(set1: Set[str], set2: Set[str]) -> float:
    """è®¡ç®—ä¸¤ä¸ªé›†åˆçš„ Jaccard ç›¸ä¼¼åº¦ï¼ˆç”¨æˆ·è‡ªå®šä¹‰ï¼š|Aâˆ©B| / max(|A|, |B|)ï¼‰"""
    if not set1 and not set2:
        return 1.0
    return len(set1 & set2) / max([len(set1), len(set2)])


def remove_similar_paths(chains: List[List[str]], threshold: float = 0.8) -> List[List[str]]:
    """
    åˆ é™¤ä¸å·²ä¿ç•™é“¾ Jaccard ç›¸ä¼¼åº¦ >= threshold çš„é“¾
    """
    filtered: List[List[str]] = []
    for chain in chains:
        set_chain = set(chain)
        keep = True
        for kept in filtered:
            sim = jaccard_similarity(set_chain, set(kept))
            if sim >= threshold:
                keep = False
                break
        if keep:
            filtered.append(chain)
    return filtered


def get_frequent_subchains(chains: List[List[str]], min_length: int = 2, min_count: int = 2):
    """
    ç»Ÿè®¡äº‹ä»¶é“¾ä¸­å‡ºç°é¢‘ç‡è¾ƒé«˜çš„è¿ç»­å­é“¾
    Args:
        chains: äº‹ä»¶é“¾åˆ—è¡¨
        min_length: æœ€çŸ­å­é“¾é•¿åº¦
        min_count: æœ€å°‘å‡ºç°æ¬¡æ•°ï¼ˆé¢‘ç‡é˜ˆå€¼ï¼‰
    Returns:
        List[List[str]]  å­é“¾åˆ—è¡¨ï¼ˆæŒ‰é¢‘ç‡ä¸é•¿åº¦é™åºï¼‰
    """
    counter = Counter()

    for chain in chains:
        n = len(chain)
        # æšä¸¾æ‰€æœ‰è¿ç»­å­é“¾
        for i in range(n):
            for j in range(i + min_length, n + 1):
                sub = tuple(chain[i:j])
                counter[sub] += 1

    # è¿‡æ»¤ä½é¢‘
    results = [(list(sub), cnt) for sub, cnt in counter.items() if cnt >= min_count]
    # æŒ‰é¢‘ç‡æ’åº
    results.sort(key=lambda x: (-x[1], -len(x[0]), x[0]))

    return [pair[0] for pair in results]


class EventCausalityBuilder:
    """
    äº‹ä»¶å› æœå›¾æ„å»ºå™¨

    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä»Neo4jåŠ è½½å’Œæ’åºäº‹ä»¶
    2. é€šè¿‡è¿é€šä½“å’Œç¤¾åŒºè¿‡æ»¤äº‹ä»¶å¯¹
    3. ä½¿ç”¨extractoræ£€æŸ¥å› æœå…³ç³»
    4. æ„å»ºæœ‰å‘å¸¦æƒNetworkXå›¾
    5. ä¿å­˜å’ŒåŠ è½½å›¾æ•°æ®
    6. æ„å»ºPlotæƒ…èŠ‚å•å…ƒå›¾è°±
    """

    def __init__(self, config):
        """
        åˆå§‹åŒ–äº‹ä»¶å› æœå›¾æ„å»ºå™¨

        Args:
            config: KAGé…ç½®å¯¹è±¡
        """
        self.config = config
        self.llm = OpenAILLM(config)
        self.graph_store = GraphStore(config)
        self.vector_store = VectorStore(config, "documents")
        self.event_fallback = []  # å¯ä»¥åŠ å…¥Goalå’ŒAction

        self.doc_type = config.knowledge_graph_builder.doc_type

        if self.doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {self.doc_type}")
        self.meta = DOC_TYPE_META[self.doc_type]

        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, self.doc_type)
        self.neo4j_utils.load_embedding_model(config.graph_embedding)

        # åˆå§‹åŒ–Plotç›¸å…³ç»„ä»¶
        prompt_dir = config.knowledge_graph_builder.prompt_dir
        self.prompt_loader = PromptLoader(prompt_dir)
        settings_path = os.path.join(self.config.storage.graph_schema_path, "settings.json")
        if not os.path.exists(settings_path):
            settings_path = self.config.probing.default_background_path

        settings = json.load(open(settings_path, "r", encoding="utf-8"))

        self.system_prompt_text = self.construct_system_prompt(
            background=settings["background"],
            abbreviations=settings["abbreviations"]
        )

        self.graph_analyzer = GraphManager(config, self.llm)

        # Plotæ„å»ºé…ç½®å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
        self.causality_threshold = "Medium"
        self.logger = logging.getLogger(__name__)
        self.sorted_scenes = []
        self.event_list = []
        self.event2section_map = {}
        self.max_depth = config.event_plot_graph_builder.max_depth
        self.check_weakly_connected_components = True
        self.min_component_size = config.event_plot_graph_builder.min_connected_component_size
        self.max_workers = config.event_plot_graph_builder.max_workers
        self.max_iteration = config.event_plot_graph_builder.max_iterations
        self.check_weakly_connected_components = config.event_plot_graph_builder.check_weakly_connected_components
        self.max_num_triangles = config.event_plot_graph_builder.max_num_triangles

        # å› æœå…³ç³»å¼ºåº¦åˆ°æƒé‡çš„æ˜ å°„
        self.event_cards: Dict[str, Dict[str, Any]] = {}

        self.logger.info("EventCausalityBuilderåˆå§‹åŒ–å®Œæˆ")

    def construct_system_prompt(self, background, abbreviations):
        background_info = self.get_background_info(background, abbreviations)

        if self.doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel"

        system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})
        return system_prompt_text

    def get_background_info(self, background, abbreviations):
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" if background else ""

        # ---------- 2) ç¼©å†™è¡¨ï¼ˆé”®åå®½å®¹ï¼‰ ----------
        def fmt(item: dict) -> str:
            """
            å°†ä¸€ä¸ªç¼©å†™é¡¹è½¬ä¸º Markdown åˆ—è¡¨æ¡ç›®ã€‚ä»»ä½•å­—æ®µéƒ½å¯é€‰ï¼Œæ ‡é¢˜å­—æ®µä¼˜å…ˆçº§ä¸ºï¼š
            abbr > full > å…¶ä»–å­—æ®µ > N/A
            """
            if not isinstance(item, dict):
                return ""

            # æ ‡é¢˜å­—æ®µä¼˜å…ˆçº§
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )

            # å‰©ä¸‹å­—æ®µå»é™¤æ ‡é¢˜å­—æ®µ
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())

            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(item) for item in abbreviations if isinstance(item, dict))

        if background and abbr_block:
            background_info = f"{bg_block}\n{abbr_block}"
        else:
            background_info = bg_block or abbr_block

        return background_info

    def build_event_list(self) -> List[Entity]:
        """
        æ„å»ºæ’åºåçš„äº‹ä»¶åˆ—è¡¨

        Returns:
            æ’åºåçš„äº‹ä»¶åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æ„å»ºäº‹ä»¶åˆ—è¡¨...")

        # 1. è·å–æ‰€æœ‰åœºæ™¯å¹¶æ’åº
        section_entities = self.neo4j_utils.search_entities_by_type(
            entity_type=self.meta["section_label"]
        )

        self.sorted_sections = sorted(
            section_entities,
            key=lambda e: int(e.properties.get("order", 99999))
        )

        print(f"âœ… æ‰¾åˆ° {len(self.sorted_sections)} ä¸ªsection")

        # 2. ä»åœºæ™¯ä¸­æå–äº‹ä»¶
        event_list = []
        event2section_map = {}

        for section in tqdm(self.sorted_sections, desc="æå–åœºæ™¯ä¸­çš„äº‹ä»¶"):
            # ä¼˜å…ˆæŸ¥æ‰¾äº‹ä»¶
            results = self.neo4j_utils.search_related_entities(
                source_id=section.id,
                predicate=self.meta["contains_pred"],
                entity_types=["Event"],
                return_relations=False
            )

            # å¦‚æœåœºæ™¯ä¸­æ²¡æœ‰äº‹ä»¶ï¼Œåˆ™ç”¨åŠ¨ä½œæˆ–è€…ç›®æ ‡æ¥å¡«å……ï¼ˆå¦‚æœä½ å¯ç”¨äº† fallbackï¼‰
            if not results and self.event_fallback:
                results = self.neo4j_utils.search_related_entities(
                    source_id=section.id,
                    relation_type=self.meta["contains_pred"],  # è‹¥æ–¹æ³•å…¼å®¹ aliasï¼Œè¿™é‡Œå¯ä¿ç•™
                    entity_types=self.event_fallback,
                    return_relations=False
                )

            for result in results:
                if result.id not in event2section_map:
                    event2section_map[result.id] = section.id
                    event_list.append(result)

        self.event_list = event_list
        self.event2section_map = event2section_map

        print(f"âœ… æ„å»ºå®Œæˆï¼Œå…±æ‰¾åˆ° {len(event_list)} ä¸ªäº‹ä»¶")
        return event_list

    # =========================
    # äº‹ä»¶å¡ç‰‡å¹¶å‘é¢„ç”Ÿæˆï¼ˆæ–°å¢ï¼‰
    # =========================
    def precompute_event_cards(
        self,
        events: List[Entity],
        per_task_timeout: float = 180,
        max_retries: int = 3,
        retry_timeout: float = 60.0,
    ) -> Dict[str, Dict[str, Any]]:
        """
        å¹¶å‘ä¸ºæ‰€æœ‰äº‹ä»¶ç”Ÿæˆ event_cardï¼š
        - è½¯è¶…æ—¶ + å¤šè½®é‡è¯•ï¼ˆä»…å¯¹å¤±è´¥é¡¹ï¼‰
        - ç”Ÿæˆç»“æœå†™å…¥ self.event_cards å¹¶è½ç›˜
        è¿”å›ï¼š{event_id: event_card}
        """
        from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED
        import time

        def _collect_related_context_by_section(ev: Entity) -> str:
            """
            ä¼˜å…ˆé€šè¿‡æ‰€å± section çš„ title å¬å›æ–‡æ¡£ï¼›è‹¥å¤±è´¥ï¼Œå›é€€åˆ°äº‹ä»¶çš„ source_chunksã€‚
            """
            ctx_set = set()

            # 1) é€šè¿‡ section æ ‡é¢˜æ£€ç´¢
            sec_id = self.event2section_map.get(ev.id)
            if sec_id:
                sec = self.neo4j_utils.get_entity_by_id(sec_id)
                titles = sec.properties.get(self.meta["title"], [])
                if isinstance(titles, str):
                    titles = [titles]
                for t in titles or []:
                    try:
                        docs = self.vector_store.search_by_metadata({"title": t})
                        for d in docs:
                            if getattr(d, "content", None):
                                ctx_set.add(d.content)
                    except Exception:
                        pass

            # 2) å›é€€ï¼šäº‹ä»¶è‡ªå·±çš„ source_chunks
            if not ctx_set:
                try:
                    node = self.neo4j_utils.get_entity_by_id(ev.id)
                    chunk_ids = set((node.source_chunks or [])[:50])  # å®‰å…¨ä¸Šé™
                    if chunk_ids:
                        docs = self.vector_store.search_by_ids(list(chunk_ids))
                        for d in docs:
                            if getattr(d, "content", None):
                                ctx_set.add(d.content)
                except Exception:
                    pass

            return "\n".join(ctx_set)

        def _build_one(ev: Entity) -> Tuple[str, Dict[str, Any]]:
            # äº‹ä»¶è‡ªèº«ç»“æ„åŒ–ä¿¡æ¯ï¼ˆå«å±æ€§/å…³ç³»ï¼‰
            info = self.neo4j_utils.get_entity_info(
                ev.id, entity_type="äº‹ä»¶",
                contain_properties=True, contain_relations=True
            )
            related_ctx = _collect_related_context_by_section(ev)

            # ç”Ÿæˆå¡ç‰‡
            out = self.graph_analyzer.generate_event_context(info, related_ctx)
            card = json.loads(correct_json_format(out))["event_card"]
            card = format_event_card(card)
            return ev.id, card

        def _run_batch(evts: List[Entity], timeout: float, allow_placeholder: bool, desc: str):
            """é€šç”¨å¹¶å‘è·‘ä¸€æ‰¹ï¼›è¿”å› (batch_map, failed_ids)ã€‚"""
            results: Dict[str, Any] = {}
            failed: Set[str] = set()

            executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="card")
            try:
                fut_info = {}
                for ev in evts:
                    f = executor.submit(_build_one, ev)
                    fut_info[f] = {"start": time.monotonic(), "event": ev}

                pbar = tqdm(total=len(fut_info), desc=desc, ncols=100)
                pending = set(fut_info.keys())

                while pending:
                    done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)

                    # æ”¶é›†å·²å®Œæˆ
                    for f in done:
                        ev = fut_info[f]["event"]
                        try:
                            eid, card = f.result()
                            results[eid] = card
                        except Exception:
                            failed.add(ev.id)
                            if allow_placeholder:
                                # å ä½ skeletonï¼Œä¿è¯æœ‰å€¼ï¼›å­˜ä¸º str é¿å…æ‹¼æ¥æŠ¥é”™
                                skeleton = {
                                    "name": ev.properties.get("name") or ev.name or f"event_{ev.id}",
                                    "summary": "",
                                    "time_hint": "unknown",
                                    "locations": [],
                                    "participants": [],
                                    "action": "",
                                    "outcomes": [],
                                    "evidence": ""
                                }
                                results[ev.id] = json.dumps(skeleton, ensure_ascii=False)
                        pbar.update(1)
                        fut_info.pop(f, None)

                    # è½¯è¶…æ—¶
                    now = time.monotonic()
                    to_forget = []
                    for f in list(pending):
                        start = fut_info[f]["start"]
                        if now - start >= timeout:
                            ev = fut_info[f]["event"]
                            f.cancel()
                            failed.add(ev.id)
                            if allow_placeholder:
                                skeleton = {
                                    "name": ev.properties.get("name") or ev.name or f"event_{ev.id}",
                                    "summary": "",
                                    "time_hint": "unknown",
                                    "locations": [],
                                    "participants": [],
                                    "action": "",
                                    "outcomes": [],
                                    "evidence": ""
                                }
                                results[ev.id] = json.dumps(skeleton, ensure_ascii=False)
                            pbar.update(1)
                            to_forget.append(f)

                    for f in to_forget:
                        pending.remove(f)
                        fut_info.pop(f, None)

                pbar.close()
            finally:
                executor.shutdown(wait=False, cancel_futures=True)

            return results, failed

        # === é¦–è½®ï¼šå…è®¸å ä½ ===
        head_map, failed_ids = _run_batch(
            events, timeout=per_task_timeout, allow_placeholder=True, desc="é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡ï¼ˆé¦–è½®ï¼‰"
        )
        card_map = dict(head_map)

        # === é‡è¯•ï¼šä»…å¯¹å¤±è´¥é¡¹ï¼ŒæˆåŠŸåˆ™è¦†ç›–å ä½ ===
        need_ids = list(failed_ids)
        for attempt in range(1, max_retries + 1):
            if not need_ids:
                break
            # è½»å¾®é€€é¿
            try:
                time.sleep(min(2 ** (attempt - 1), 5.0))
            except Exception:
                pass

            id2evt = {e.id: e for e in events}
            retry_evts = [id2evt[i] for i in need_ids if i in id2evt]
            retry_map, retry_failed = _run_batch(
                retry_evts, timeout=retry_timeout, allow_placeholder=False,
                desc=f"é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡ï¼ˆé‡è¯• {attempt}/{max_retries}ï¼‰"
            )

            # è¦†ç›–æˆåŠŸé¡¹ï¼ˆæ›¿æ¢æ‰é¦–è½®å ä½ï¼‰
            for eid, card in retry_map.items():
                card_map[eid] = card

            need_ids = list(retry_failed)

        # å†™å…¥å†…å­˜ + è½ç›˜
        self.event_cards = card_map
        base = self.config.storage.knowledge_graph_path
        os.makedirs(base, exist_ok=True)
        with open(os.path.join(base, "event_cards.json"), "w", encoding="utf-8") as f:
            json.dump(self.event_cards, f, ensure_ascii=False, indent=2)

        print(f"ğŸ—‚ï¸ äº‹ä»¶å¡ç‰‡ç”Ÿæˆå®Œæˆï¼šæˆåŠŸ {len(card_map)} / æ€»è®¡ {len(events)}ï¼›ä»ç¼ºå¤± {len(need_ids)}")
        return card_map


    def filter_event_pairs_by_community(
        self,
        events: List[Entity],
        max_depth: int = 3
    ) -> List[Tuple[Entity, Entity]]:
        """
        åˆ©ç”¨ Neo4j ä¸­ Louvain ç»“æœç›´æ¥ç­›é€‰åŒç¤¾åŒºä¸” max_depth å†…å¯è¾¾çš„äº‹ä»¶å¯¹
        """
        # æŠŠäº‹ä»¶ ID åšæˆé›†åˆï¼Œä¾¿äºåé¢å®ä½“æ˜ å°„
        id2entity = {e.id: e for e in events}

        pairs = self.neo4j_utils.fetch_event_pairs_same_community()
        filtered_pairs = []
        for row in pairs:
            src_id, dst_id = row["srcId"], row["dstId"]
            if src_id in id2entity and dst_id in id2entity:
                filtered_pairs.append((id2entity[src_id], id2entity[dst_id]))

        print(f"[âœ“] åŒç¤¾åŒºäº‹ä»¶å¯¹: {len(filtered_pairs)}")
        return filtered_pairs

    def write_event_cause_edges(self, causality_results):
        rows = []
        for (src_id, dst_id), res in causality_results.items():
            rel = (res.get("relation") or "").upper()
            if rel != "NONE":
                confidence = float(res.get("confidence", 0.3) or 0.0)
                rows.append({
                    "srcId": src_id,
                    "dstId": dst_id,
                    "confidence": confidence,
                    "reason": res.get("reason", ""),
                    "predicate": res.get("relation", "NONE")
                })
        self.neo4j_utils.write_event_causes(rows)

    def check_causality_batch(
        self,
        pairs: List[Tuple[Entity, Entity]]
    ) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        å¹¶å‘æ£€æŸ¥äº‹ä»¶å¯¹çš„å› æœå…³ç³»ï¼ˆè½¯è¶…æ—¶ + å¤±è´¥æ”¶é›† + æœ«å°¾å¤šè½®é‡è¯•ï¼‰
        - ä¾èµ– self.event_cardsï¼ˆåœ¨ä¸»æµç¨‹é¢„ç”Ÿæˆï¼‰ï¼Œç¼ºå¤±é¡¹ä¼šå…œåº•å³æ—¶è¡¥å»ºä¸€æ¬¡
        - é¦–è½®ï¼šå®Œæˆå³æ”¶é›†ï¼›è¶…æ—¶/å¼‚å¸¸ -> å…ˆå ä½ + è®°å…¥é‡è¯•é˜Ÿåˆ—
        - æœ«å°¾ï¼šä»…å¯¹å¤±è´¥é¡¹åš N è½®é‡è¯•ï¼›æˆåŠŸå³è¦†ç›–æ—§ç»“æœ
        - è¿”å›ï¼š{(src_id, tgt_id): result_dict}
        """

        PER_TASK_TIMEOUT = 1800  # ç§’ï¼›é¦–è½®è½¯è¶…æ—¶
        MAX_RETRIES = 2          # é‡è¯•è½®æ•°ï¼Œå¯è°ƒ
        RETRY_BACKOFF = 2.0      # æŒ‡æ•°å›é€€åº•æ•°ï¼š1, 2, 4...ç§’
        RETRY_TIMEOUT = 600      # é‡è¯•è½®çš„å•ä»»åŠ¡è½¯è¶…æ—¶ï¼ˆå¯æ¯”é¦–è½®æ›´çŸ­/æ›´é•¿ï¼‰

        def _make_result(src_event, tgt_event,
                         relation="NONE",
                         reason="",
                         temporal_order="Unknown",
                         confidence=0.0,
                         raw_result="",
                         timeout=False) -> Dict[str, Any]:
            res = {
                "src_event": src_event,
                "tgt_event": tgt_event,
                "relation": relation,               # CAUSES | INDIRECT_CAUSES | PART_OF | NONE
                "reason": reason,
                "temporal_order": temporal_order,   # E1_before_E2 | E2_before_E1 | Overlap_or_Simultaneous | Unknown
                "confidence": float(confidence) if confidence is not None else 0.0,
                "raw_result": raw_result
            }
            if timeout:
                res["causality_timeout"] = True
            return res

        def _get_common_neighbor_info(src_id, tgt_id):
            commons = self.neo4j_utils.get_common_neighbors(src_id, tgt_id, limit=50)
            info = "ä¸¤ä¸ªäº‹ä»¶å…·æœ‰çš„å…±åŒé‚»å±…çš„ä¿¡æ¯ä¸ºï¼š\n"
            if not commons:
                return info + "æ— "
            for ent_ in commons:
                try:
                    ent_type = "/".join(ent_.type) if isinstance(ent_.type, (list, set, tuple)) else str(ent_.type)
                except Exception:
                    ent_type = "Unknown"
                info += f"- å®ä½“åç§°ï¼š{ent_.name}ï¼Œå®ä½“ç±»å‹ï¼š{ent_type}ï¼Œç›¸å…³æè¿°ä¸ºï¼š{ent_.description}\n"
            return info

        def _ensure_card(e: Entity, info_text: str) -> Dict[str, Any]:
            """ä¼˜å…ˆè¯»ç¼“å­˜ï¼›ç¼ºå¤±æ—¶å…œåº•æ„å»ºä¸€æ¬¡å¹¶å†™å›ç¼“å­˜ï¼ˆæå°‘è§¦å‘ï¼‰"""
            if e.id in self.event_cards:
                return self.event_cards[e.id]
            out = self.graph_analyzer.generate_event_context(info_text, "")
            card = json.loads(correct_json_format(out))["event_card"]
            card = format_event_card(card)
            self.event_cards[e.id] = card
            return card

        def _process_pair(pair: Tuple[Entity, Entity]):
            src_event, tgt_event = pair
            pair_key = (src_event.id, tgt_event.id)
            try:
                # 1) è·å–äº‹ä»¶ä¿¡æ¯æ–‡æœ¬ï¼ˆå«å±æ€§ä¸å…³ç³»ï¼‰
                info_1 = self.neo4j_utils.get_entity_info(
                    src_event.id, entity_type="äº‹ä»¶",
                    contain_properties=True, contain_relations=True
                )
                info_2 = self.neo4j_utils.get_entity_info(
                    tgt_event.id, entity_type="äº‹ä»¶",
                    contain_properties=True, contain_relations=True
                )

                # 2) å…±åŒé‚»å±…ä¸ä¸Šä¸‹æ–‡ï¼ˆæ­¤å¤„ä¸»è¦æ‹¼æ¥ç»“æ„åŒ–æ–‡æœ¬ + å…±åŒé‚»å±…æ‘˜è¦ï¼‰
                related_context = info_1 + "\n" + info_2 + "\n" + _get_common_neighbor_info(src_event.id, tgt_event.id)

                # 3) è¯»å–é¢„ç”Ÿæˆçš„ event_cardï¼ˆç¼ºå¤±åˆ™å…œåº•æ„å»ºä¸€æ¬¡ï¼‰
                src_event_card = _ensure_card(src_event, info_1)
                tgt_event_card = _ensure_card(tgt_event, info_2)

                # 4) LLM åˆ¤å®š
                result_json = self.graph_analyzer.check_event_causality(
                    src_event_card, tgt_event_card,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )

                # 5) è§£æ JSON
                if isinstance(result_json, dict):
                    result_dict = result_json
                    raw_str = json.dumps(result_json, ensure_ascii=False)
                else:
                    result_dict = json.loads(correct_json_format(result_json))
                    raw_str = result_json

                relation = result_dict.get("relation", "NONE")
                reason = result_dict.get("reason", "")
                temporal_order = result_dict.get("temporal_order", "Unknown")
                confidence = result_dict.get("confidence", 0.3)

                return pair_key, _make_result(
                    src_event, tgt_event,
                    relation=relation,
                    reason=reason,
                    temporal_order=temporal_order,
                    confidence=confidence,
                    raw_result=raw_str,
                    timeout=False
                )

            except Exception as e:
                # å¤±è´¥é™çº§ï¼šNONE + é”™è¯¯ä¿¡æ¯ï¼ˆé¦–è½®ä¼šè¿›å…¥é‡è¯•é˜Ÿåˆ—ï¼‰
                return pair_key, _make_result(
                    src_event, tgt_event,
                    relation="NONE",
                    reason=f"æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}",
                    temporal_order="Unknown",
                    confidence=0.0,
                    raw_result="",
                    timeout=True  # ç»Ÿä¸€å½“ä½œéœ€é‡è¯•
                )

        def _run_batch(pairs_to_run: List[Tuple[Entity, Entity]], per_task_timeout: float,
                       allow_placeholders: bool, desc: str):
            """é€šç”¨å¹¶å‘è·‘ä¸€æ‰¹ï¼›è¿”å› (batch_results, failed_keys)ã€‚"""
            results_batch: Dict[Tuple[str, str], Dict[str, Any]] = {}
            failed_keys: set = set()

            executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="causal")
            try:
                fut_info: Dict[Any, Dict[str, Any]] = {}
                for pair in pairs_to_run:
                    f = executor.submit(_process_pair, pair)
                    fut_info[f] = {"start": time.monotonic(), "pair": pair}

                pbar = tqdm(total=len(fut_info), desc=desc, ncols=100)
                pending = set(fut_info.keys())

                while pending:
                    done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)

                    # æ”¶é›†å·²å®Œæˆ
                    for f in done:
                        pair = fut_info[f]["pair"]
                        src_event, tgt_event = pair
                        key = (src_event.id, tgt_event.id)
                        try:
                            k2, res = f.result()  # å·²å®Œæˆï¼Œä¸é˜»å¡
                            results_batch[k2] = res
                            # æ ‡è®°å¤±è´¥é”®ï¼ˆå¦‚æœæ˜¯è¶…æ—¶æ ‡è®° true ä¹Ÿè¿›å…¥å¤±è´¥é›†ï¼‰
                            if res.get("causality_timeout"):
                                failed_keys.add(k2)
                        except Exception as e:
                            # æ”¶é›†é˜¶æ®µå¼‚å¸¸
                            res = _make_result(
                                src_event, tgt_event,
                                relation="NONE",
                                reason=f"ç»“æœæ”¶é›†å‡ºé”™: {e}",
                                temporal_order="Unknown",
                                confidence=0.0,
                                raw_result="",
                                timeout=True
                            )
                            results_batch[key] = res
                            failed_keys.add(key)
                        pbar.update(1)
                        fut_info.pop(f, None)

                    # æ£€æŸ¥è½¯è¶…æ—¶
                    now = time.monotonic()
                    to_forget = []
                    for f in list(pending):
                        start = fut_info[f]["start"]
                        if now - start >= per_task_timeout:
                            pair = fut_info[f]["pair"]
                            src_event, tgt_event = pair
                            key = (src_event.id, tgt_event.id)
                            # è¯•å›¾å–æ¶ˆï¼šæœªå¯åŠ¨çš„ä¼šæˆåŠŸï¼›è¿è¡Œä¸­è¿”å› Falseï¼Œä½†æˆ‘ä»¬ä¸å†ç­‰å¾…
                            f.cancel()
                            if allow_placeholders:
                                res = _make_result(
                                    src_event, tgt_event,
                                    relation="NONE",
                                    reason="è½¯è¶…æ—¶ï¼Œå ä½è¿”å›",
                                    temporal_order="Unknown",
                                    confidence=0.0,
                                    raw_result="",
                                    timeout=True
                                )
                                results_batch[key] = res
                            failed_keys.add(key)
                            pbar.update(1)
                            to_forget.append(f)

                    for f in to_forget:
                        pending.remove(f)
                        fut_info.pop(f, None)

                pbar.close()
            finally:
                executor.shutdown(wait=False, cancel_futures=True)

            return results_batch, failed_keys

        # ------- ä¸»æµç¨‹ -------
        print(f"ğŸ” å¼€å§‹å¹¶å‘æ£€æŸ¥ {len(pairs)} å¯¹äº‹ä»¶çš„å› æœå…³ç³»...")

        # å»ºç«‹ key->pair æ˜ å°„ï¼Œä¾¿äºé‡è¯•
        key2pair: Dict[Tuple[str, str], Tuple[Entity, Entity]] = {
            (src.id, tgt.id): (src, tgt) for (src, tgt) in pairs
        }

        # é¦–è½®ï¼šå…è®¸å ä½ï¼ˆä¿è¯â€œå®Œæˆå³æ”¶é›†â€ä½“éªŒï¼‰
        head_results, failed_keys = _run_batch(
            pairs_to_run=pairs,
            per_task_timeout=PER_TASK_TIMEOUT,
            allow_placeholders=True,
            desc="å¹¶å‘æ£€æŸ¥å› æœå…³ç³»ï¼ˆé¦–è½®ï¼‰"
        )

        results: Dict[Tuple[str, str], Dict[str, Any]] = dict(head_results)

        # éœ€è¦é‡è¯•çš„ keyï¼štimeout æˆ–å¼‚å¸¸
        def _needs_retry(key: Tuple[str, str], res: Dict[str, Any]) -> bool:
            # è§„åˆ™ï¼šè¶…æ—¶ æˆ– reason å«â€œå‡ºé”™â€
            if res.get("causality_timeout"):
                return True
            reason = (res.get("reason") or "").strip()
            return ("å‡ºé”™" in reason)

        needs_retry = [k for k in failed_keys if k in results and _needs_retry(k, results[k])]
        print(f"â© é¦–è½®åå‡†å¤‡é‡è¯•ï¼š{len(needs_retry)} / {len(pairs)}")

        # å¤šè½®é‡è¯•ï¼šä¸å†å†™å ä½ï¼ŒæˆåŠŸæ‰è¦†ç›–æ—§å€¼ï¼›ä»ç„¶ä½¿ç”¨è½¯è¶…æ—¶ï¼Œä½†é€šå¸¸æ›´çŸ­
        for attempt in range(1, MAX_RETRIES + 1):
            if not needs_retry:
                break
            # æŒ‡æ•°å›é€€ï¼ˆè½»å¾®ä¼‘çœ ï¼Œé¿å…ç¬æ—¶æŠ–åŠ¨ï¼‰
            backoff = (RETRY_BACKOFF ** (attempt - 1))
            try:
                time.sleep(min(backoff, 5.0))
            except Exception:
                pass

            pairs_for_retry = [key2pair[k] for k in needs_retry if k in key2pair]
            batch_desc = f"å¹¶å‘æ£€æŸ¥å› æœå…³ç³»ï¼ˆé‡è¯•ç¬¬ {attempt}/{MAX_RETRIES} è½®ï¼‰"
            retry_results, retry_failed = _run_batch(
                pairs_to_run=pairs_for_retry,
                per_task_timeout=RETRY_TIMEOUT,
                allow_placeholders=False,  # é‡è¯•ä¸å†™å ä½ï¼Œåªæ”¶æˆåŠŸä¸å¤±è´¥é›†åˆ
                desc=batch_desc
            )

            # è¦†ç›–æˆåŠŸé¡¹ï¼ˆé‚£äº›æ²¡æœ‰ timeout æ ‡è®°ã€ä¸”æ— â€œå‡ºé”™â€å­—æ ·çš„ï¼‰
            improved = 0
            for k, v in retry_results.items():
                if not _needs_retry(k, v):
                    results[k] = v
                    improved += 1
                else:
                    # ä»å¤±è´¥çš„ä¿æŒæ—§å€¼ï¼ˆæ—§å€¼å¯èƒ½æ˜¯å ä½ï¼‰
                    pass

            print(f"ğŸ” é‡è¯•ç¬¬ {attempt} è½®ï¼šæˆåŠŸè¦†ç›– {improved} é¡¹ï¼Œä»éœ€é‡è¯• {len(retry_failed)} é¡¹")

            # ä¸‹ä¸€è½®åªå¯¹ä»å¤±è´¥çš„è¿›è¡Œ
            needs_retry = [k for k in retry_failed]

        print(f"âœ… å› æœå…³ç³»å¹¶å‘æ£€æŸ¥å®Œæˆï¼ˆæˆåŠŸ {len(results) - len(needs_retry)} / {len(pairs)}ï¼Œä»å¤±è´¥ {len(needs_retry)}ï¼‰")

        # æœ€ç»ˆè‹¥ä»æœ‰å¤±è´¥é¡¹ï¼Œå¯åœ¨è¿™é‡Œç»Ÿä¸€æ‰“ä¸Šâ€œfinal_fallbackâ€æ ‡è®°ï¼ˆå¯é€‰ï¼‰
        for k in needs_retry:
            if k in results:
                r = results[k]
                r["final_fallback"] = True
                r["retries"] = MAX_RETRIES

        return results

    def sort_event_pairs_by_section_order(
        self, pairs: List[Tuple[Entity, Entity]]
    ) -> List[Tuple[Entity, Entity]]:
        def get_order(evt: Entity) -> int:
            sec_id = self.event2section_map.get(evt.id)
            if not sec_id:
                return 99999
            sec = self.neo4j_utils.get_entity_by_id(sec_id)
            return int(sec.properties.get("order", 99999))

        ordered = []
        for e1, e2 in pairs:
            ordered.append((e1, e2) if get_order(e1) <= get_order(e2) else (e2, e1))
        return ordered

    def initialize(self):
        # 1. åˆ›å»ºå­å›¾å’Œè®¡ç®—ç¤¾åŒºåˆ’åˆ†
        for relation_type in ["EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"]:
            self.neo4j_utils.delete_relation_type(relation_type)

        self.neo4j_utils.create_subgraph(
            graph_name="knowledge_graph",
            exclude_entity_types=[self.meta["section_label"]],
            exclude_relation_types=[self.meta["contains_pred"]],
            force_refresh=True
        )

        self.neo4j_utils.run_louvain(
            graph_name="knowledge_graph",
            write_property="community",
            force_run=True
        )

    def filter_pair_by_distance_and_similarity(self, pairs):
        filtered_pairs = []
        for pair in tqdm(pairs, desc="ç­›é€‰èŠ‚ç‚¹å¯¹"):
            src_id, tgt_id = pair[0].id, pair[1].id
            reachable = self.neo4j_utils.check_nodes_reachable(
                src_id, tgt_id,
                excluded_rels=[self.meta["contains_pred"], "EVENT_CAUSES", "EVENT_INDIRECT_CAUSES", "EVENT_PART_OF"],
                max_depth=self.max_depth
            )
            if reachable:  # å¦‚æœèŠ‚ç‚¹é—´è·ç¦» <= max_depthï¼Œä¿ç•™ã€‚
                filtered_pairs.append(pair)
            else:
                score = self.neo4j_utils.compute_semantic_similarity(src_id, tgt_id)
                if score >= 0.5:  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼ä¸æ³¨é‡Šä¸€è‡´
                    filtered_pairs.append(pair)
        return filtered_pairs

    def build_event_causality_graph(
        self,
        limit_events: Optional[int] = None
    ) -> None:
        """
        å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹

        Args:
            limit_events: é™åˆ¶å¤„ç†çš„äº‹ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰

        Returns:
            æ„å»ºå®Œæˆçš„Neo4jæœ‰å‘å›¾
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹...")

        # 2. æ„å»ºäº‹ä»¶åˆ—è¡¨
        print("\nğŸ” æ„å»ºäº‹ä»¶åˆ—è¡¨...")
        event_list = self.build_event_list()

        # 3. é™åˆ¶äº‹ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if limit_events and limit_events < len(event_list):
            event_list = event_list[:limit_events]
            print(f"âš ï¸ é™åˆ¶å¤„ç†äº‹ä»¶æ•°é‡ä¸º: {limit_events}")

        # === âœ… æ–°å¢ï¼šå¹¶å‘é¢„ç”Ÿæˆæ‰€æœ‰äº‹ä»¶å¡ç‰‡ï¼ˆç¼“å­˜+è½ç›˜ï¼‰ ===
        print("\nğŸ§© å¹¶å‘é¢„ç”Ÿæˆäº‹ä»¶å¡ç‰‡...")
        self.precompute_event_cards(event_list)

        # 4. è¿‡æ»¤äº‹ä»¶å¯¹
        print("\nğŸ” è¿‡æ»¤äº‹ä»¶å¯¹...")
        filtered_pairs = self.filter_event_pairs_by_community(event_list, max_depth=self.max_depth)
        filtered_pairs = self.filter_pair_by_distance_and_similarity(filtered_pairs)
        filtered_pairs = self.sort_event_pairs_by_section_order(filtered_pairs)
        print("     æœ€ç»ˆå€™é€‰äº‹ä»¶å¯¹æ•°é‡ï¼š ", len(filtered_pairs))

        # 5. æ£€æŸ¥å› æœå…³ç³»ï¼ˆå°†ç›´æ¥è¯»å– self.event_cardsï¼‰
        print("\nğŸ” æ£€æŸ¥å› æœå…³ç³»...")
        causality_results = self.check_causality_batch(filtered_pairs)

        base = self.config.storage.knowledge_graph_path
        with open(os.path.join(base, "event_casality_results.pkl"), "wb") as f:
            pickle.dump(causality_results, f)

        with open(os.path.join(base, "event_cards.json"), "w", encoding="utf-8") as f:
            json.dump(self.event_cards, f, ensure_ascii=False, indent=2)

        # 6. å†™å› EVENT å…³ç³»
        print("\nğŸ”— å†™å›Eventé—´å…³ç³»...")
        self.write_event_cause_edges(causality_results)
        self.neo4j_utils.create_event_causality_graph("event_causality_graph", force_refresh=True)

    def detect_flattened_causal_patterns(self, edges: List[Dict]) -> List[Dict]:
        """
        ä»è¾¹é›†ä¸­å‘ç°ç±»ä¼¼ Aâ†’B, Aâ†’C, Aâ†’D ä¸”å­˜åœ¨ Bâ†’D çš„å†—ä½™ç»“æ„ï¼Œç”¨äºåç»­å› æœé“¾ç²¾ç‚¼

        Returns:
            List of {
                "source": A,
                "targets": [B, C, D],
                "internal_links": [(B, D), (C, D)]
            }
        """
        # æ„å»ºé‚»æ¥è¡¨å’Œåå‘è¾¹é›†åˆ
        forward_graph = defaultdict(set)
        edge_set = set()

        for edge in edges:
            sid = edge["sid"]
            tid = edge["tid"]
            forward_graph[sid].add(tid)
            edge_set.add((sid, tid))

        patterns = []

        for a, a_children in forward_graph.items():
            a_children = list(a_children)
            if len(a_children) < 2:
                continue  # è‡³å°‘ä¸¤ä¸ªæŒ‡å‘æ‰å¯èƒ½æ„æˆè¯¥æ¨¡å¼

            internal_links = []
            for i in range(len(a_children)):
                for j in range(len(a_children)):
                    if i == j:
                        continue
                    u, v = a_children[i], a_children[j]
                    if (u, v) in edge_set:
                        internal_links.append((u, v))

            if internal_links:
                patterns.append({
                    "source": a,
                    "targets": a_children,
                    "internal_links": internal_links
                })

        return patterns

    def filter_weak_edges_in_patterns(
        self,
        patterns: List[Dict],
        edge_map: Dict[Tuple[str, str], Dict],
        conf_threshold: float = 0.5
    ) -> List[Dict]:
        """
        ä» flattened patterns ä¸­å‰”é™¤ weight å’Œ confidence éƒ½åä½çš„è¾¹
        """
        cleaned_patterns = []
        for pat in patterns:
            src = pat["source"]
            targets = pat["targets"]
            internals = pat["internal_links"]

            # è¿‡æ»¤ source â†’ target è¾¹
            new_targets = []
            for t in targets:
                info = edge_map.get((src, t))
                confidence = info.get("confidence", 0) if info else 0
                if not info:
                    continue
                if confidence < conf_threshold:
                    new_targets.append(t)

            # è¿‡æ»¤ internal è¾¹
            new_internals = []
            for u, v in internals:
                info = edge_map.get((u, v))
                confidence = info.get("confidence", 0) if info else 0
                if not info:
                    continue
                if not confidence < conf_threshold:
                    new_internals.append((u, v))

            # ä¿ç•™ç»“æ„
            if len(new_targets) >= 2 and new_internals:
                cleaned_patterns.append({
                    "source": src,
                    "targets": new_targets,
                    "internal_links": new_internals
                })

        return cleaned_patterns

    def collect_removed_edges(self,
                              original_patterns: List[Dict],
                              filtered_patterns: List[Dict]
                              ) -> Set[Tuple[str, str]]:
        """
        æ¯”è¾ƒä¸¤ç»„ pattern ç»“æ„ï¼Œæ”¶é›†è¢«åˆ é™¤å¯¼è‡´ç»“æ„å˜åŒ–çš„è¾¹

        Returns:
            è¢«æ ‡è®°ä¸ºåˆ é™¤å€™é€‰çš„è¾¹é›†åˆï¼ˆsid, tidï¼‰
        """
        # æŠ½å–åŸå§‹ç»“æ„ä¸­çš„å…¨éƒ¨è¾¹
        def extract_edges(patterns: List[Dict]) -> Set[Tuple[str, str]]:
            edge_set = set()
            for pat in patterns:
                src = pat["source"]
                for tgt in pat["targets"]:
                    edge_set.add((src, tgt))
                edge_set.update(pat["internal_links"])
            return edge_set

        origin_edges = extract_edges(original_patterns)
        filtered_edges = extract_edges(filtered_patterns)

        removed_edges = origin_edges - filtered_edges
        print(f"[+] Found {len(removed_edges)} candidate edges removed due to pattern collapse")
        return list(removed_edges)  # åŸé€»è¾‘è¿”å› listï¼›ä¿æŒä¸€è‡´

    def filter_pattern(self, pattern, edge_map):
        source = pattern["source"]
        targets = pattern["targets"]
        internal_links = pattern["internal_links"]
        context_to_check = []
        for link in internal_links:
            mid_tgt_sim = self.neo4j_utils.compute_semantic_similarity(link[0], link[1])
            src_mid_sim = self.neo4j_utils.compute_semantic_similarity(source, link[0])
            src_tgt_sim = self.neo4j_utils.compute_semantic_similarity(source, link[1])

            mid_tgt_conf = edge_map.get((link[0], link[1]))["confidence"]
            src_mid_conf = edge_map.get((source, link[0]))["confidence"]
            src_tgt_conf = edge_map.get((source, link[1]))["confidence"]

            if (src_mid_sim > src_tgt_sim and mid_tgt_sim > src_tgt_sim) or (src_mid_conf > src_tgt_conf and mid_tgt_conf > src_tgt_conf):
                context_to_check.append({
                    "entities": [source, link[0], link[1]],
                    "details": [
                        {"edge": [source, link[0]], "similarity": src_mid_sim, "confidence": src_mid_conf},
                        {"edge": [source, link[1]], "similarity": src_tgt_sim, "confidence": src_tgt_conf},
                        {"edge": [link[0], link[1]], "similarity": mid_tgt_sim, "confidence": mid_tgt_conf},
                    ]
                })

        return context_to_check

    def prepare_context(self, pattern_detail):
        def _safe_str(x: Any) -> str:
            return x if isinstance(x, str) else ("" if x is None else str(x))

        event_details = self.neo4j_utils.get_event_details(pattern_detail["entities"])
        full_event_details = "ä¸‰ä¸ªäº‹ä»¶å®ä½“çš„æè¿°å¦‚ä¸‹ï¼š\n"

        for i, event_info in enumerate(event_details):
            event_id = event_info["event_id"]
            full_event_details += f"**äº‹ä»¶{i+1}çš„ç›¸å…³æè¿°å¦‚ä¸‹ï¼š**\näº‹ä»¶idï¼š{event_id}\n"

            # 1) å¯èƒ½è¿”å› Noneï¼šç»Ÿä¸€è½¬æˆå­—ç¬¦ä¸²
            background = self.neo4j_utils.get_entity_info(event_id, "äº‹ä»¶", True, True)
            background = _safe_str(background)

            # 2) event_properties å¯èƒ½ä¸º None / str(JSON) / dict
            props_raw = event_info.get("event_properties")
            if isinstance(props_raw, dict):
                event_props = props_raw
            elif isinstance(props_raw, str) and props_raw.strip():
                try:
                    event_props = json.loads(props_raw)
                    if not isinstance(event_props, dict):
                        event_props = {}
                except Exception:
                    event_props = {}
            else:
                event_props = {}

            # ä»…ä¿ç•™éç©ºå­—ç¬¦ä¸²å±æ€§
            non_empty_props = {k: v for k, v in event_props.items() if isinstance(v, str) and v.strip()}

            if non_empty_props:
                background += "\näº‹ä»¶çš„å±æ€§å¦‚ä¸‹ï¼š\n"
                for k, v in non_empty_props.items():
                    background += f"- {k}ï¼š{v}\n"

            if i + 1 != len(event_details):
                background += "\n"

            full_event_details += background

        # å…³ç³»ç»†èŠ‚å­—ç¬¦ä¸²
        full_relation_details = "å®ƒä»¬ä¹‹é—´å·²ç»å­˜åœ¨çš„å› æœå…³ç³»æœ‰ï¼š\n"
        relation_details = pattern_detail["details"]
        for i, relation_info in enumerate(relation_details):
            src, tgt = relation_info["edge"]
            # 3) get_relation_summary ä¹Ÿåšä¸€æ¬¡å…œåº•è½¬å­—ç¬¦ä¸²
            rel_summary = self.neo4j_utils.get_relation_summary(src, tgt, "EVENT_CAUSES")
            rel_summary = _safe_str(rel_summary)
            background = f"{i+1}. " + rel_summary
            background += f"\nå…³ç³»çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºï¼š{round(relation_info['similarity'], 4)}ï¼Œç½®ä¿¡åº¦ä¸ºï¼š{relation_info['confidence']}ã€‚"
            if i + 1 != len(relation_details):
                background += "\n\n"
            full_relation_details += background

        return full_event_details, full_relation_details


    def run_SABER(self):
        """
        æ‰§è¡ŒåŸºäºç»“æ„+LLMçš„å› æœè¾¹ç²¾ç®€ä¼˜åŒ–è¿‡ç¨‹
        """
        loop_count = 0
        removed_edges: List[Tuple[str, str]] = []  # é˜²æ­¢æœªå®šä¹‰å¼•ç”¨
        while True:
            print(f"\n===== [ç¬¬ {loop_count + 1} è½®ä¼˜åŒ–] =====")

            # === è·å–è¿é€šä½“ï¼ˆä¼˜å…ˆ SCCï¼Œå†é€‰ WCCï¼‰ ===
            scc_components = self.neo4j_utils.fetch_scc_components("event_causality_graph", 2)
            wcc_components = []
            if self.check_weakly_connected_components:
                wcc_components = self.neo4j_utils.fetch_wcc_components("event_causality_graph", self.min_component_size)

            connected_components = scc_components + wcc_components
            print(f"ğŸ“Œ å½“å‰è¿é€šä½“æ•°é‡ï¼šSCC={len(scc_components)}ï¼ŒWCC={len(wcc_components)}")

            # === æ„é€ æ‰€æœ‰ triangle å’Œè¾¹ä¿¡æ¯ ===
            all_triangles = []
            edge_map_global = {}

            for cc in connected_components:
                node_map, edges = self.neo4j_utils.load_connected_components_subgraph(cc)
                edge_map = {
                    (e["sid"], e["tid"]): {"confidence": e.get("confidence", 0.0)}
                    for e in edges
                }
                edge_map_global.update(edge_map)

                old_patterns = self.detect_flattened_causal_patterns(edges)
                new_patterns = self.filter_weak_edges_in_patterns(old_patterns, edge_map, conf_threshold=0.5)
                for pattern in new_patterns:
                    all_triangles += self.filter_pattern(pattern, edge_map)

            print(f"ğŸ”º æœ¬è½®éœ€åˆ¤æ–­çš„ä¸‰å…ƒå› æœç»“æ„æ•°é‡ï¼š{len(all_triangles)}")
            if len(all_triangles) >= self.max_num_triangles:
                print(f"âš ï¸ æ£€æµ‹åˆ°ä¸‰å…ƒç»“æ„æ•°é‡è¿‡å¤šï¼Œåªé€‰æ‹©å‰{self.max_num_triangles}ä¸ªè¿›è¡Œå¤„ç†ã€‚")
                all_triangles = all_triangles[:self.max_num_triangles]
                return

            # === âœ… æå‰é€€å‡ºæ¡ä»¶ ===
            if loop_count >= 1:
                if len(scc_components) == 0 and len(set(removed_edges)) == 0:
                    print("âœ… å›¾ç»“æ„å·²æ— å¼ºè¿é€šä½“ï¼Œä¸”æ— å¾…åˆ¤å®šä¸‰å…ƒç»“æ„ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
                    break
            elif loop_count >= self.max_iteration:
                break

            # === å¹¶å‘å¤„ç†ä¸‰å…ƒç»“æ„ ===
            removed_edges = []

            def process_triangle(triangle_):
                try:
                    event_details, relation_details = self.prepare_context(triangle_)
                    chunks = [self.neo4j_utils.get_entity_by_id(ent_id).source_chunks[0] for ent_id in triangle_["entities"]]
                    chunks = list(set(chunks))
                    documents = self.vector_store.search_by_ids(chunks)
                    results = {doc.content for doc in documents}
                    related_context = "\n".join(list(results))

                    output = self.graph_analyzer.evaluate_event_redundancy(
                        event_details, relation_details, self.system_prompt_text, related_context
                    )
                    output = json.loads(correct_json_format(output))
                    if output.get("remove_edge", False):
                        return (triangle_["entities"][0], triangle_["entities"][2])
                except Exception as e:
                    print(f"[âš ï¸ é”™è¯¯] Triangle åˆ¤æ–­å¤±è´¥: {triangle_['entities']}, é”™è¯¯ä¿¡æ¯: {str(e)}")
                return None

            print(f"ğŸ§  æ­£åœ¨å¹¶å‘åˆ¤æ–­ä¸‰å…ƒç»“æ„...")
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(process_triangle, tri) for tri in all_triangles]
                for f in tqdm(as_completed(futures), total=len(futures), desc="LLMåˆ¤æ–­"):
                    res = f.result()
                    if res:
                        removed_edges.append(res)

            print(f"âŒ æœ¬è½®å¾…å®šç§»é™¤è¾¹æ•°é‡ï¼š{len(set(removed_edges))}")

            # === åˆ é™¤è¾¹ ===
            for edge in removed_edges:
                self.neo4j_utils.delete_relation_by_ids(edge[0], edge[1], "EVENT_CAUSES")

            # === åˆ·æ–° GDS å›¾ ===
            self.neo4j_utils.create_event_causality_graph("event_causality_graph", min_confidence=0, force_refresh=True)
            loop_count += 1

    def get_all_event_chains(self, min_confidence: float = 0.0):
        """
        è·å–æ‰€æœ‰å¯èƒ½çš„äº‹ä»¶é“¾ï¼ˆä»èµ·ç‚¹åˆ°æ²¡æœ‰å‡ºè¾¹çš„ç»ˆç‚¹ï¼‰
        """
        starting_events = self.neo4j_utils.get_starting_events()
        chains = []
        for event in starting_events:
            all_chains = self.neo4j_utils.find_event_chain(event, min_confidence)
            chains.extend([chain for chain in all_chains if len(chain) >= 2])
        return chains

    def prepare_chain_context(self, chain):
        if len(chain) > 1:
            context = "äº‹ä»¶é“¾ï¼š" + "->".join(chain) + "\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        else:
            context = f"äº‹ä»¶ï¼š{chain[0]}" + "\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        for i, event in enumerate(chain):
            # context += f"äº‹ä»¶{i+1}ï¼š{event}\n" + self.neo4j_utils.get_entity_info(event, "äº‹ä»¶", False, True) + "\n"
            context += f"äº‹ä»¶{i+1}ï¼š{event}\n" + self.event_cards[event] + "\n"
        
        return context
    
    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_embedding_model(self.config.graph_embedding)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            entity_types=["Event", "Plot"]
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… äº‹ä»¶æƒ…èŠ‚å›¾å‘é‡æ„å»ºå®Œæˆ")

    def generate_plot_relations(self):
        """
        åŸºäºå€™é€‰æƒ…èŠ‚å¯¹ï¼Œåˆ¤å®šå¹¶å†™å…¥æƒ…èŠ‚é—´å…³ç³»ã€‚
        å…³ç³»é›†ï¼ˆæœ€ç»ˆç‰ˆï¼‰ï¼š
        - æœ‰å‘ï¼šPLOT_PREREQUISITE_FOR, PLOT_ADVANCES, PLOT_BLOCKS, PLOT_RESOLVES
        - æ— å‘ï¼šPLOT_CONFLICTS_WITH, PLOT_PARALLELS
        å…¼å®¹æ—§ç±»å‹ï¼šPLOT_CONTRIBUTES_TO / PLOT_SETS_UP â†’ ç»Ÿä¸€æ˜ å°„ä¸º PLOT_ADVANCES
        """
        # é¢„å¤„ç†ï¼šå‘é‡ã€GDS å›¾ä¸åµŒå…¥
        self.neo4j_utils.process_all_embeddings(entity_types=["Plot", self.meta["section_label"]])
        self.neo4j_utils.create_event_plot_graph()
        self.neo4j_utils.run_node2vec()

        # å¬å›å€™é€‰æƒ…èŠ‚å¯¹
        all_plot_pairs = self.neo4j_utils.get_plot_pairs(threshold=0)
        print("[âœ“] å¾…åˆ¤å®šæƒ…èŠ‚å…³ç³»æ•°é‡ï¼š", len(all_plot_pairs))

        # å…³ç³»ç±»å‹ä¸å‘åå…¼å®¹æ˜ å°„
        LEGACY_MAP = {
            "PLOT_CONTRIBUTES_TO": "PLOT_ADVANCES",
            "PLOT_SETS_UP": "PLOT_ADVANCES"
        }
        DIRECTED = {
            "PLOT_PREREQUISITE_FOR",
            "PLOT_ADVANCES",
            "PLOT_BLOCKS",
            "PLOT_RESOLVES",
        }
        UNDIRECTED = {
            "PLOT_CONFLICTS_WITH",
            "PLOT_PARALLELS",
        }
        VALID_TYPES = DIRECTED | UNDIRECTED | {"None", None}

        edges_to_add = []

        def _make_edge(src_id, tgt_id, rtype, confidence, reason):
            return {
                "src": src_id,
                "tgt": tgt_id,
                "relation_type": rtype,
                "confidence": float(confidence) if confidence is not None else 0.0,
                "reason": reason or ""
            }

        def _parse_direction_to_edge(pair, direction_str, rtype, confidence, reason):
            """
            å°† A/B æ–¹å‘æ˜ å°„ä¸ºçœŸå® src/tgt è¾¹ï¼›è¿”å› [edge] æˆ– []ã€‚
            direction_str: "A->B" / "B->A"
            """
            if direction_str == "A->B":
                return [_make_edge(pair["src"], pair["tgt"], rtype, confidence, reason)]
            elif direction_str == "B->A":
                return [_make_edge(pair["tgt"], pair["src"], rtype, confidence, reason)]
            else:
                # æ— æ•ˆæˆ–ç¼ºå¤±æ–¹å‘ï¼Œè·³è¿‡è¯¥å¯¹ï¼ˆä»…å¯¹æœ‰å‘å…³ç³»ç”Ÿæ•ˆï¼‰
                print(f"[!] è·³è¿‡ï¼šæœ‰å‘å…³ç³»ç¼ºå°‘æœ‰æ•ˆæ–¹å‘ direction={direction_str} pair={pair}")
                return []

        def process_pair(pair):
            try:
                plot_A_info = self.neo4j_utils.get_entity_info(pair["src"], "æƒ…èŠ‚", contain_properties=True, contain_relations=True)
                plot_B_info = self.neo4j_utils.get_entity_info(pair["tgt"], "æƒ…èŠ‚", contain_properties=True, contain_relations=True)

                # è°ƒç”¨å…³ç³»åˆ¤å®šï¼ˆLLM/è§„åˆ™ï¼‰
                result = self.graph_analyzer.extract_plot_relation(plot_A_info, plot_B_info, self.system_prompt_text)

                # å°è¯•ä¿®æ­£/è§£æ JSON
                try:
                    result = json.loads(correct_json_format(result))
                except Exception:
                    # è‹¥ç›´æ¥æ˜¯ dict åˆ™ä¿ç•™ï¼Œå¦åˆ™æŠ›å‡º
                    if isinstance(result, dict):
                        pass
                    else:
                        raise

                # è¯»å–å­—æ®µ
                rtype = result.get("relation_type")
                direction = result.get("direction", None)  # æœ‰å‘æ—¶åº”ä¸º "A->B" / "B->A"ï¼Œæ— å‘æˆ– None ç”¨ null
                confidence = result.get("confidence", 0.0)
                reason = result.get("reason", "")

                # å…¼å®¹æ—§æšä¸¾
                if rtype in LEGACY_MAP:
                    rtype = LEGACY_MAP[rtype]

                # è¿‡æ»¤æ— æ•ˆç±»å‹
                if rtype not in VALID_TYPES:
                    print(f"[!] æœªçŸ¥ relation_type={rtype}ï¼Œè·³è¿‡ pair={pair}")
                    return []

                # None æˆ–æ— å…³ç³»
                if rtype in {"None", None}:
                    return []

                pair_edges = []

                # æœ‰å‘å…³ç³»
                if rtype in DIRECTED:
                    pair_edges.extend(_parse_direction_to_edge(pair, direction, rtype, confidence, reason))

                # æ— å‘å…³ç³»ï¼šå†™åŒå‘è¾¹
                elif rtype in UNDIRECTED:
                    pair_edges.append(_make_edge(pair["src"], pair["tgt"], rtype, confidence, reason))
                    pair_edges.append(_make_edge(pair["tgt"], pair["src"], rtype, confidence, reason))

                return pair_edges

            except Exception as e:
                print(f"[âš ] å¤„ç†æƒ…èŠ‚å¯¹ {pair} å‡ºé”™: {e}")
                return []

        # å¹¶å‘å¤„ç†
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from tqdm import tqdm

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_pair, pair) for pair in all_plot_pairs]
            for future in tqdm(as_completed(futures), total=len(futures), desc="æŠ½å–æƒ…èŠ‚å…³ç³»"):
                try:
                    res = future.result()
                    if res:
                        edges_to_add.extend(res)
                except Exception as e:
                    print(f"[âš ] future ç»“æœå¤„ç†å‡ºé”™: {e}")

        # æ‰¹é‡å†™å…¥ Neo4j
        if edges_to_add:
            self.neo4j_utils.create_plot_relations(edges_to_add)
            print(f"[âœ“] å·²åˆ›å»ºæƒ…èŠ‚å…³ç³» {len(edges_to_add)} æ¡")
        else:
            print("[!] æ²¡æœ‰ç”Ÿæˆä»»ä½•æƒ…èŠ‚å…³ç³»")


    def build_event_plot_graph(self):
        # æ¸…ç©ºæ—§çš„ Plot å›¾ä¸å…³ç³»ï¼ˆå·²é€‚é…æ–°å…­ç§ Plot å…³ç³» + HAS_EVENTï¼‰
        self.neo4j_utils.reset_event_plot_graph()

        base = self.config.storage.knowledge_graph_path
        print("äº‹ä»¶å¡ç‰‡ä¿¡æ¯...")
        with open(os.path.join(base, "event_cards.json"), "r", encoding="utf-8") as f:
            self.event_cards = json.load(f)

        # å»ºè®®ï¼šè¿™é‡Œçš„ get_all_event_chains å†…éƒ¨è¯·ç¡®è®¤å·²ä½¿ç”¨ä¸‰ç±»äº‹ä»¶å…³ç³» + confidence è¿‡æ»¤
        all_chains = self.get_all_event_chains(min_confidence=0.0)

        print("[âœ“] å½“å‰äº‹ä»¶é“¾æ€»æ•°ï¼š", len(all_chains))
        filtered_chains = get_frequent_subchains(all_chains, 2, 1)
        filtered_chains = remove_subset_paths(filtered_chains)
        filtered_chains = remove_similar_paths(filtered_chains, 0.7)
        print("[âœ“] è¿‡æ»¤åäº‹ä»¶é“¾æ€»æ•°ï¼š", len(filtered_chains))

        def _stable_plot_id(title: str, chain: list[str]) -> str:
            key = f"{title}||{'->'.join(chain)}"
            return "plot_" + hashlib.sha1(key.encode("utf-8")).hexdigest()[:16]

        def _to_bool(v) -> bool:
            if isinstance(v, bool):
                return v
            if v is None:
                return False
            return str(v).strip().lower() in ("true", "yes", "1")

        def process_chain(chain):
            try:
                # ç»„ç»‡ä¸Šä¸‹æ–‡
                event_chain_info = self.prepare_chain_context(chain)

                # å–ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆå®¹é”™ç©ºå€¼ï¼‰
                chunk_ids = []
                for ent_id in chain:
                    ent = self.neo4j_utils.get_entity_by_id(ent_id)
                    if not ent:
                        continue
                    sc = ent.source_chunks or []
                    if sc:
                        chunk_ids.append(sc[0])
                chunk_ids = list(set(chunk_ids))

                related_context = ""
                if chunk_ids:
                    documents = self.vector_store.search_by_ids(chunk_ids)
                    contents = {getattr(doc, "content", "") for doc in documents if getattr(doc, "content", "")}
                    related_context = "\n".join(list(contents))

                # ç”Ÿæˆæƒ…èŠ‚åˆ¤å®š
                raw = self.graph_analyzer.generate_event_plot(
                    event_chain_info=event_chain_info,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )
                result = json.loads(correct_json_format(raw))

                if not _to_bool(result.get("is_plot")):
                    return False

                plot_info = result.get("plot_info") or {}
                title = (plot_info.get("title") or "").strip()
                if not title:
                    # å…œåº•æ ‡é¢˜ï¼šé¦–å°¾äº‹ä»¶åæˆ–ID
                    title = f"æƒ…èŠ‚é“¾ï¼š{chain[0]}â†’{chain[-1]}"

                plot_info["id"] = _stable_plot_id(title, chain)
                plot_info["event_ids"] = chain
                plot_info["reason"] = result.get("reason", "")

                # å†™å…¥ Neo4jï¼ˆå·²åœ¨ Neo4jUtils å†…é€‚é…æ–°å…³ç³»/å­—æ®µï¼‰
                self.neo4j_utils.write_plot_to_neo4j(plot_data=plot_info)
                return True

            except Exception as e:
                print(f"[!] å¤„ç†äº‹ä»¶é“¾ {chain} æ—¶å‡ºé”™: {e}")
                return False

        success_count = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_chain, chain) for chain in filtered_chains]
            for future in tqdm(as_completed(futures), total=len(futures), desc="å¹¶å‘ç”Ÿæˆæƒ…èŠ‚å›¾è°±"):
                try:
                    if future.result():
                        success_count += 1
                except Exception as e:
                    print(f"[!] å­ä»»åŠ¡å¼‚å¸¸ï¼š{e}")

        print(f"[âœ“] æˆåŠŸç”Ÿæˆæƒ…èŠ‚æ•°é‡ï¼š{success_count}/{len(filtered_chains)}")
        return

