# core/builder/graph_builder.py
from __future__ import annotations

import json
import os
import sqlite3
import pickle
import multiprocessing as mp
import time
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError, wait, FIRST_COMPLETED
import time
from collections import defaultdict
from copy import deepcopy
from typing import Any, Dict, List, Optional
import asyncio
import pandas as pd
import random
import re
import glob
from tqdm import tqdm
from core.utils.prompt_loader import PromptLoader
from core.utils.format import correct_json_format
from core.models.data import Entity, KnowledgeGraph, Relation, TextChunk, Document
from ..storage.graph_store import GraphStore
from ..storage.vector_store import VectorStore
from ..utils.config import KAGConfig
from ..utils.neo4j_utils import Neo4jUtils
from core.model_providers.openai_llm import OpenAILLM
from core.agent.knowledge_extraction_agent import InformationExtractionAgent
from core.agent.attribute_extraction_agent import AttributeExtractionAgent
from core.agent.graph_probing_agent import GraphProbingAgent
from .document_processor import DocumentProcessor
from core.builder.graph_preprocessor import GraphPreprocessor
from core.utils.format import DOC_TYPE_META
from core.builder.reflection import DynamicReflector
from collections import defaultdict

def _normalize_type(t):
    """
    - è¾“å…¥å¯ä¸º str æˆ– list
    - è‹¥åŒ…å« 'Event' â†’ æŠŠ 'Event' æ”¾åˆ°é¦–ä½
    - å»é‡ä½†ä¿æŒåŸç›¸å¯¹é¡ºåº
    - è‹¥ä¸ºç©º â†’ è¿”å› 'Concept'
    """
    if not t:
        return "Concept"
    if isinstance(t, str):
        return t

    # list æƒ…å†µ
    seen = set()
    ordered = []
    for x in t:
        if x and x not in seen:
            seen.add(x)
            ordered.append(x)

    if "Event" in seen:
        ordered = ["Event"] + [x for x in ordered if x != "Event"]

    return ordered if len(ordered) > 1 else (ordered[0] if ordered else "Concept")
    
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               Builder
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ï¼ˆæ”¯æŒå¤šæ–‡æ¡£æ ¼å¼ï¼‰"""
    def __init__(self, config: KAGConfig, doc_type: str = None):
        if doc_type:
            self.doc_type = doc_type
        else:
            self.doc_type = config.knowledge_graph_builder.doc_type
        if self.doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {self.doc_type}")
        
        self.config = config
        self.max_workers = config.knowledge_graph_builder.max_workers
        self.meta = DOC_TYPE_META[self.doc_type]
        self.section_chunk_ids = defaultdict(set)

        prompt_dir = config.knowledge_graph_builder.prompt_dir
        self.prompt_loader = PromptLoader(prompt_dir)
        self.llm = OpenAILLM(config)
        self.processor = DocumentProcessor(config, self.llm, self.doc_type)

        # å­˜å‚¨ / æ•°æ®åº“
        self.graph_store = GraphStore(config)
        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, doc_type=self.doc_type)
        self.document_vector_store = VectorStore(config, "documents")
        self.sentence_vector_store = VectorStore(config, "sentences")
        # self.document_store = DocumentStore(config)
        # åˆå§‹åŒ–è®°å¿†æ¨¡å—
        self.reflector = DynamicReflector(config)

        # è¿è¡Œæ•°æ®
        self.kg = KnowledgeGraph()
        self.probing_mode = self.config.probing.probing_mode
        self.graph_probing_agent = GraphProbingAgent(self.config, self.llm, self.reflector)
        
    def clear_directory(self, path):
        for file in glob.glob(os.path.join(path, "*.json")):
            try:
                os.remove(file)
            except Exception as e:
                print(f"åˆ é™¤å¤±è´¥: {file} -> {e}")

    def construct_system_prompt(self, background, abbreviations):
        
        background_info = self.get_background_info(background, abbreviations)
        
        if self.doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel"
            
        system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})
        return system_prompt_text
    
    def get_background_info(self, background, abbreviations):
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" 

        # ---------- 2) ç¼©å†™è¡¨ï¼ˆé”®åå®½å®¹ï¼‰ ----------
        def fmt(item: dict) -> str:
            """
            å°†ä¸€ä¸ªç¼©å†™é¡¹è½¬ä¸º Markdown åˆ—è¡¨æ¡ç›®ã€‚ä»»ä½•å­—æ®µéƒ½å¯é€‰ï¼Œæ ‡é¢˜å­—æ®µä¼˜å…ˆçº§ä¸ºï¼š
            abbr > full > å…¶ä»–å­—æ®µ > N/A
            """
            if not isinstance(item, dict):
                return ""

            # æ ‡é¢˜å­—æ®µä¼˜å…ˆçº§
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )

            # å‰©ä¸‹å­—æ®µå»é™¤æ ‡é¢˜å­—æ®µ
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())

            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(item) for item in abbreviations if isinstance(item, dict))

        if background and abbr_block:
            background_info = f"{bg_block}\n{abbr_block}"
        else:
            background_info = bg_block or abbr_block
            
        return background_info
        
    def prepare_chunks(
        self,
        json_file_path: str,
        verbose: bool = True,
        retries: int = 2,
        per_task_timeout: float = 120.0,
        retry_backoff: float = 1.0,
    ):
        """
        å¹¶å‘æ‹†åˆ†æ–‡æ¡£ä¸º TextChunkï¼ŒæŒ‰ã€å®Œæˆé¡ºåºã€‘æ”¶é›†ï¼›
        - å•æ–‡æ¡£è½¯è¶…æ—¶ = per_task_timeoutï¼ˆé»˜è®¤120sï¼‰ï¼Œè¶…æ—¶åˆ™è·³è¿‡å…¶å—å¹¶åœ¨ä¸‹ä¸€è½®é‡è¯•ï¼›
        - å¯é…ç½®é‡è¯•è½®æ•° retriesï¼ˆé»˜è®¤2è½®ï¼‰ã€‚æ¯è½®ä»…å¯¹æœªæˆåŠŸçš„æ–‡æ¡£è¿›è¡Œå¹¶å‘å¤„ç†ï¼›
        - æˆåŠŸï¼šç«‹å³æ”¶é›†å…¶ document_chunksï¼›
        - å¤±è´¥/è¶…æ—¶ï¼šè®¡å…¥é˜Ÿåˆ—ï¼Œè‹¥ä»æœ‰å‰©ä½™è½®æ¬¡åˆ™è¿›å…¥ä¸‹ä¸€è½®é‡è¯•ã€‚

        Args:
            json_file_path: è¾“å…¥ JSON è·¯å¾„
            verbose: æ˜¯å¦æ‰“å°æ—¥å¿—
            retries: æœ€å¤§é‡è¯•è½®æ•°ï¼ˆå«é¦–è½®ï¼‰ï¼Œé»˜è®¤2
            per_task_timeout: å•æ–‡æ¡£è½¯è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤120
            retry_backoff: æ¯è½®ä¹‹é—´çš„é€€é¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1.0
        """
        # â€”â€” åˆå§‹åŒ–/æ¸…ç† â€”â€” #
        self.reflector.clear()
        base = self.config.storage.knowledge_graph_path
        self.clear_directory(base)

        if verbose:
            print(f"ğŸš€ å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±: {json_file_path}")
            print("ğŸ“– åŠ è½½æ–‡æ¡£...")

        documents = self.processor.load_from_json(json_file_path, extract_metadata=True)
        n_docs = len(documents)

        if verbose:
            print(f"âœ… æˆåŠŸåŠ è½½ {n_docs} ä¸ªæ–‡æ¡£")

        # å•æ–‡æ¡£æ‹†åˆ†ä»»åŠ¡
        def _run(doc):
            # æœŸæœ›è¿”å› {"document_chunks": List[TextChunk]}
            return self.processor.prepare_chunk(doc)

        # â€”â€” å¹¶å‘ & é‡è¯• â€”â€” #
        all_chunks = []
        # è¿™äº›ç´¢å¼•æœ€ç»ˆç”¨äºæ±‡æŠ¥ï¼šæ¯è½®åŠ¨æ€æ›´æ–°
        final_failures = set()   # å…¨éƒ¨è½®æ¬¡é‡Œä»å¤±è´¥ï¼ˆå¼‚å¸¸ï¼‰çš„æ–‡æ¡£ idx
        final_timeouts = set()   # å…¨éƒ¨è½®æ¬¡é‡Œä»è¶…æ—¶çš„æ–‡æ¡£ idx

        # å¾…å¤„ç†æ–‡æ¡£ç´¢å¼•é›†åˆï¼ˆç¬¬ä¸€è½®æ˜¯å…¨éƒ¨ï¼‰
        remaining = list(range(n_docs))

        # å¹¶å‘å¤§å°
        max_workers = getattr(self, "max_workers", getattr(self, "max_worker", 4))

        for round_id in range(1, max(1, retries) + 1):
            if not remaining:
                if verbose:
                    print(f"ğŸ‰ æ‰€æœ‰æ–‡æ¡£åœ¨ç¬¬ {round_id-1} è½®å‰å·²å®Œæˆï¼Œæ— éœ€ç»§ç»­ã€‚")
                break

            if verbose:
                print(f"\nğŸ” ç¬¬ {round_id}/{max(1, retries)} è½®å¹¶å‘æ‹†åˆ†ï¼šå¾…å¤„ç† {len(remaining)} ä¸ªæ–‡æ¡£")

            # å½“è½®çš„å¤±è´¥/è¶…æ—¶æš‚å­˜ï¼ˆä»…æœ¬è½®è®¡ç®—ï¼Œç”¨äºä¸‹ä¸€è½®é‡è¯•ï¼‰
            round_failures = []
            round_timeouts = []

            # â€”â€” æäº¤ä»»åŠ¡ â€”â€” #
            with ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix=f"chunk-r{round_id}") as executor:
                fut_info = {}  # future -> {"start": float, "idx": int}
                for idx in remaining:
                    f = executor.submit(_run, documents[idx])
                    fut_info[f] = {"start": time.time(), "idx": idx}

                pbar = tqdm(total=len(fut_info), desc=f"å¹¶å‘æ‹†åˆ†ä¸­/ç¬¬{round_id}è½®", ncols=100)
                pending = set(fut_info.keys())

                while pending:
                    # 1) å…ˆæ”¶é›†å·²å®Œæˆçš„
                    done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)
                    for f in done:
                        info = fut_info.pop(f, None)
                        try:
                            grp = f.result()  # å·²å®Œæˆï¼Œä¸é˜»å¡
                            chunks = (grp or {}).get("document_chunks", [])
                            if isinstance(chunks, list):
                                all_chunks.extend(chunks)
                            else:
                                round_failures.append(info["idx"])
                        except Exception:
                            round_failures.append(info["idx"])
                        pbar.update(1)

                    # 2) æ£€æŸ¥æœªå®Œæˆæ˜¯å¦è¶…è¿‡è½¯è¶…æ—¶ï¼›è¶…æ—¶åˆ™ä¸å†ç­‰å¾…
                    now = time.time()
                    to_forget = []
                    for f in pending:
                        start = fut_info[f]["start"]
                        if now - start >= per_task_timeout:
                            info = fut_info[f]
                            try:
                                f.cancel()  # è‹¥å·²åœ¨è¿è¡Œåˆ™è¿”å› Falseï¼›æ— è®ºå¦‚ä½•ä¸å†ç­‰å¾…
                            except Exception:
                                pass
                            round_timeouts.append(info["idx"])
                            pbar.update(1)
                            to_forget.append(f)
                    if to_forget:
                        for f in to_forget:
                            pending.remove(f)
                            fut_info.pop(f, None)

                pbar.close()

                # executor åœ¨ with ç»“æŸæ—¶ä¼šç­‰å¾…æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡å®Œæˆï¼›
                # ä½†æˆ‘ä»¬ä¸Šé¢å·²å¯¹è¶…æ—¶çš„ future åšäº† pbar æ›´æ–°å¹¶ç§»é™¤ pendingï¼Œä¸ä¼šå¡ä½ã€‚

            # â€”â€” è®¡ç®—ä¸‹ä¸€è½® remaining â€”â€” #
            # å½“è½®æœªæˆåŠŸ = (å½“è½®å¤±è´¥ âˆª å½“è½®è¶…æ—¶)
            # æ³¨æ„ï¼šæˆåŠŸçš„ idx å·²ç»é€šè¿‡ all_chunks æ”¶é›†ï¼Œä¸éœ€è¦è®°å½•ã€‚
            remaining = list(set(round_failures) | set(round_timeouts))

            # æ±‡æ€»åˆ°â€œæœ€ç»ˆç»Ÿè®¡é›†åˆâ€ï¼ˆç”¨äºå…¨éƒ¨è½®æ¬¡ç»“æŸåçš„æ±‡æŠ¥ï¼‰
            final_failures.update(round_failures)
            final_timeouts.update(round_timeouts)

            if verbose:
                print(f"ğŸ“¦ ç¬¬ {round_id} è½®ç»“æŸï¼šæˆåŠŸ {n_docs - len(remaining) - (len(all_chunks) == 0)}ï¼ˆç´¯è®¡å— {len(all_chunks)}ï¼‰")
                if round_timeouts:
                    print(f"â±ï¸ æœ¬è½®è¶…æ—¶ {len(round_timeouts)} ä¸ªï¼š{sorted(round_timeouts)}")
                if round_failures:
                    print(f"â—æœ¬è½®å¤±è´¥ {len(round_failures)} ä¸ªï¼š{sorted(round_failures)}")

            # è‹¥è¿˜æœ‰æœªå®Œæˆä¸”ä»æœ‰ä¸‹ä¸€è½®ï¼Œç¨ä½œé€€é¿
            if remaining and round_id < max(1, retries) and retry_backoff > 0:
                time.sleep(retry_backoff)

        # â€”â€” è½ç›˜ï¼ˆåªå†™æˆåŠŸå—ï¼‰â€”â€” #
        os.makedirs(base, exist_ok=True)
        out_path = os.path.join(base, "all_document_chunks.json")
        with open(out_path, "w", encoding="utf-8") as fw:
            json.dump([c.dict() for c in all_chunks], fw, ensure_ascii=False, indent=2)

        # â€”â€” æœ€ç»ˆæŠ¥å‘Š â€”â€” #
        if verbose:
            print(f"\nâœ… æœ€ç»ˆç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—ï¼Œå·²å†™å…¥ï¼š{out_path}")

            # â€œæœ€ç»ˆå¤±è´¥/è¶…æ—¶é›†åˆâ€é‡Œï¼Œå»æ‰é‚£äº›åæ¥æˆåŠŸå®Œæˆçš„ç´¢å¼•
            # åšæ³•ï¼šæ¨æ–­æˆåŠŸæ–‡æ¡£ç´¢å¼• = å…¨éƒ¨ç´¢å¼• - æœ€ç»ˆ remaining é›†åˆ
            # ä½†ç”±äºæˆ‘ä»¬æ²¡æœ‰é€æ–‡æ¡£çš„ chunk è®¡æ•°ï¼Œè¿™é‡Œé‡‡ç”¨ä¿å®ˆæ±‡æŠ¥ï¼š
            #   æŠ¥å‘Šåœ¨â€œæœ€åä¸€è½®ä»æœªå®Œæˆâ€çš„ç´¢å¼•
            if remaining:
                print(f"âš ï¸ ä»¥ä¸‹ {len(remaining)} ä¸ªæ–‡æ¡£åœ¨ {retries} è½®åä»æœªå®Œæˆï¼š{sorted(remaining)}")

            # è¾…åŠ©ï¼šç»™å‡ºæ‰€æœ‰å›åˆä¸­å‡ºç°è¿‡çš„å¼‚å¸¸/è¶…æ—¶ç´¢å¼•ï¼ˆä¾¿äºæ’æŸ¥ï¼‰
            if final_timeouts:
                print(f"â±ï¸ æ‰€æœ‰è½®æ¬¡ç´¯è®¡å‡ºç°è¿‡è¶…æ—¶çš„æ–‡æ¡£ç´¢å¼•ï¼š{sorted(final_timeouts)}")
            if final_failures:
                print(f"â—æ‰€æœ‰è½®æ¬¡ç´¯è®¡å‡ºç°è¿‡å¤±è´¥çš„æ–‡æ¡£ç´¢å¼•ï¼š{sorted(final_failures)}")


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  2) å­˜å‚¨ Chunkï¼ˆRDB + VDBï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def store_chunks(self, verbose: bool = True):
        base = self.config.storage.knowledge_graph_path

        # æè¿°å—
        doc_chunks = [TextChunk(**o) for o in
                       json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        # å†™å…¥ KGï¼ˆDocument + Chunkï¼‰
        for ch in doc_chunks:
            self.kg.add_document(self.processor.prepare_document(ch))
            self.kg.add_chunk(ch)

        # å†™å…¥å‘é‡æ•°æ®åº“
        if verbose:
            print("ğŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
            
        self._store_vectordb(verbose)
        
        
    def run_graph_probing(self, verbose: bool = True, sample_ratio: float = None):
        self.reflector.clear()
        
        base = self.config.storage.graph_schema_path
        os.makedirs(base, exist_ok=True)
        self.clear_directory(base)
        
        if self.probing_mode == "fixed" or self.probing_mode == "adjust" :
            schema = json.load(open(self.config.probing.default_graph_schema_path, "r", encoding="utf-8"))
            if os.path.exists(self.config.probing.default_background_path):
                settings = json.load(open(os.path.join(self.config.probing.default_background_path), "r", encoding="utf-8"))        
            else:
                settings = {"background": "", "abbreviations": []}
        else:
            schema = {}
            settings = {"background": "", "abbreviations": []}
        
        if self.probing_mode != "fixed":
            schema , settings = self.update_schema(schema, background=settings["background"], abbreviations=settings["abbreviations"],
                                                   verbose=verbose, sample_ratio=sample_ratio)
        else:
            if verbose:
                print("ğŸ“Œ è·³è¿‡probingæ­¥éª¤")
            
        with open(os.path.join(base, "graph_schema.json"), "w", encoding="utf-8") as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)
            
        with open(os.path.join(base, "settings.json"), "w", encoding="utf-8") as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
            
            
    def initialize_agents(self):
        
        base = self.config.storage.graph_schema_path
        schema_path = os.path.join(base, "graph_schema.json")
        settings_path = os.path.join(base, "settings.json")
        if os.path.exists(schema_path):
            schema = json.load(open(schema_path, "r", encoding="utf-8"))
        elif os.path.exists(self.config.probing.default_graph_schema_path):
            schema = json.load(open(self.config.probing.default_graph_schema_path, "r", encoding="utf-8"))
        else:
            raise FileNotFoundError("æ²¡æœ‰graph schemaï¼Œè¯·æä¾›æ­£ç¡®è·¯å¾„æˆ–è€…å…ˆè¿è¡Œprobing")
        
        if os.path.exists(settings_path):
            settings = json.load(open(settings_path, "r", encoding="utf-8"))
        elif os.path.exists(self.config.probing.default_background_path):
            settings = json.load(open(self.config.probing.default_background_path, "r", encoding="utf-8"))
        else:
            settings = {"background": "", "abbreviations": []}
 
        self.system_prompt_text = self.construct_system_prompt(
            background=settings["background"],
            abbreviations=settings["abbreviations"]
        )

        # æŠ½å– agent
        self.information_extraction_agent = InformationExtractionAgent(self.config, self.llm, self.system_prompt_text, schema, self.reflector)
        self.attribute_extraction_agent = AttributeExtractionAgent(self.config, self.llm, self.system_prompt_text, schema)
        self.graph_preprocessor = GraphPreprocessor(self.config, self.llm, system_prompt=self.system_prompt_text)
        
    def update_schema(self, schema: Dict = {}, background: str = "", abbreviations: List = [], verbose: bool = True, sample_ratio: float = None, documents: List[Document] = None):
        if documents:
            doc_chunks = documents
        else:
            base = self.config.storage.knowledge_graph_path
            doc_chunks = [TextChunk(**o) for o in
                        json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]
            
        if not sample_ratio:
            sample_ratio = 0.35
        k = int(len(doc_chunks) * sample_ratio)
        sampled_chunks = random.sample(doc_chunks, k=k)
        # ä¿å­˜ä¸€äº›æ´å¯Ÿï¼Œç”¨äºä¹‹åçš„æ¢ç´¢ã€‚
        sampled_chunks = self.processor.extract_insights(sampled_chunks)
        
        for chunk in tqdm(sampled_chunks, desc="ä¿å­˜æ´è§ä¸­", total=len(sampled_chunks)):
            insights = chunk.metadata.get("insights", []) 
            for item in insights:
                # print("[CHECK] insight", item)
                self.reflector.insight_memory.add(text=item, metadata={})
        if verbose:
            print("ğŸ’¾ ä¿å­˜æ´è§å®Œæˆ")
            
        params = dict()
        params["documents"] = sampled_chunks   
        params["schema"] = schema  
        params["background"] = background
        params["abbreviations"] = abbreviations
        
        result = self.graph_probing_agent.run(params)
        # result = json.loads(correct_json_format(result))
        
        return result["schema"], result["settings"]
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  3) å®ä½“ / å…³ç³» æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def extract_entity_and_relation(self, verbose: bool = True):
        asyncio.run(self.extract_entity_and_relation_async(verbose=verbose))
            
    async def extract_entity_and_relation_async(self, verbose: bool = True):
        """
        å¹¶å‘æŠ½å– â†’ è®°å½•å¤±è´¥çš„ chunk â†’ ç»Ÿä¸€é‡è¯•ä¸€è½® â†’ å†™ç›˜
        """
        base = self.config.storage.knowledge_graph_path
        desc_chunks = [TextChunk(**o) for o in
                    json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        if verbose:
            print("ğŸ§  å®ä½“ä¸å…³ç³»ä¿¡æ¯å¼‚æ­¥æŠ½å–ä¸­...")

        sem = asyncio.Semaphore(self.max_workers)

        async def _arun_once(ch: TextChunk):
            async with sem:
                try:
                    if not ch.content.strip():
                        result = {"entities": [], "relations": []}
                    else:
                        result = await self.information_extraction_agent.arun(
                            ch.content,
                            timeout=self.config.agent.async_timeout,
                            max_attempts=self.config.agent.async_max_attempts,
                            backoff_seconds=self.config.agent.async_backoff_seconds
                        )
                    result.update(chunk_id=ch.id, chunk_metadata=ch.metadata)
                    return result
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] æŠ½å–å¤±è´¥ chunk_id={ch.id} | {e.__class__.__name__}: {e}")
                    return {
                        "chunk_id": ch.id,
                        "chunk_metadata": ch.metadata,
                        "entities": [],
                        "relations": [],
                        "error": f"{e.__class__.__name__}: {e}"
                    }

        async def _arun_with_ch(ch: TextChunk):
            """è¿”å› (chunk, result) æ–¹ä¾¿ç›´æ¥è®°å½•å¤±è´¥çš„ chunkã€‚"""
            res = await _arun_once(ch)
            return ch, res

        # ====== é¦–è½®å¹¶å‘ ======
        tasks = [_arun_with_ch(ch) for ch in desc_chunks]
        first_round_pairs = []   # [(ch, res), ...]
        failed_chs = []          # [ch, ...]

        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="å¼‚æ­¥æŠ½å–ä¸­"):
            ch, res = await coro
            first_round_pairs.append((ch, res))
            if res.get("error"):
                failed_chs.append(ch)

        # ====== ç»Ÿä¸€é‡è¯•ï¼ˆåªä¸€è½®ï¼‰======
        retry_pairs = []
        if failed_chs:
            if verbose:
                print(f"ğŸ”„ å¼€å§‹é‡è¯•å¤±è´¥çš„ {len(failed_chs)} ä¸ªæ–‡æœ¬å—...")
            retry_tasks = [_arun_with_ch(ch) for ch in failed_chs]
            for coro in tqdm(asyncio.as_completed(retry_tasks), total=len(retry_tasks), desc="é‡è¯•æŠ½å–ä¸­"):
                ch, res = await coro
                retry_pairs.append((ch, res))

        # ====== åˆå¹¶ç»“æœï¼ˆç”¨å¤±è´¥çš„ ch è¿‡æ»¤é¦–è½®å¯¹åº”ç»“æœï¼Œå†è¿½åŠ é‡è¯•ç»“æœï¼‰======
        failed_ids = {ch.id for ch in failed_chs}
        final_results = [res for ch, res in first_round_pairs if ch.id not in failed_ids]
        final_results += [res for _, res in retry_pairs]

        # ====== è½ç›˜ ======
        output_path = os.path.join(base, "extraction_results.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)

        if verbose:
            still_failed = sum(1 for r in final_results if r.get("error"))
            print(f"âœ… å®ä½“ä¸å…³ç³»ä¿¡æ¯æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {len(final_results)} ä¸ªæ–‡æœ¬å—")
            if still_failed:
                print(f"âš ï¸ ä»æœ‰ {still_failed} ä¸ªæ–‡æœ¬å—åœ¨é‡è¯•åå¤±è´¥ï¼ˆä¿ç•™ error å­—æ®µä»¥ä¾¿æ’æŸ¥ï¼‰")
            print(f"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  4) å±æ€§æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def run_extraction_refinement(self, verbose=False):
        """
        å®ä½“æ¶ˆæ­§
        """
        KW = ("é—ªå›", "ä¸€ç»„è’™å¤ªå¥‡")
        base = self.config.storage.knowledge_graph_path
        extraction_results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))
        for doc in extraction_results:
            # 1) å…ˆåˆ å®ä½“ï¼šåå­—å«å…³é”®è¯çš„ä¸€å¾‹åˆ é™¤
            ents = doc.get("entities", [])
            removed_names = {e.get("name", "") for e in ents if any(k in e.get("name", "") for k in KW)}
            doc["entities"] = [e for e in ents if e.get("name", "") not in removed_names]

            # 2) å†åˆ å…³ç³»ï¼šsubject/object å«å…³é”®è¯ï¼Œæˆ–æŒ‡å‘å·²åˆ é™¤å®ä½“å
            rels = doc.get("relations", [])
            doc["relations"] = [
                r for r in rels
                if not any(k in r.get("subject", "") or k in r.get("object", "") for k in KW)
                and r.get("subject", "") not in removed_names
                and r.get("object", "") not in removed_names
            ]
    
        if verbose:
            print("ğŸ“Œ ä¼˜åŒ–å®ä½“ç±»å‹")
        extraction_results =self.graph_preprocessor.refine_entity_types(extraction_results)
        if verbose:
            print("ğŸ“Œ ä¼˜åŒ–å®ä½“scope")
        extraction_results = self.graph_preprocessor.refine_entity_scope(extraction_results)
        if verbose:
            print("ğŸ“Œ å®ä½“æ¶ˆæ­§")
        extraction_results = self.graph_preprocessor.run_entity_disambiguation(extraction_results)
        
        base = self.config.storage.knowledge_graph_path
        with open(os.path.join(base, "extraction_results_refined.json"), "w") as f:
            json.dump(extraction_results, f, ensure_ascii=False, indent=2)
        

    def extract_entity_attributes(self, verbose: bool = True) -> Dict[str, Entity]:
        asyncio.run(self.extract_entity_attributes_async(verbose=verbose))
    
    async def extract_entity_attributes_async(self, verbose: bool = True) -> Dict[str, Entity]:
        """
        âš¡ å¼‚æ­¥æ‰¹é‡å±æ€§æŠ½å–  
        Â· æŒ‰ extract_entity_and_relation_async ç”Ÿæˆçš„ entity_map å»å¹¶å‘  
        Â· æ¯ä¸ªå®ä½“è°ƒç”¨ attribute_extraction_agent.arun()  
        Â· å†…éƒ¨ arun å·²å¸¦è¶…æ—¶ï¼‹é‡è¯•ä¿æŠ¤ï¼Œä¸ä¼šå¡æ­»
        """
        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results_refined.json"), "r", encoding="utf-8"))
        
        #print(results[0])
        # å°†å®ä½“åˆå¹¶ / å»é‡
        entity_map = self.merge_entities_info(results)            # {name: Entity}

        if verbose:
            print(f"ğŸ” å¼€å§‹å±æ€§æŠ½å–ï¼ˆå¼‚æ­¥ï¼‰ï¼Œå®ä½“æ•°ï¼š{len(entity_map)}")

        sem = asyncio.Semaphore(self.max_workers)
        updated_entities: Dict[str, Entity] = {}

        async def _arun_attr(name: str, ent: Entity):
            async with sem:
                try:
                    txt = ent.description or ""
                    if not txt.strip():
                        return name, None

                    # AttributeExtractionAgent.arun å·²è‡ªå¸¦ timeout+é‡è¯•
                    res = await self.attribute_extraction_agent.arun(
                        text=txt,
                        entity_name=name,
                        entity_type=ent.type,
                        source_chunks=ent.source_chunks,
                        original_text="",
                        timeout=self.config.agent.async_timeout,
                        max_attempts=self.config.agent.async_max_attempts,
                        backoff_seconds=self.config.agent.async_backoff_seconds
                    )

                    if res.get("error"):          # è¶…æ—¶æˆ–å¼‚å¸¸
                        return name, None

                    attrs = res.get("attributes", {}) or {}
                    if isinstance(attrs, str):
                        try:
                            attrs = json.loads(attrs)
                        except json.JSONDecodeError:
                            attrs = {}

                    new_ent = deepcopy(ent)
                    new_ent.properties = attrs

                    nd = res.get("new_description", "")
                    if nd:
                        new_ent.description = nd

                    return name, new_ent
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] å±æ€§æŠ½å–å¤±è´¥ï¼ˆå¼‚æ­¥ï¼‰ï¼š{name}: {e}")
                    return name, None

        # å¹¶å‘æ‰§è¡Œ
        tasks = [_arun_attr(n, e) for n, e in entity_map.items()]
        for coro in tqdm(asyncio.as_completed(tasks),
                               total=len(tasks),
                               desc="å±æ€§æŠ½å–ä¸­ï¼ˆasyncï¼‰"):
            n, e2 = await coro
            if e2:
                updated_entities[n] = e2

        # å†™æ–‡ä»¶
        output_path = os.path.join(base, "entity_info.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({k: v.dict() for k, v in updated_entities.items()},
                      f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±æ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®ä½“ {len(updated_entities)} ä¸ª")
            print(f"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

        return updated_entities

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  5) æ„å»ºå¹¶å­˜å‚¨å›¾è°±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def build_graph_from_results(self, verbose: bool = True) -> KnowledgeGraph:
        if verbose:
            print("ğŸ“‚ åŠ è½½å·²æœ‰æŠ½å–ç»“æœå’Œå®ä½“ä¿¡æ¯...")

        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results_refined.json"), "r", encoding="utf-8"))
        ent_raw = json.load(open(os.path.join(base, "entity_info.json"), "r", encoding="utf-8"))
        
        with open(os.path.join(base, "section_entities_collection.pkl"), "rb") as f:
            self.section_entities_collection = pickle.load(f)

        # id â†’ Entity
        entity_map = {d["id"]: Entity(**d) for d in ent_raw.values()}
        name2id: Dict[str, str] = {e.name: e.id for e in entity_map.values()}
        
        for e in entity_map.values():
            for al in e.aliases:
                name2id.setdefault(al, e.id)
            self.kg.add_entity(e)

        if verbose:
            print("ğŸ”— æ„å»ºçŸ¥è¯†å›¾è°±...")

        self.section_names = []
        for res in results:
            md = res.get("chunk_metadata", {})
            # Section å®ä½“
            secs = self._create_section_entities(md, res["chunk_id"])
            for se in secs:
                # if se.id not in self.kg.entities:
                if se.name not in self.section_names and se.id not in self.kg.entities:
                    self.kg.add_entity(se)
                    self.section_names.append(se.name)
                else:
                    exist = self.kg.entities.get(se.id)
                    if exist:
                        # å»é‡å¹¶ä¿æŒç¨³å®šé¡ºåº
                        merged = list(dict.fromkeys(list(exist.source_chunks) + list(se.source_chunks)))
                        exist.source_chunks = merged
                

            inner = self.section_entities_collection[se.name]
            # print("[CHECK] inner entities: ", [e.name for e in inner])
            for se in secs:
                self._link_section_to_entities(se, inner, res["chunk_id"])

            # æ™®é€šå…³ç³»
            for rdata in res.get("relations", []):
                rel = self._create_relation_from_data(rdata, res["chunk_id"], entity_map, name2id)
                if rel:
                    self.kg.add_relation(rel)

        # å†™å…¥æ•°æ®åº“
        if verbose:
            print("ğŸ’¾ å­˜å‚¨åˆ°æ•°æ®åº“...")
        self._store_knowledge_graph(verbose)
        self.neo4j_utils.enrich_event_nodes_with_context()
        self.neo4j_utils.compute_centrality(exclude_rel_types=[self.meta['contains_pred']])

        if verbose:
            st = self.kg.stats()
            print(f"ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ!")
            # print(f"   - å®ä½“æ•°é‡: {st['entities']}")
            # print(f"   - å…³ç³»æ•°é‡: {st['relations']}")
            graph_stats = self.graph_store.get_stats()
            print(f"   - å®ä½“æ•°é‡: {graph_stats['entities']}")
            print(f"   - å…³ç³»æ•°é‡: {graph_stats['relations']}")
            print(f"   - æ–‡æ¡£æ•°é‡: {st['documents']}")
            print(f"   - æ–‡æœ¬å—æ•°é‡: {st['chunks']}")

        return self.kg

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  å†…éƒ¨å·¥å…·
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # -------- åˆå¹¶å®ä½“ï¼ˆæ ¹æ® doc_type é€‚é…ï¼‰ --------
    def merge_entities_info(self, extraction_results):
        """
        éå†ä¿¡æ¯æŠ½å–ç»“æœï¼Œåˆå¹¶ / å»é‡å®ä½“ã€‚
        - â€œå±€éƒ¨ä½œç”¨åŸŸâ€å®ä½“ï¼ˆscope == localï¼‰è‹¥å‘½åå†²çªï¼Œä¼šåœ¨å‰é¢åŠ ä¸Š
          â€œåœºæ™¯N â€¦â€ æˆ– â€œç« èŠ‚N â€¦â€ ä½œä¸ºå‰ç¼€ï¼Œé¿å…é‡åã€‚
        - section çš„ç¼–å·ä¼˜å…ˆä½¿ç”¨ chunk_metadata.orderï¼›è‹¥æ— ï¼Œåˆ™é€€åŒ–ä¸º titleã€‚
        """
        entity_map: Dict[str, Entity] = {}
        self.chunk2section_map = {result["chunk_id"]: result["chunk_metadata"]["doc_title"] for result in extraction_results}
        self.section_entities_collection = dict()
        
        base = self.config.storage.knowledge_graph_path
        output_path = os.path.join(base, "chunk2section.json")
        # with open(output_path, "w") as f:
        #     json.dump(self.chunk2section_map, f)
    
        # ä¸­æ–‡å‰ç¼€è¯ï¼šScene â†’ åœºæ™¯ï¼›Chapter â†’ ç« èŠ‚
        for i, result in enumerate(extraction_results):
            md = result.get("chunk_metadata", {}) or {}
            label = md.get('doc_title', md.get('subtitle', md.get('title', "")))
            chunk_id = result.get("chunk_id", md.get("order", 0))
            
            if label not in self.section_entities_collection:
                self.section_entities_collection[label] = []
                
            # â€”â€” å¤„ç†å½“å‰ chunk æŠ½å–å‡ºçš„å®ä½“ â€”â€”
            for ent_data in result.get("entities", []):
                t = ent_data.get("type", "")
                is_event = (t == "Event") or (isinstance(t, list) and "Event" in t)
                is_action_like = (
                    (isinstance(t, str) and t in ["Action", "Emotion", "Goal"]) or
                    (isinstance(t, list) and any(x in ["Action", "Emotion", "Goal"] for x in t))
                )
                if is_event:
                    is_action_like = False
                            
                # å†²çªå¤„ç†ï¼šå±€éƒ¨å®ä½“é‡åå‰åŠ å‰ç¼€
                if (ent_data.get("scope", "").lower() == "local" or is_action_like) and ent_data["name"] in entity_map:
                    existing_entity = entity_map[ent_data["name"]]
                    existing_chunk_id = existing_entity.source_chunks[0]
                    existing_section_name = self.chunk2section_map[existing_chunk_id]
                    current_section_name = md["doc_title"]
                    if current_section_name != existing_section_name: # å¦‚æœä¸å±äºåŒç« èŠ‚çš„localï¼Œéœ€è¦é‡å‘½åã€‚
                        new_name = f"{ent_data['name']}_in_{chunk_id}"
                        suffix = 1
                        while new_name in entity_map:        # ä»å†²çªåˆ™è¿½åŠ  _n
                            suffix += 1
                            new_name = f"{ent_data['name']}_in_{chunk_id}_{suffix}"
                        ent_data["name"] = new_name

                # åˆ›å»º / åˆå¹¶
                ent_obj = self._create_entity_from_data(ent_data, result["chunk_id"])
                existing = self._find_existing_entity(ent_obj, entity_map)
                if existing:
                    self._merge_entities(existing, ent_obj)
                else:
                    entity_map[ent_obj.name] = ent_obj
                self.section_entities_collection[label].append(ent_obj)

        
        output_path = os.path.join(base, "section_entities_collection.pkl")
        # print("[CHECK] self.section_entities_collection: ", self.section_entities_collection)
        with open(output_path, "wb") as f:
            pickle.dump(self.section_entities_collection, f)
        
        return entity_map

    
    def _find_existing_entity(self, entity: Entity, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„å®ä½“"""
        if (entity.type == "Event") or (isinstance(entity.type, list) and "Event" in entity.type):
            return None
        if entity.name in entity_map:
            return entity_map[entity.name]
        for existing_entity in entity_map.values():
            if entity.name in existing_entity.aliases:
                return existing_entity
            if any(alias in existing_entity.aliases for alias in entity.aliases):
                return existing_entity
        return None
    
    def _merge_types(self, a, b):
        """
        æŠŠ a å’Œ b çš„ç±»å‹å¹¶é›†ååšè§„èŒƒåŒ–ï¼ˆEvent ä¼˜å…ˆ + å»é‡ä¿åºï¼‰
        """
        a_list = a if isinstance(a, list) else ([a] if a else [])
        b_list = b if isinstance(b, list) else ([b] if b else [])
        merged = []
        seen = set()
        for x in a_list + b_list:
            if x and x not in seen:
                seen.add(x)
                merged.append(x)
        if "Event" in seen:
            merged = ["Event"] + [x for x in merged if x != "Event"]
        return merged if len(merged) > 1 else (merged[0] if merged else "Concept")


    def _merge_entities(self, existing: Entity, new: Entity) -> None:
        """åˆå¹¶å®ä½“ä¿¡æ¯"""
        for alias in new.aliases:
            if alias not in existing.aliases:
                existing.aliases.append(alias)
        existing.properties.update(new.properties)
        for chunk_id in new.source_chunks:
            if chunk_id not in existing.source_chunks:
                existing.source_chunks.append(chunk_id)
        if new.description:
            if not existing.description:
                existing.description = new.description
            elif new.description not in existing.description:
                existing.description = existing.description + "\n" + new.description
                
        existing.type = self._merge_types(existing.type, new.type)
        
    def _ensure_entity_exists(self, entity_id: str, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        return entity_map.get(entity_id, None)

    # -------- Section / Contains --------
    def _create_section_entities(self, md: Dict[str, Any], chunk_id: str) -> List[Entity]:
        """
        åˆ›å»ºç« èŠ‚/åœºæ™¯å®ä½“ã€‚
        - title/subtitle æ€»æ˜¯ä» "title"/"subtitle" å­—æ®µè¯»å–
        - Entity.properties å†™å…¥æ˜ å°„å­—æ®µï¼ˆå¦‚ scene_nameï¼‰ + å…¶ä»–æœ‰ç”¨ metadata å­—æ®µ
        """
        raw_title = md.get("title", "").strip()
        raw_subtitle = md.get("subtitle", "").strip()
        order = md.get("order", None)

        if not raw_title:
            return []

        label = self.meta["section_label"]
        full_name = md.get("doc_title", f"{label}{raw_title}-{raw_subtitle}" if raw_subtitle else f"{label}{raw_title}")
        eid = f"{label.lower()}_{order}" if order is not None else f"{label.lower()}_{hash(full_name) % 1_000_000}"

        title_field = self.meta["title"]
        subtitle_field = self.meta["subtitle"]

        # æ„å»º propertiesï¼šå†™å…¥ title/subtitle æ˜ å°„å­—æ®µ + å…¶ä»–æœ‰æ•ˆå­—æ®µ
        excluded = {"chunk_index", "chunk_type", "doc_title", "title", "subtitle", "total_description_chunks", "total_doc_chunks"}
        properties = {
            title_field: raw_title,
            subtitle_field: raw_subtitle,
        }
        
        if order is not None:
            properties["order"] = order

        for k, v in md.items():
            if k not in excluded:
                properties[k] = v

        self.section_chunk_ids[eid].add(chunk_id)
        agg_chunks = sorted(self.section_chunk_ids[eid])

        return [
            Entity(
                id=eid,
                name=full_name,
                type=label,
                scope="local",  
                description=md.get("summary", ""),  # å¯é€‰ï¼šç”¨ summary ä½œä¸ºç®€è¦æè¿°
                properties=properties,
                source_chunks=agg_chunks 
            )
        ]


    def _link_section_to_entities(self, section: Entity, inners: List[Entity], chunk_id: str):
        pred = self.meta["contains_pred"]
        for tgt in inners:
            # print(f"[CHECK] linking {section.name} to {tgt.name}")
            rid = f"rel_{hash(f'{section.id}_{pred}_{tgt.id}') % 1_000_000}"
            self.kg.add_relation(
                Relation(id=rid, subject_id=section.id, predicate=pred,
                         object_id=tgt.id, properties={}, source_chunks=[chunk_id])
            )

    # -------- Entity / Relation creation --------
    
    @staticmethod
    def _create_entity_from_data(data: Dict, chunk_id: str) -> Entity:
        return Entity(id=f"ent_{hash(data['name']) % 1_000_000}",
                      name=data["name"],
                      type=_normalize_type(data.get("type", "Concept")),
                      scope=data.get("scope", "local"),
                      description=data.get("description", ""),
                      aliases=data.get("aliases", []),
                      source_chunks=[chunk_id])

    @staticmethod
    def _create_relation_from_data(d: Dict, chunk_id: str,
                                   entity_map: Dict[str, Entity],
                                   name2id: Dict[str, str]) -> Optional[Relation]:
        subj = d.get("subject") or d.get("source") or d.get("head") or d.get("relation_subject")
        obj = d.get("object") or d.get("target") or d.get("tail") or d.get("relation_object")
        pred = d.get("predicate") or d.get("relation") or d.get("relation_type")
        if not subj or not obj or not pred:
            return None
        sid, oid = name2id.get(subj), name2id.get(obj)
        if not sid or not oid:
            return None
        rid = f"rel_{hash(f'{sid}_{pred}_{oid}') % 1_000_000}"
        
        return Relation(
            id=rid,
            subject_id=sid,
            predicate=pred,
            object_id=oid,
            properties={
                "description": d.get("description", ""),
                "relation_name": d.get("relation_name", "")
            },
            source_chunks=[chunk_id]
        )


    def _store_vectordb(self, verbose: bool):
        try:
            all_documents = list(self.kg.documents.values())
            self.document_vector_store.delete_collection()
            self.document_vector_store._initialize()
            self.document_vector_store.store_documents(all_documents)
            
            all_sentences = []
            for doc in all_documents:
                content = doc.content
                sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', content)
                for i, sentence in enumerate(sentences):
                    sentence = sentence.replace("\\n", "").strip()
                    all_sentences.append(Document(id=f"{doc.id}-{i+1}", content=sentence, metadata=doc.metadata))
            
            self.sentence_vector_store.delete_collection()
            self.sentence_vector_store._initialize()
            self.sentence_vector_store.store_documents(all_sentences)
            
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")

    def _store_knowledge_graph(self, verbose: bool):
        try:
            self.graph_store.reset_knowledge_graph()
            self.graph_store.store_knowledge_graph(self.kg)
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Embedding & Stats
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_embedding_model(self.config.graph_embedding)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            # exclude_entity_types=[self.meta["section_label"]]
            # exclude_relation_types=[self.meta["contains_pred"]],
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… å›¾å‘é‡æ„å»ºå®Œæˆ")

    #
    def get_stats(self) -> Dict[str, Any]:
        return {
            "knowledge_graph": self.kg.stats(),
            "graph_store": self.graph_store.get_stats(),
            "document_vector_store": self.document_vector_store.get_stats(),
            "sentence_vector_store": self.sentence_vector_store.get_stats(),
        }
