# core/builder/graph_builder.py
from __future__ import annotations

import json
import os
import sqlite3
import pickle
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError, wait, FIRST_COMPLETED
import time
from collections import defaultdict
from copy import deepcopy
from typing import Any, Dict, List, Optional
import asyncio
import pandas as pd
import random
import re
import glob
from tqdm import tqdm
from core.utils.prompt_loader import PromptLoader
from core.utils.format import correct_json_format
from core.models.data import Entity, KnowledgeGraph, Relation, TextChunk, Document
from ..storage.document_store import DocumentStore
from ..storage.graph_store import GraphStore
from ..storage.vector_store import VectorStore
from core.memory.vector_memory import VectorMemory
from ..utils.config import KAGConfig
from ..utils.neo4j_utils import Neo4jUtils
from core.model_providers.openai_llm import OpenAILLM
from core.agent.knowledge_extraction_agent import InformationExtractionAgent
from core.agent.attribute_extraction_agent import AttributeExtractionAgent
from core.agent.graph_probing_agent import GraphProbingAgent
from .document_processor import DocumentProcessor
from core.builder.graph_preprocessor import GraphPreprocessor
from core.utils.format import DOC_TYPE_META
from core.builder.reflection import DynamicReflector
from collections import defaultdict

def _normalize_type(t):
    """
    - è¾“å…¥å¯ä¸º str æˆ– list
    - è‹¥åŒ…å« 'Event' â†’ æŠŠ 'Event' æ”¾åˆ°é¦–ä½
    - åŽ»é‡ä½†ä¿æŒåŽŸç›¸å¯¹é¡ºåº
    - è‹¥ä¸ºç©º â†’ è¿”å›ž 'Concept'
    """
    if not t:
        return "Concept"
    if isinstance(t, str):
        return t

    # list æƒ…å†µ
    seen = set()
    ordered = []
    for x in t:
        if x and x not in seen:
            seen.add(x)
            ordered.append(x)

    if "Event" in seen:
        ordered = ["Event"] + [x for x in ordered if x != "Event"]

    return ordered if len(ordered) > 1 else (ordered[0] if ordered else "Concept")
    
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               Builder
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æž„å»ºå™¨ï¼ˆæ”¯æŒå¤šæ–‡æ¡£æ ¼å¼ï¼‰"""
    def __init__(self, config: KAGConfig):
        self.doc_type = config.knowledge_graph_builder.doc_type
        if self.doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {self.doc_type}")
        
        self.config = config
        self.max_workers = config.knowledge_graph_builder.max_workers
        self.meta = DOC_TYPE_META[self.doc_type]
        self.section_chunk_ids = defaultdict(set)

        prompt_dir = config.knowledge_graph_builder.prompt_dir
        self.prompt_loader = PromptLoader(prompt_dir)
        self.llm = OpenAILLM(config)
        self.processor = DocumentProcessor(config, self.llm, self.doc_type)

        # å­˜å‚¨ / æ•°æ®åº“
        self.graph_store = GraphStore(config)
        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, doc_type=self.doc_type)
        self.document_vector_store = VectorStore(config, "documents")
        self.sentence_vector_store = VectorStore(config, "sentences")
        # self.document_store = DocumentStore(config)
        # åˆå§‹åŒ–è®°å¿†æ¨¡å—
        self.reflector = DynamicReflector(config)

        # è¿è¡Œæ•°æ®
        self.kg = KnowledgeGraph()
        self.probing_mode = self.config.probing.probing_mode
        self.graph_probing_agent = GraphProbingAgent(self.config, self.llm, self.reflector)
        
    def clear_directory(self, path):
        for file in glob.glob(os.path.join(path, "*.json")):
            try:
                os.remove(file)
            except Exception as e:
                print(f"åˆ é™¤å¤±è´¥: {file} -> {e}")

    def construct_system_prompt(self, background, abbreviations):
        
        background_info = self.get_background_info(background, abbreviations)
        
        if self.doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel"
            
        system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})
        return system_prompt_text
    
    def get_background_info(self, background, abbreviations):
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" 

        # ---------- 2) ç¼©å†™è¡¨ï¼ˆé”®åå®½å®¹ï¼‰ ----------
        def fmt(item: dict) -> str:
            """
            å°†ä¸€ä¸ªç¼©å†™é¡¹è½¬ä¸º Markdown åˆ—è¡¨æ¡ç›®ã€‚ä»»ä½•å­—æ®µéƒ½å¯é€‰ï¼Œæ ‡é¢˜å­—æ®µä¼˜å…ˆçº§ä¸ºï¼š
            abbr > full > å…¶ä»–å­—æ®µ > N/A
            """
            if not isinstance(item, dict):
                return ""

            # æ ‡é¢˜å­—æ®µä¼˜å…ˆçº§
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )

            # å‰©ä¸‹å­—æ®µåŽ»é™¤æ ‡é¢˜å­—æ®µ
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())

            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(item) for item in abbreviations if isinstance(item, dict))

        if background and abbr_block:
            background_info = f"{bg_block}\n{abbr_block}"
        else:
            background_info = bg_block or abbr_block
            
        return background_info
        
    def prepare_chunks(self, json_file_path: str, verbose: bool = True):
        """
        å¹¶å‘æ‹†åˆ†æ–‡æ¡£ä¸º TextChunkï¼ŒæŒ‰ã€å®Œæˆé¡ºåºã€‘æ”¶é›†ï¼›
        å•æ–‡æ¡£è½¯è¶…æ—¶=120sï¼ˆå¯è°ƒï¼‰ï¼Œè¶…æ—¶åˆ™è·³è¿‡å…¶å—å¹¶ç»§ç»­å¤„ç†å…¶å®ƒæ–‡æ¡£ã€‚
        """
        from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED
        import time
        from tqdm import tqdm
        import os, json

        PER_TASK_TIMEOUT = 120.0  # å¯è°ƒï¼›æ¯”å¦‚ 180

        # â€”â€” åˆå§‹åŒ–/æ¸…ç† â€”â€” #
        self.reflector.clear()
        base = self.config.storage.knowledge_graph_path
        self.clear_directory(base)
        if verbose:
            print(f"ðŸš€ å¼€å§‹æž„å»ºçŸ¥è¯†å›¾è°±: {json_file_path}")
            print("ðŸ“– åŠ è½½æ–‡æ¡£...")

        documents = self.processor.load_from_json(json_file_path, extract_metadata=True)
        if verbose:
            print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # â€”â€” å•æ–‡æ¡£æ‹†åˆ†ä»»åŠ¡ â€”â€” #
        def _run(doc):
            # æœŸæœ›è¿”å›ž {"document_chunks": List[TextChunk]}
            return self.processor.prepare_chunk(doc)

        # â€”â€” å¹¶å‘æ‰§è¡Œï¼šå®Œæˆå³æ”¶é›† + è½¯è¶…æ—¶ â€”â€” #
        all_chunks = []
        timeouts = []
        failures = []

        max_workers = getattr(self, "max_workers", getattr(self, "max_worker", 4))
        executor = ThreadPoolExecutor(max_workers=max_workers, thread_name_prefix="chunk")

        try:
            fut_info = {}  # future -> {"start": float, "doc": Any, "idx": int}
            for idx, d in enumerate(documents):
                f = executor.submit(_run, d)
                fut_info[f] = {"start": time.time(), "doc": d, "idx": idx}

            pbar = tqdm(total=len(fut_info), desc="å¹¶å‘æ‹†åˆ†ä¸­", ncols=100)
            pending = set(fut_info.keys())

            while pending:
                # 1) å…ˆæ”¶é›†å·²å®Œæˆçš„
                done, pending = wait(pending, timeout=0.25, return_when=FIRST_COMPLETED)
                for f in done:
                    info = fut_info.pop(f, None)
                    try:
                        grp = f.result()  # å·²å®Œæˆï¼Œä¸é˜»å¡ž
                        chunks = (grp or {}).get("document_chunks", [])
                        if isinstance(chunks, list):
                            all_chunks.extend(chunks)
                        else:
                            failures.append(info["idx"])
                    except Exception:
                        failures.append(info["idx"])
                    pbar.update(1)

                # 2) æ£€æŸ¥æœªå®Œæˆæ˜¯å¦è¶…è¿‡è½¯è¶…æ—¶ï¼›è¶…æ—¶åˆ™ä¸å†ç­‰å¾…
                now = time.time()
                to_forget = []
                for f in pending:
                    start = fut_info[f]["start"]
                    if now - start >= PER_TASK_TIMEOUT:
                        info = fut_info[f]
                        f.cancel()  # è‹¥å·²åœ¨è¿è¡Œåˆ™è¿”å›ž Falseï¼›æ— è®ºå¦‚ä½•æˆ‘ä»¬ä¸å†ç­‰å¾…
                        timeouts.append(info["idx"])
                        pbar.update(1)
                        to_forget.append(f)
                if to_forget:
                    for f in to_forget:
                        pending.remove(f)
                        fut_info.pop(f, None)

            pbar.close()
        finally:
            # ä¸ç­‰å¾…æœªå®Œæˆçš„çº¿ç¨‹ï¼›å–æ¶ˆé˜Ÿåˆ—é‡Œå°šæœªå¼€å§‹çš„ä»»åŠ¡ï¼Œé¿å…é€€å‡ºå¡ä½
            executor.shutdown(wait=False, cancel_futures=True)

        # â€”â€” è½ç›˜ â€”â€” #
        os.makedirs(base, exist_ok=True)
        out_path = os.path.join(base, "all_document_chunks.json")
        with open(out_path, "w", encoding="utf-8") as fw:
            json.dump([c.dict() for c in all_chunks], fw, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… ç”Ÿæˆ {len(all_chunks)} ä¸ªæ–‡æœ¬å—")
            if timeouts:
                print(f"â±ï¸ æœ‰ {len(timeouts)} ä¸ªæ–‡æ¡£åœ¨ {int(PER_TASK_TIMEOUT)}s å†…æœªå®Œæˆæ‹†åˆ†ï¼ˆå·²è·³è¿‡ï¼‰ï¼š{timeouts}")
            if failures:
                print(f"â—æœ‰ {len(failures)} ä¸ªæ–‡æ¡£æ‹†åˆ†å¤±è´¥ï¼ˆå·²è·³è¿‡ï¼‰ï¼š{failures}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  2) å­˜å‚¨ Chunkï¼ˆRDB + VDBï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def store_chunks(self, verbose: bool = True):
        base = self.config.storage.knowledge_graph_path

        # æè¿°å—
        doc_chunks = [TextChunk(**o) for o in
                       json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        # å†™å…¥ KGï¼ˆDocument + Chunkï¼‰
        for ch in doc_chunks:
            self.kg.add_document(self.processor.prepare_document(ch))
            self.kg.add_chunk(ch)

        # å†™å…¥å‘é‡æ•°æ®åº“
        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
            
        self._store_vectordb(verbose)
        
        
    def run_graph_probing(self, verbose: bool = True, sample_ratio: float = None):
        self.reflector.clear()
        
        base = self.config.storage.graph_schema_path
        os.makedirs(base, exist_ok=True)
        self.clear_directory(base)
        
        if self.probing_mode == "fixed" or self.probing_mode == "adjust" :
            schema = json.load(open(self.config.probing.default_graph_schema_path, "r", encoding="utf-8"))
            if os.path.exists(self.config.probing.default_background_path):
                settings = json.load(open(os.path.join(self.config.probing.default_background_path), "r", encoding="utf-8"))        
            else:
                settings = {"background": "", "abbreviations": []}
        else:
            schema = {}
            settings = {"background": "", "abbreviations": []}
        
        if self.probing_mode != "fixed":
            schema , settings = self.update_schema(schema, background=settings["background"], abbreviations=settings["abbreviations"],
                                                   verbose=verbose, sample_ratio=sample_ratio)
        else:
            if verbose:
                print("ðŸ“Œ è·³è¿‡probingæ­¥éª¤")
            
        with open(os.path.join(base, "graph_schema.json"), "w", encoding="utf-8") as f:
            json.dump(schema, f, ensure_ascii=False, indent=2)
            
        with open(os.path.join(base, "settings.json"), "w", encoding="utf-8") as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)
            
            
    def initialize_agents(self):
        
        base = self.config.storage.graph_schema_path
        schema_path = os.path.join(base, "graph_schema.json")
        settings_path = os.path.join(base, "settings.json")
        if os.path.exists(schema_path):
            schema = json.load(open(schema_path, "r", encoding="utf-8"))
        elif os.path.exists(self.config.probing.default_graph_schema_path):
            schema = json.load(open(self.config.probing.default_graph_schema_path, "r", encoding="utf-8"))
        else:
            raise FileNotFoundError("æ²¡æœ‰graph schemaï¼Œè¯·æä¾›æ­£ç¡®è·¯å¾„æˆ–è€…å…ˆè¿è¡Œprobing")
        
        if os.path.exists(settings_path):
            settings = json.load(open(settings_path, "r", encoding="utf-8"))
        elif os.path.exists(self.config.probing.default_background_path):
            settings = json.load(open(self.config.probing.default_background_path, "r", encoding="utf-8"))
        else:
            settings = {"background": "", "abbreviations": []}
 
        self.system_prompt_text = self.construct_system_prompt(
            background=settings["background"],
            abbreviations=settings["abbreviations"]
        )

        # æŠ½å– agent
        self.information_extraction_agent = InformationExtractionAgent(self.config, self.llm, self.system_prompt_text, schema, self.reflector)
        self.attribute_extraction_agent = AttributeExtractionAgent(self.config, self.llm, self.system_prompt_text, schema)
        self.graph_preprocessor = GraphPreprocessor(self.config, self.llm, system_prompt=self.system_prompt_text)
        
    def update_schema(self, schema: Dict = {}, background: str = "", abbreviations: List = [], verbose: bool = True, sample_ratio: float = None):
        
        base = self.config.storage.knowledge_graph_path
        doc_chunks = [TextChunk(**o) for o in
                       json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]
        if not sample_ratio:
            sample_ratio = 0.35
        k = int(len(doc_chunks) * sample_ratio)
        sampled_chunks = random.sample(doc_chunks, k=k)
        # ä¿å­˜ä¸€äº›æ´žå¯Ÿï¼Œç”¨äºŽä¹‹åŽçš„æŽ¢ç´¢ã€‚
        sampled_chunks = self.processor.extract_insights(sampled_chunks)
        
        for chunk in tqdm(sampled_chunks, desc="ä¿å­˜æ´žè§ä¸­", total=len(sampled_chunks)):
            insights = chunk.metadata.get("insights", []) 
            for item in insights:
                # print("[CHECK] insight", item)
                self.reflector.insight_memory.add(text=item, metadata={})
        if verbose:
            print("ðŸ’¾ ä¿å­˜æ´žè§å®Œæˆ")
            
        params = dict()
        params["documents"] = sampled_chunks   
        params["schema"] = schema  
        params["background"] = background
        params["abbreviations"] = abbreviations
        
        result = self.graph_probing_agent.run(params)
        # result = json.loads(correct_json_format(result))
        
        return result["schema"], result["settings"]
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  3) å®žä½“ / å…³ç³» æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def extract_entity_and_relation(self, verbose: bool = True):
        asyncio.run(self.extract_entity_and_relation_async(verbose=verbose))
            
    async def extract_entity_and_relation_async(self, verbose: bool = True):
        """
        å¹¶å‘æŠ½å– â†’ è®°å½•å¤±è´¥çš„ chunk â†’ ç»Ÿä¸€é‡è¯•ä¸€è½® â†’ å†™ç›˜
        """
        base = self.config.storage.knowledge_graph_path
        desc_chunks = [TextChunk(**o) for o in
                    json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        if verbose:
            print("ðŸ§  å®žä½“ä¸Žå…³ç³»ä¿¡æ¯å¼‚æ­¥æŠ½å–ä¸­...")

        sem = asyncio.Semaphore(self.max_workers)

        async def _arun_once(ch: TextChunk):
            async with sem:
                try:
                    if not ch.content.strip():
                        result = {"entities": [], "relations": []}
                    else:
                        result = await self.information_extraction_agent.arun(
                            ch.content,
                            timeout=self.config.agent.async_timeout,
                            max_attempts=self.config.agent.async_max_attempts,
                            backoff_seconds=self.config.agent.async_backoff_seconds
                        )
                    result.update(chunk_id=ch.id, chunk_metadata=ch.metadata)
                    return result
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] æŠ½å–å¤±è´¥ chunk_id={ch.id} | {e.__class__.__name__}: {e}")
                    return {
                        "chunk_id": ch.id,
                        "chunk_metadata": ch.metadata,
                        "entities": [],
                        "relations": [],
                        "error": f"{e.__class__.__name__}: {e}"
                    }

        async def _arun_with_ch(ch: TextChunk):
            """è¿”å›ž (chunk, result) æ–¹ä¾¿ç›´æŽ¥è®°å½•å¤±è´¥çš„ chunkã€‚"""
            res = await _arun_once(ch)
            return ch, res

        # ====== é¦–è½®å¹¶å‘ ======
        tasks = [_arun_with_ch(ch) for ch in desc_chunks]
        first_round_pairs = []   # [(ch, res), ...]
        failed_chs = []          # [ch, ...]

        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="å¼‚æ­¥æŠ½å–ä¸­"):
            ch, res = await coro
            first_round_pairs.append((ch, res))
            if res.get("error"):
                failed_chs.append(ch)

        # ====== ç»Ÿä¸€é‡è¯•ï¼ˆåªä¸€è½®ï¼‰======
        retry_pairs = []
        if failed_chs:
            if verbose:
                print(f"ðŸ”„ å¼€å§‹é‡è¯•å¤±è´¥çš„ {len(failed_chs)} ä¸ªæ–‡æœ¬å—...")
            retry_tasks = [_arun_with_ch(ch) for ch in failed_chs]
            for coro in tqdm(asyncio.as_completed(retry_tasks), total=len(retry_tasks), desc="é‡è¯•æŠ½å–ä¸­"):
                ch, res = await coro
                retry_pairs.append((ch, res))

        # ====== åˆå¹¶ç»“æžœï¼ˆç”¨å¤±è´¥çš„ ch è¿‡æ»¤é¦–è½®å¯¹åº”ç»“æžœï¼Œå†è¿½åŠ é‡è¯•ç»“æžœï¼‰======
        failed_ids = {ch.id for ch in failed_chs}
        final_results = [res for ch, res in first_round_pairs if ch.id not in failed_ids]
        final_results += [res for _, res in retry_pairs]

        # ====== è½ç›˜ ======
        output_path = os.path.join(base, "extraction_results.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)

        if verbose:
            still_failed = sum(1 for r in final_results if r.get("error"))
            print(f"âœ… å®žä½“ä¸Žå…³ç³»ä¿¡æ¯æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {len(final_results)} ä¸ªæ–‡æœ¬å—")
            if still_failed:
                print(f"âš ï¸ ä»æœ‰ {still_failed} ä¸ªæ–‡æœ¬å—åœ¨é‡è¯•åŽå¤±è´¥ï¼ˆä¿ç•™ error å­—æ®µä»¥ä¾¿æŽ’æŸ¥ï¼‰")
            print(f"ðŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  4) å±žæ€§æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def run_extraction_refinement(self, verbose=False):
        """
        å®žä½“æ¶ˆæ­§
        """
        KW = ("é—ªå›ž", "ä¸€ç»„è’™å¤ªå¥‡")
        base = self.config.storage.knowledge_graph_path
        extraction_results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))
        for doc in extraction_results:
            # 1) å…ˆåˆ å®žä½“ï¼šåå­—å«å…³é”®è¯çš„ä¸€å¾‹åˆ é™¤
            ents = doc.get("entities", [])
            removed_names = {e.get("name", "") for e in ents if any(k in e.get("name", "") for k in KW)}
            doc["entities"] = [e for e in ents if e.get("name", "") not in removed_names]

            # 2) å†åˆ å…³ç³»ï¼šsubject/object å«å…³é”®è¯ï¼Œæˆ–æŒ‡å‘å·²åˆ é™¤å®žä½“å
            rels = doc.get("relations", [])
            doc["relations"] = [
                r for r in rels
                if not any(k in r.get("subject", "") or k in r.get("object", "") for k in KW)
                and r.get("subject", "") not in removed_names
                and r.get("object", "") not in removed_names
            ]
    
        if verbose:
            print("ðŸ“Œ ä¼˜åŒ–å®žä½“ç±»åž‹")
        extraction_results =self.graph_preprocessor.refine_entity_types(extraction_results)
        if verbose:
            print("ðŸ“Œ ä¼˜åŒ–å®žä½“scope")
        extraction_results = self.graph_preprocessor.refine_entity_scope(extraction_results)
        if verbose:
            print("ðŸ“Œ å®žä½“æ¶ˆæ­§")
        extraction_results = self.graph_preprocessor.run_entity_disambiguation(extraction_results)
        
        base = self.config.storage.knowledge_graph_path
        with open(os.path.join(base, "extraction_results_refined.json"), "w") as f:
            json.dump(extraction_results, f, ensure_ascii=False, indent=2)
        

    def extract_entity_attributes(self, verbose: bool = True) -> Dict[str, Entity]:
        asyncio.run(self.extract_entity_attributes_async(verbose=verbose))
    
    async def extract_entity_attributes_async(self, verbose: bool = True) -> Dict[str, Entity]:
        """
        âš¡ å¼‚æ­¥æ‰¹é‡å±žæ€§æŠ½å–  
        Â· æŒ‰ extract_entity_and_relation_async ç”Ÿæˆçš„ entity_map åŽ»å¹¶å‘  
        Â· æ¯ä¸ªå®žä½“è°ƒç”¨ attribute_extraction_agent.arun()  
        Â· å†…éƒ¨ arun å·²å¸¦è¶…æ—¶ï¼‹é‡è¯•ä¿æŠ¤ï¼Œä¸ä¼šå¡æ­»
        """
        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results_refined.json"), "r", encoding="utf-8"))
        
        #print(results[0])
        # å°†å®žä½“åˆå¹¶ / åŽ»é‡
        entity_map = self.merge_entities_info(results)            # {name: Entity}

        if verbose:
            print(f"ðŸ”Ž å¼€å§‹å±žæ€§æŠ½å–ï¼ˆå¼‚æ­¥ï¼‰ï¼Œå®žä½“æ•°ï¼š{len(entity_map)}")

        sem = asyncio.Semaphore(self.max_workers)
        updated_entities: Dict[str, Entity] = {}

        async def _arun_attr(name: str, ent: Entity):
            async with sem:
                try:
                    txt = ent.description or ""
                    if not txt.strip():
                        return name, None

                    # AttributeExtractionAgent.arun å·²è‡ªå¸¦ timeout+é‡è¯•
                    res = await self.attribute_extraction_agent.arun(
                        text=txt,
                        entity_name=name,
                        entity_type=ent.type,
                        source_chunks=ent.source_chunks,
                        original_text="",
                        timeout=self.config.agent.async_timeout,
                        max_attempts=self.config.agent.async_max_attempts,
                        backoff_seconds=self.config.agent.async_backoff_seconds
                    )

                    if res.get("error"):          # è¶…æ—¶æˆ–å¼‚å¸¸
                        return name, None

                    attrs = res.get("attributes", {}) or {}
                    if isinstance(attrs, str):
                        try:
                            attrs = json.loads(attrs)
                        except json.JSONDecodeError:
                            attrs = {}

                    new_ent = deepcopy(ent)
                    new_ent.properties = attrs

                    nd = res.get("new_description", "")
                    if nd:
                        new_ent.description = nd

                    return name, new_ent
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] å±žæ€§æŠ½å–å¤±è´¥ï¼ˆå¼‚æ­¥ï¼‰ï¼š{name}: {e}")
                    return name, None

        # å¹¶å‘æ‰§è¡Œ
        tasks = [_arun_attr(n, e) for n, e in entity_map.items()]
        for coro in tqdm(asyncio.as_completed(tasks),
                               total=len(tasks),
                               desc="å±žæ€§æŠ½å–ä¸­ï¼ˆasyncï¼‰"):
            n, e2 = await coro
            if e2:
                updated_entities[n] = e2

        # å†™æ–‡ä»¶
        output_path = os.path.join(base, "entity_info.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({k: v.dict() for k, v in updated_entities.items()},
                      f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±žæ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®žä½“ {len(updated_entities)} ä¸ª")
            print(f"ðŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

        return updated_entities

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  5) æž„å»ºå¹¶å­˜å‚¨å›¾è°±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def build_graph_from_results(self, verbose: bool = True) -> KnowledgeGraph:
        if verbose:
            print("ðŸ“‚ åŠ è½½å·²æœ‰æŠ½å–ç»“æžœå’Œå®žä½“ä¿¡æ¯...")

        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results_refined.json"), "r", encoding="utf-8"))
        ent_raw = json.load(open(os.path.join(base, "entity_info.json"), "r", encoding="utf-8"))
        
        with open(os.path.join(base, "section_entities_collection.pkl"), "rb") as f:
            self.section_entities_collection = pickle.load(f)
            
       #  self.section_entities_collection = json.load(open(os.path.join(base, "section_entities_collection.json"), "r", encoding="utf-8"))
        
        # id â†’ Entity
        entity_map = {d["id"]: Entity(**d) for d in ent_raw.values()}
        name2id: Dict[str, str] = {e.name: e.id for e in entity_map.values()}
        
        for e in entity_map.values():
            for al in e.aliases:
                name2id.setdefault(al, e.id)
            self.kg.add_entity(e)

        if verbose:
            print("ðŸ”— æž„å»ºçŸ¥è¯†å›¾è°±...")

        self.section_names = []
        for res in results:
            md = res.get("chunk_metadata", {})
            # Section å®žä½“
            secs = self._create_section_entities(md, res["chunk_id"])
            for se in secs:
                # if se.id not in self.kg.entities:
                if se.name not in self.section_names and se.id not in self.kg.entities:
                    self.kg.add_entity(se)
                    self.section_names.append(se.name)
                else:
                    exist = self.kg.entities.get(se.id)
                    if exist:
                        # åŽ»é‡å¹¶ä¿æŒç¨³å®šé¡ºåº
                        merged = list(dict.fromkeys(list(exist.source_chunks) + list(se.source_chunks)))
                        exist.source_chunks = merged
                

            inner = self.section_entities_collection[se.name]
            for se in secs:
                self._link_section_to_entities(se, inner, res["chunk_id"])

            # æ™®é€šå…³ç³»
            for rdata in res.get("relations", []):
                rel = self._create_relation_from_data(rdata, res["chunk_id"], entity_map, name2id)
                if rel:
                    self.kg.add_relation(rel)

        # å†™å…¥æ•°æ®åº“
        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°æ•°æ®åº“...")
        self._store_knowledge_graph(verbose)
        self.neo4j_utils.enrich_event_nodes_with_context()

        if verbose:
            st = self.kg.stats()
            print(f"ðŸŽ‰ çŸ¥è¯†å›¾è°±æž„å»ºå®Œæˆ!")
            # print(f"   - å®žä½“æ•°é‡: {st['entities']}")
            # print(f"   - å…³ç³»æ•°é‡: {st['relations']}")
            graph_stats = self.graph_store.get_stats()
            print(f"   - å®žä½“æ•°é‡: {graph_stats['entities']}")
            print(f"   - å…³ç³»æ•°é‡: {graph_stats['relations']}")
            print(f"   - æ–‡æ¡£æ•°é‡: {st['documents']}")
            print(f"   - æ–‡æœ¬å—æ•°é‡: {st['chunks']}")

        return self.kg

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  å†…éƒ¨å·¥å…·
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # -------- åˆå¹¶å®žä½“ï¼ˆæ ¹æ® doc_type é€‚é…ï¼‰ --------
    def merge_entities_info(self, extraction_results):
        """
        éåŽ†ä¿¡æ¯æŠ½å–ç»“æžœï¼Œåˆå¹¶ / åŽ»é‡å®žä½“ã€‚
        - â€œå±€éƒ¨ä½œç”¨åŸŸâ€å®žä½“ï¼ˆscope == localï¼‰è‹¥å‘½åå†²çªï¼Œä¼šåœ¨å‰é¢åŠ ä¸Š
          â€œåœºæ™¯N â€¦â€ æˆ– â€œç« èŠ‚N â€¦â€ ä½œä¸ºå‰ç¼€ï¼Œé¿å…é‡åã€‚
        - section çš„ç¼–å·ä¼˜å…ˆä½¿ç”¨ chunk_metadata.orderï¼›è‹¥æ— ï¼Œåˆ™é€€åŒ–ä¸º titleã€‚
        """
        entity_map: Dict[str, Entity] = {}
        self.chunk2section_map = {result["chunk_id"]: result["chunk_metadata"]["doc_title"] for result in extraction_results}
        self.section_entities_collection = dict()
        
        base = self.config.storage.knowledge_graph_path
        output_path = os.path.join(base, "chunk2section.json")
        # with open(output_path, "w") as f:
        #     json.dump(self.chunk2section_map, f)
    
        # ä¸­æ–‡å‰ç¼€è¯ï¼šScene â†’ åœºæ™¯ï¼›Chapter â†’ ç« èŠ‚
        for result in extraction_results:
            md = result.get("chunk_metadata", {}) or {}
            label = md.get('doc_title', md.get('subtitle', md.get('title', "")))
            
            if label not in self.section_entities_collection:
                self.section_entities_collection[label] = []
                
            # â€”â€” å¤„ç†å½“å‰ chunk æŠ½å–å‡ºçš„å®žä½“ â€”â€”
            for ent_data in result.get("entities", []):
                t = ent_data.get("type", "")
                is_event = (t == "Event") or (isinstance(t, list) and "Event" in t)
                is_action_like = (
                    (isinstance(t, str) and t in ["Action", "Emotion", "Goal"]) or
                    (isinstance(t, list) and any(x in ["Action", "Emotion", "Goal"] for x in t))
                )
                if is_event:
                    is_action_like = False
                            
                # å†²çªå¤„ç†ï¼šå±€éƒ¨å®žä½“é‡åå‰åŠ å‰ç¼€
                if (ent_data.get("scope", "").lower() == "local" or is_action_like) and ent_data["name"] in entity_map:
                    existing_entity = entity_map[ent_data["name"]]
                    existing_chunk_id = existing_entity.source_chunks[0]
                    existing_section_name = self.chunk2section_map[existing_chunk_id]
                    current_section_name = md["doc_title"]
                    if current_section_name != existing_section_name: # å¦‚æžœä¸å±žäºŽåŒç« èŠ‚çš„localï¼Œéœ€è¦é‡å‘½åã€‚
                        new_name = f"{ent_data['name']}_in_{label}"
                        suffix = 1
                        while new_name in entity_map:        # ä»å†²çªåˆ™è¿½åŠ  _n
                            suffix += 1
                            new_name = f"{ent_data['name']}_in_{label}_{suffix}"
                        ent_data["name"] = new_name

                # åˆ›å»º / åˆå¹¶
                ent_obj = self._create_entity_from_data(ent_data, result["chunk_id"])
                existing = self._find_existing_entity(ent_obj, entity_map)
                if existing:
                    self._merge_entities(existing, ent_obj)
                else:
                    entity_map[ent_obj.name] = ent_obj
                self.section_entities_collection[label].append(ent_obj)

        
        output_path = os.path.join(base, "section_entities_collection.pkl")
        # print("[CHECK] self.section_entities_collection: ", self.section_entities_collection)
        with open(output_path, "wb") as f:
            pickle.dump(self.section_entities_collection, f)
        
        return entity_map

    
    def _find_existing_entity(self, entity: Entity, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„å®žä½“"""
        if (entity.type == "Event") or (isinstance(entity.type, list) and "Event" in entity.type):
            return None
        if entity.name in entity_map:
            return entity_map[entity.name]
        for existing_entity in entity_map.values():
            if entity.name in existing_entity.aliases:
                return existing_entity
            if any(alias in existing_entity.aliases for alias in entity.aliases):
                return existing_entity
        return None
    
    def _merge_types(self, a, b):
        """
        æŠŠ a å’Œ b çš„ç±»åž‹å¹¶é›†åŽåšè§„èŒƒåŒ–ï¼ˆEvent ä¼˜å…ˆ + åŽ»é‡ä¿åºï¼‰
        """
        a_list = a if isinstance(a, list) else ([a] if a else [])
        b_list = b if isinstance(b, list) else ([b] if b else [])
        merged = []
        seen = set()
        for x in a_list + b_list:
            if x and x not in seen:
                seen.add(x)
                merged.append(x)
        if "Event" in seen:
            merged = ["Event"] + [x for x in merged if x != "Event"]
        return merged if len(merged) > 1 else (merged[0] if merged else "Concept")


    def _merge_entities(self, existing: Entity, new: Entity) -> None:
        """åˆå¹¶å®žä½“ä¿¡æ¯"""
        for alias in new.aliases:
            if alias not in existing.aliases:
                existing.aliases.append(alias)
        existing.properties.update(new.properties)
        for chunk_id in new.source_chunks:
            if chunk_id not in existing.source_chunks:
                existing.source_chunks.append(chunk_id)
        if new.description:
            if not existing.description:
                existing.description = new.description
            elif new.description not in existing.description:
                existing.description = existing.description + "\n" + new.description
                
        existing.type = self._merge_types(existing.type, new.type)
        
    def _ensure_entity_exists(self, entity_id: str, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        return entity_map.get(entity_id, None)

    # -------- Section / Contains --------
    def _create_section_entities(self, md: Dict[str, Any], chunk_id: str) -> List[Entity]:
        """
        åˆ›å»ºç« èŠ‚/åœºæ™¯å®žä½“ã€‚
        - title/subtitle æ€»æ˜¯ä»Ž "title"/"subtitle" å­—æ®µè¯»å–
        - Entity.properties å†™å…¥æ˜ å°„å­—æ®µï¼ˆå¦‚ scene_nameï¼‰ + å…¶ä»–æœ‰ç”¨ metadata å­—æ®µ
        """
        raw_title = md.get("title", "").strip()
        raw_subtitle = md.get("subtitle", "").strip()
        order = md.get("order", None)

        if not raw_title:
            return []

        label = self.meta["section_label"]
        full_name = md.get("doc_title", f"{label}{raw_title}-{raw_subtitle}" if raw_subtitle else f"{label}{raw_title}")
        eid = f"{label.lower()}_{order}" if order is not None else f"{label.lower()}_{hash(full_name) % 1_000_000}"

        title_field = self.meta["title"]
        subtitle_field = self.meta["subtitle"]

        # æž„å»º propertiesï¼šå†™å…¥ title/subtitle æ˜ å°„å­—æ®µ + å…¶ä»–æœ‰æ•ˆå­—æ®µ
        excluded = {"chunk_index", "chunk_type", "doc_title", "title", "subtitle", "total_description_chunks", "total_doc_chunks"}
        properties = {
            title_field: raw_title,
            subtitle_field: raw_subtitle,
        }
        
        if order is not None:
            properties["order"] = order

        for k, v in md.items():
            if k not in excluded:
                properties[k] = v

        self.section_chunk_ids[eid].add(chunk_id)
        agg_chunks = sorted(self.section_chunk_ids[eid])

        return [
            Entity(
                id=eid,
                name=full_name,
                type=label,
                description=md.get("summary", ""),  # å¯é€‰ï¼šç”¨ summary ä½œä¸ºç®€è¦æè¿°
                properties=properties,
                source_chunks=agg_chunks 
            )
        ]


    def _link_section_to_entities(self, section: Entity, inners: List[Entity], chunk_id: str):
        pred = self.meta["contains_pred"]
        for tgt in inners:
            rid = f"rel_{hash(f'{section.id}_{pred}_{tgt.id}') % 1_000_000}"
            self.kg.add_relation(
                Relation(id=rid, subject_id=section.id, predicate=pred,
                         object_id=tgt.id, properties={}, source_chunks=[chunk_id])
            )

    # -------- Entity / Relation creation --------
    
    @staticmethod
    def _create_entity_from_data(data: Dict, chunk_id: str) -> Entity:
        return Entity(id=f"ent_{hash(data['name']) % 1_000_000}",
                      name=data["name"],
                      type=_normalize_type(data.get("type", "Concept")),
                      scope=data.get("scope", "local"),
                      description=data.get("description", ""),
                      aliases=data.get("aliases", []),
                      source_chunks=[chunk_id])

    @staticmethod
    def _create_relation_from_data(d: Dict, chunk_id: str,
                                   entity_map: Dict[str, Entity],
                                   name2id: Dict[str, str]) -> Optional[Relation]:
        subj = d.get("subject") or d.get("source") or d.get("head") or d.get("relation_subject")
        obj = d.get("object") or d.get("target") or d.get("tail") or d.get("relation_object")
        pred = d.get("predicate") or d.get("relation") or d.get("relation_type")
        if not subj or not obj or not pred:
            return None
        sid, oid = name2id.get(subj), name2id.get(obj)
        if not sid or not oid:
            return None
        rid = f"rel_{hash(f'{sid}_{pred}_{oid}') % 1_000_000}"
        
        return Relation(
            id=rid,
            subject_id=sid,
            predicate=pred,
            object_id=oid,
            properties={
                "description": d.get("description", ""),
                "relation_name": d.get("relation_name", "")
            },
            source_chunks=[chunk_id]
        )

    # -------- å­˜å‚¨ --------
    # def _build_relational_database(self, dialog_chunks: List[TextChunk]):
    #     rows = [{
    #         "id": c.id,
    #         "content": c.content.split("ï¼š")[-1].strip(),
    #         "character": c.metadata.get("character", ""),
    #         "type": c.metadata.get("type") or "regular",
    #         "remark": "ï¼Œ".join(c.metadata.get("remark", [])),
    #         "title": c.metadata.get("title", ""),
    #         "subtitle": c.metadata.get("subtitle", ""),
    #     } for c in dialog_chunks]

    #     db_dir = self.config.storage.sql_database_path
    #     os.makedirs(db_dir, exist_ok=True)
    #     db_path = os.path.join(db_dir, "conversations.db")
    #     if os.path.exists(db_path):
    #         os.remove(db_path)
    #     df = pd.DataFrame(rows)
    #     df.to_sql("dialogues", sqlite3.connect(db_path), if_exists="replace", index=False)

    def _store_vectordb(self, verbose: bool):
        try:
            all_documents = list(self.kg.documents.values())
            self.document_vector_store.delete_collection()
            self.document_vector_store._initialize()
            self.document_vector_store.store_documents(all_documents)
            
            all_sentences = []
            for doc in all_documents:
                content = doc.content
                sentences = re.split(r'(?<=[ã€‚ï¼ï¼Ÿ])', content)
                for i, sentence in enumerate(sentences):
                    all_sentences.append(Document(id=f"{doc.id}-{i+1}", content=sentence, metadata=doc.metadata))
            
            self.sentence_vector_store.delete_collection()
            self.sentence_vector_store._initialize()
            self.sentence_vector_store.store_documents(all_sentences)
            
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")

    def _store_knowledge_graph(self, verbose: bool):
        try:
            self.graph_store.store_knowledge_graph(self.kg)
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Embedding & Stats
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_embedding_model(self.config.graph_embedding)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            # exclude_entity_types=[self.meta["section_label"]]
            # exclude_relation_types=[self.meta["contains_pred"]],
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… å›¾å‘é‡æž„å»ºå®Œæˆ")

    #
    def get_stats(self) -> Dict[str, Any]:
        return {
            "knowledge_graph": self.kg.stats(),
            "graph_store": self.graph_store.get_stats(),
            "vector_store": self.vector_store.get_stats(),
        }
