import json
import os
from typing import List, Dict, Any, Optional

from langchain.text_splitter import RecursiveCharacterTextSplitter
from tqdm import tqdm
from itertools import chain

from ..utils.config import KAGConfig
# from kag.functions.regular_functions import MetadataParser, SemanticSplitter
from kag.builder.document_parser import DocumentParser
from kag.utils.format import correct_json_format, safe_text_for_json
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
import numpy as np
from collections import defaultdict
from kag.utils.format import correct_json_format


def compute_weighted_similarity_and_laplacian(entity_dict, alpha=0.8, knn_k=40, top_k=10):
    names = list(entity_dict.keys())
    name_embs = np.vstack([entity_dict[n]['name_embedding'] for n in names])
    desc_embs = np.vstack([entity_dict[n]['description_embedding'] for n in names])

    # åˆ†åˆ«è®¡ç®— name å’Œ description çš„ç›¸ä¼¼åº¦
    sim_name = cosine_similarity(name_embs)
    sim_desc = cosine_similarity(desc_embs)
    
    # åŠ æƒèåˆ
    sim = alpha * sim_name + (1 - alpha) * sim_desc
    
    # æ„å»ºé‚»æ¥çŸ©é˜µï¼ˆKNNå›¾ï¼‰
    n = sim.shape[0]
    adj = np.zeros((n, n))
    for i in range(n):
        idx = np.argsort(sim[i])[-(knn_k+1):-1]  # æ’é™¤è‡ªå·±
        adj[i, idx] = sim[i, idx]
    adj = np.maximum(adj, adj.T)  # å¯¹ç§°åŒ–

    # æ„å»ºå›¾æ‹‰æ™®æ‹‰æ–¯çŸ©é˜µ
    deg = np.diag(adj.sum(axis=1))
    lap = deg - adj

    # ç‰¹å¾å€¼è®¡ç®—
    eigvals = np.linalg.eigvalsh(lap)
    gaps = np.diff(eigvals)
    # print("gaps: ", gaps[0], gaps[1])
    estimated_k = int(np.argmax(gaps[1:]) + 1)  # è·³è¿‡ç¬¬ä¸€ä¸ªgap
    
    return estimated_k, sim

    
def run_kmeans_clustering(entity_dict, n_clusters, alpha=0.8):
    """
    ä½¿ç”¨ KMeans å¯¹å®ä½“èšç±»ï¼ˆæ”¯æŒ name/desc embedding åŠ æƒæ‹¼æ¥ï¼‰
    """
    names = list(entity_dict.keys())
    name_embs = np.vstack([entity_dict[n]['name_embedding'] for n in names])
    desc_embs = np.vstack([entity_dict[n]['description_embedding'] for n in names])

    # åŠ æƒæ‹¼æ¥
    combined_embs = np.hstack([
        name_embs * alpha,
        desc_embs * (1 - alpha)
    ])

    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(combined_embs)

    cluster_result = defaultdict(list)
    for name, label in zip(names, labels):
        cluster_result[label].append(name)

    clusters = dict(cluster_result)
    collected_clusters = []
    for label, group in clusters.items():
        if len(group) >= 2:
            # print(f"\nğŸ“¦ Cluster {label}:")
            collected_clusters.append(group)
    return collected_clusters


class GraphPreprocessor:
    """é€šç”¨æ–‡æ¡£å¤„ç†å™¨"""

    # ------------------------------------------------------------------ #
    # åˆå§‹åŒ–
    # ------------------------------------------------------------------ #
    def __init__(self, config: KAGConfig, llm, system_prompt):
        self.config = config        
        self.system_prompt_text = system_prompt
        
        self.document_parser = DocumentParser(config, llm)
        self.model = self.load_embedding_model(config.memory.embedding_model_name)
        self.max_worker = 16
        # self.rename = dict()
            
    def load_embedding_model(self, model_name):
        if self.config.embedding.provider == "openai":
            from kag.model_providers.openai_embedding import OpenAIEmbeddingModel
            model = OpenAIEmbeddingModel(self.config)
        else:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer(model_name)
        return model
        
    def collect_global_entities(self, extraction_results):                
        global_entities = dict() # collect global entities by type
        for result in extraction_results:
            entities = result["entities"]
            for entity in entities:
                if entity["scope"] == "global" and entity["type"] in ["Character", "Object", "Concept", "Event"]:
                    if entity["type"] in global_entities:
                        global_entities[entity["type"]].append(entity)
                    else:
                        global_entities[entity["type"]] =[entity]
         
        merged_global_entities = dict()
        for type in global_entities:
            filtered_entities = dict()
            for entity in global_entities[type]:
                if entity["name"] in filtered_entities:
                    filtered_entities[entity["name"]]["description"] += entity["description"]
                else:
                    filtered_entities[entity["name"]] = entity
                merged_global_entities[type] = filtered_entities
    
        return merged_global_entities
    
    def compute_embeddings(self, filtered_entities):
        # print("[CHECK] filtered_entities: ", type(filtered_entities))
        for entity in tqdm(filtered_entities):
            # entity = filtered_entities[entity_name]
            name_embedding = self.model.encode(entity["name"])
            description_embedding = self.model.encode(entity.get("summary", entity["description"])) # ä¼˜å…ˆä½¿ç”¨summary
            entity["name_embedding"] = name_embedding
            entity["description_embedding"] = description_embedding
        return filtered_entities        

    def add_entity_summary(self, merged_global_entities):
        """
        ä½¿ç”¨å¤šçº¿ç¨‹å¹¶å‘ç”Ÿæˆå®ä½“æ‘˜è¦ï¼ˆè‹¥ description è¶³å¤Ÿé•¿ï¼‰ï¼Œæ›´æ–° merged_global_entitiesã€‚
        """
        # å±•å¼€ entity åˆ—è¡¨ï¼ˆæ–¹ä¾¿å¹¶å‘å¤„ç†ï¼‰
        entity_list = []
        for type in merged_global_entities:
            for entity in merged_global_entities[type].values():
                entity_list.append(entity)

        # å®šä¹‰å¤„ç†å‡½æ•°
        def summarize_entity(entity):
            try:
                if len(entity["description"]) >= 300:
                    result = self.document_parser.summarize_paragraph(
                        text=entity["description"], max_length=250
                    )
                    result = json.loads(correct_json_format(result))
                    summary = result["summary"]
                else:
                    summary = entity["description"]
                entity["summary"] = summary
            except Exception as e:
                entity["summary"] = entity["description"]  # fallback
                print(f"â—æ‘˜è¦å¤±è´¥: {entity['name']} -> {str(e)}")
            return entity

        # å¤šçº¿ç¨‹å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_worker) as executor:
            futures = [executor.submit(summarize_entity, entity) for entity in entity_list]
            entity_list_updated = [future.result() for future in tqdm(as_completed(futures), total=len(futures), desc="ç”Ÿæˆæ‘˜è¦")]

        # é‡æ–°æŒ‰ç±»å‹èšåˆï¼ˆè¿”å›ç»“æ„ä¸€è‡´ï¼‰
        merged_global_entities_new = dict()
        for entity in entity_list_updated:
            if entity["type"] in merged_global_entities_new:
                merged_global_entities_new[entity["type"]].append(entity)
            else:
                merged_global_entities_new[entity["type"]] = [entity]

        return merged_global_entities_new

    # def add_entity_summary(self, merged_global_entities):
        
    #     # å±•å¼€ï¼Œæ–¹ä¾¿å¤šçº¿ç¨‹å¹¶å‘
    #     entity_list = []
    #     for type in merged_global_entities:
    #         for entities in merged_global_entities[type]:
    #             entity_list.extend(entities)
                
    #     for entity in entity_list: # è½¬æˆå¤šçº¿ç¨‹å¹¶å‘ï¼š
    #         if len(entity["description"]) >= 300:
    #             result = self.document_parser.paragraph_summarizer(text=entity["description"], max_length=250)
    #             result = json.loads(correct_json_format(result))
    #             summary = result["summary"]
    #         else:
    #             summary = entity["description"]
    #         entity["summary"] = summary
        
    #     merged_global_entities_new = dict() # é‡æ–°åˆå¹¶
    #     for entity in entity_list:
    #         if entity["type"] in merged_global_entities_new:
    #             merged_global_entities_new[entity["type"]].append(entity)
    #         else:
    #             merged_global_entities_new[entity["type"]] = [entity]
                
    #     return merged_global_entities_new
           
    def detect_candidates(self, merged_global_entities):
        candidates = []
        for type in merged_global_entities:
            filtered_entities = dict()
            for entity in merged_global_entities[type].copy():
                if entity["name"] in filtered_entities:
                    filtered_entities[entity["name"]]["description"] += entity["description"]
                else:
                    filtered_entities[entity["name"]] = entity
                    
            knn_k = min(int(len(filtered_entities)/4) ,25) 
            estimated_k, sim_matrix = compute_weighted_similarity_and_laplacian(filtered_entities, alpha=0.8, knn_k=25)
            n_clusters=int((estimated_k+len(filtered_entities)/2)/2)
            collected_clusters = run_kmeans_clustering(
                filtered_entities,
                n_clusters=n_clusters,
                alpha=0.5
            )
            candidates.extend(collected_clusters)
            
        return candidates
    
    # def merge_entities(self, all_candidates_with_info):
    #     rename_map = dict()
    #     for candidate in all_candidates_with_info:
    #         entity_descriptions = ""
    #         for i, entity in enumerate(candidate):
    #             entity_name = entity["name"]
    #             entity_summary = entity.get("summary", entity["description"])
    #             entity_descriptions += f"å®ä½“{i+1}çš„åç§°ï¼š{entity_name}\n{entity_summary}\n"
            
    #         result = self.document_parser.merge_entities(entity_descriptions=entity_descriptions, system_prompt=self.system_prompt_text)
    #         result = json.loads(correct_json_format(result))
    #         merges = result["merges"]
    #         unmerged = result["unmerged"]
    #         for merge in merges:
    #             for alias in merge["aliases"]:
    #                 rename_map[alias] = merge["canonical_name"]
        
    #     return rename_map
    
    def merge_entities(self, all_candidates_with_info):
        """
        å¹¶å‘è°ƒç”¨ LLM åˆå¹¶åˆ¤æ–­ï¼Œè¿”å› alias â†’ canonical_name çš„é‡å‘½åæ˜ å°„è¡¨
        """
        rename_map = dict()

        # å•ä¸ªå€™é€‰ç»„å¤„ç†é€»è¾‘
        def process_group(candidate_group):
            try:
                entity_descriptions = ""
                for i, entity in enumerate(candidate_group):
                    entity_name = entity["name"]
                    entity_summary = entity.get("summary", entity["description"])
                    entity_descriptions += f"å®ä½“{i+1}çš„åç§°ï¼š{entity_name}\n{entity_summary}\n"

                result = self.document_parser.merge_entities(
                    entity_descriptions=entity_descriptions,
                    system_prompt=self.system_prompt_text
                )
                result = json.loads(correct_json_format(result))
                return result  # è¿”å›å®Œæ•´ç»“æœç»“æ„
            except Exception as e:
                print(f"â—å®ä½“åˆå¹¶å¤±è´¥: {[e['name'] for e in candidate_group]} -> {e}")
                return {"merges": [], "unmerged": []}

        # å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_worker) as executor:
            futures = [executor.submit(process_group, group) for group in all_candidates_with_info]
            results = [future.result() for future in tqdm(as_completed(futures), total=len(futures), desc="å®ä½“åˆå¹¶åˆ¤æ–­")]

        # èšåˆé‡å‘½åæ˜ å°„
        for result in results:
            for merge in result.get("merges", []):
                canonical = merge["canonical_name"]
                for alias in merge.get("aliases", []):
                    rename_map[alias] = canonical

        return rename_map
    
    def run_entity_disambiguation(self, extraction_results):
        merged_global_entities = self.collect_global_entities(extraction_results)
        merged_global_entities = self.add_entity_summary(merged_global_entities)
        
        for type in merged_global_entities:
            merged_global_entities[type] = self.compute_embeddings(merged_global_entities[type])
        
        all_candidates = self.detect_candidates(merged_global_entities)
        
        entity_info_map = dict()
        for type in merged_global_entities:
            entities = merged_global_entities[type]
            for entity in entities:
                #print("[CHECK] entity: ", entity)
                entity_info_map[entity["name"]] = entity
        
        all_candidates_with_info = []
        for candidates in all_candidates:
            group = []
            for entity in candidates:
                group.append(entity_info_map[entity])
            all_candidates_with_info.append(group)
        rename_map = self.merge_entities(all_candidates_with_info)
        
        base = self.config.storage.knowledge_graph_path
        os.makedirs(base, exist_ok=True)
        json.dump(rename_map,
                  open(os.path.join(base, "rename_map.json"), "w", encoding="utf-8"),
                  ensure_ascii=False, indent=2)
        
        for result in extraction_results:
            for entity in result["entities"]:
                entity["name"] = rename_map.get(entity["name"], entity["name"])
                
            for relation in result["relations"]:
                relation["subject"] = rename_map.get(relation["subject"], relation["subject"])
                relation["object"] = rename_map.get(relation["object"], relation["object"])
        
        return extraction_results