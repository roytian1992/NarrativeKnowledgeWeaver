"""
äº‹ä»¶å› æœå›¾æ„å»ºå™¨
è´Ÿè´£æ„å»ºäº‹ä»¶å› æœå…³ç³»çš„æœ‰å‘å¸¦æƒå›¾å’Œæƒ…èŠ‚å•å…ƒå›¾è°±
"""

import json
import pickle
import networkx as nx
from typing import List, Dict, Tuple, Optional, Any, Set
from tqdm import tqdm
from pathlib import Path
from kag.llm.llm_manager import LLMManager
from kag.utils.neo4j_utils import Neo4jUtils
from kag.models.data import Entity
from kag.builder.graph_analyzer import GraphAnalyzer
from kag.storage.graph_store import GraphStore
from kag.storage.vector_store import VectorStore
from kag.utils.prompt_loader import PromptLoader
from kag.functions.regular_functions.plot_generation import PlotGenerator
from concurrent.futures import ThreadPoolExecutor, as_completed
from kag.utils.format import correct_json_format
import logging
from collections import defaultdict
import os
from kag.builder.kg_builder import DOC_TYPE_META

class EventCausalityBuilder:
    """
    äº‹ä»¶å› æœå›¾æ„å»ºå™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä»Neo4jåŠ è½½å’Œæ’åºäº‹ä»¶
    2. é€šè¿‡è¿é€šä½“å’Œç¤¾åŒºè¿‡æ»¤äº‹ä»¶å¯¹
    3. ä½¿ç”¨extractoræ£€æŸ¥å› æœå…³ç³»
    4. æ„å»ºæœ‰å‘å¸¦æƒNetworkXå›¾
    5. ä¿å­˜å’ŒåŠ è½½å›¾æ•°æ®
    6. æ„å»ºPlotæƒ…èŠ‚å•å…ƒå›¾è°±
    """
    
    def __init__(self, config, doc_type="novel", background_path: str = ""):
        """
        åˆå§‹åŒ–äº‹ä»¶å› æœå›¾æ„å»ºå™¨
        
        Args:
            config: KAGé…ç½®å¯¹è±¡
        """
        self.config = config
        self.llm_manager = LLMManager(config)
        self.llm = self.llm_manager.get_llm()
        self.graph_store = GraphStore(config)
        self.vector_store = VectorStore(config)
        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, doc_type)
        self.neo4j_utils.load_emebdding_model(config.memory.embedding_model_name)
        self.event_fallback = [] # å¯ä»¥åŠ å…¥Goalå’ŒAction
        
        if doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {doc_type}")
        self.doc_type = doc_type
        self.meta = DOC_TYPE_META[doc_type]

        # åˆå§‹åŒ–Plotç›¸å…³ç»„ä»¶
        prompt_dir = config.prompt_dir if hasattr(config, 'prompt_dir') else os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "kag/prompts")
        self.prompt_loader = PromptLoader(prompt_dir)
        
        self.background_info = ""
        if background_path:
            print("ğŸ“–åŠ è½½èƒŒæ™¯ä¿¡æ¯")
            self._load_settings(background_path)
        
        if doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel" 
        self.system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": self.background_info})
        self.graph_analyzer = GraphAnalyzer(config, self.llm)
        
        self.plot_generator = PlotGenerator(self.prompt_loader, self.llm)
        
        # Plotæ„å»ºé…ç½®å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
        self.causality_threshold = "Medium"
        self.min_cluster_size = 2
        self.max_cluster_size = 10
        self.logger = logging.getLogger(__name__)        
        self.sorted_scenes = []
        self.event_list = []
        self.event2section_map = {}
        self.allowed_rels = []
        self.max_depth = 3
        self.check_weakly_connected_components = True
        self.min_component_size = 10
        self.max_workers = 32
        self.max_iteration = 5
        
        # å› æœå…³ç³»å¼ºåº¦åˆ°æƒé‡çš„æ˜ å°„
        self.causality_weight_map = {
            "High": 1.0,
            "Medium": 0.6,
            "Low": 0.3
        }
        
        self.logger.info("EventCausalityBuilderåˆå§‹åŒ–å®Œæˆ")
    
    def _load_settings(self, path: str):
        """
        è¯»å– background + abbreviationsï¼Œå¹¶å°†å…¶åˆå¹¶åˆ° self.abbreviation_infoï¼ˆä¸€æ®µ Markdown æ–‡æœ¬ï¼‰ã€‚

        JSON ç»“æ„ç¤ºä¾‹ï¼ˆå­—æ®µå‡å¯é€‰ï¼‰ï¼š
        {
            "background": "â€¦â€¦",
            "abbreviations": [
                { "abbr": "UEG", "full": "United Earth Government", "zh": "è”åˆæ”¿åºœ", "description": "å…¨çƒç»Ÿä¸€æ”¿åºœã€‚" },
                { "symbol": "AI", "meaning": "äººå·¥æ™ºèƒ½", "comment": "å¹¿æ³›åº”ç”¨äºâ€¦" }
            ]
        }
        """
        self.background_info = ""

        if not os.path.exists(path):
            return

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # ---------- 1) èƒŒæ™¯æ®µè½ï¼ˆå¯é€‰ï¼‰ ----------
        background = data.get("background", "").strip()
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" if background else ""

        # ---------- 2) ç¼©å†™è¡¨ï¼ˆé”®åå®½å®¹ï¼‰ ----------
        def fmt(item: dict) -> str:
            """
            å°†ä¸€ä¸ªç¼©å†™é¡¹è½¬ä¸º Markdown åˆ—è¡¨æ¡ç›®ã€‚ä»»ä½•å­—æ®µéƒ½å¯é€‰ï¼Œæ ‡é¢˜å­—æ®µä¼˜å…ˆçº§ä¸ºï¼š
            abbr > full > å…¶ä»–å­—æ®µ > N/A
            """
            if not isinstance(item, dict):
                return ""

            # æ ‡é¢˜å­—æ®µä¼˜å…ˆçº§
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )

            # å‰©ä¸‹å­—æ®µå»é™¤æ ‡é¢˜å­—æ®µ
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())

            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_list = data.get("abbreviations", [])
        abbr_block = "\n".join(fmt(item) for item in abbr_list if isinstance(item, dict))

        if background and abbr_block:
            self.background_info = f"{bg_block}\n{abbr_block}"
        else:
            self.background_info = bg_block or abbr_block
        print(f"âœ… æˆåŠŸä»{path}åŠ è½½èƒŒæ™¯ä¿¡æ¯")
    
    def build_event_list(self) -> List[Entity]:
        """
        æ„å»ºæ’åºåçš„äº‹ä»¶åˆ—è¡¨
        
        Returns:
            æ’åºåçš„äº‹ä»¶åˆ—è¡¨
        """
        print("ğŸ” å¼€å§‹æ„å»ºäº‹ä»¶åˆ—è¡¨...")
        
        # 1. è·å–æ‰€æœ‰åœºæ™¯å¹¶æ’åº
        section_entities = self.neo4j_utils.search_entities_by_type(
            entity_type=self.meta["section_label"]
        )
        
        self.sorted_sections = sorted(
            section_entities,
            key=lambda e: int(e.properties.get("order", 99999))
        )
        
        print(f"âœ… æ‰¾åˆ° {len(self.sorted_sections )} ä¸ªsection")
        
        # 2. ä»åœºæ™¯ä¸­æå–äº‹ä»¶
        event_list = []
        event2section_map = {}
        
        for scene in tqdm(self.sorted_sections, desc="æå–åœºæ™¯ä¸­çš„äº‹ä»¶"):
            # ä¼˜å…ˆæŸ¥æ‰¾äº‹ä»¶
            results = self.neo4j_utils.search_related_entities(
                source_id=scene.id, 
                predicate=self.meta["contains_pred"], 
                entity_types=["Event"], 
                return_relations=False
            )
            
            # å¦‚æœåœºæ™¯ä¸­æ²¡æœ‰äº‹ä»¶ï¼Œåˆ™ç”¨åŠ¨ä½œæˆ–è€…ç›®æ ‡æ¥å¡«å……
            if not results and self.event_fallback:
                results = self.neo4j_utils.search_related_entities(
                    source_id=scene.id, 
                    relation_type=self.meta["contains_pred"], 
                    entity_types=self.event_fallback, 
                    return_relations=False
                )
            
            for result in results:
                if result.id not in event2section_map:
                    event2section_map[result.id] = scene.id
                    event_list.append(result)
        
        self.event_list = event_list
        self.event2section_map = event2section_map
        
        print(f"âœ… æ„å»ºå®Œæˆï¼Œå…±æ‰¾åˆ° {len(event_list)} ä¸ªäº‹ä»¶")
        return event_list
    
    
    def filter_event_pairs_by_community(
        self,
        events: List[Entity],
        max_depth: int = 3
    ) -> List[Tuple[Entity, Entity]]:
        """
        åˆ©ç”¨ Neo4j ä¸­ Louvain ç»“æœç›´æ¥ç­›é€‰åŒç¤¾åŒºä¸” max_depth å†…å¯è¾¾çš„äº‹ä»¶å¯¹
        """
        # æŠŠäº‹ä»¶ ID åšæˆé›†åˆï¼Œä¾¿äºåé¢å®ä½“æ˜ å°„
        id2entity = {e.id: e for e in events}

        pairs = self.neo4j_utils.fetch_event_pairs_same_community()
        # print("[CHECK]: ", pairs)
        filtered_pairs = []
        for row in pairs:
            src_id, dst_id = row["srcId"], row["dstId"]
            if src_id in id2entity and dst_id in id2entity:
                filtered_pairs.append((id2entity[src_id], id2entity[dst_id]))

        print(f"[âœ“] åŒç¤¾åŒºäº‹ä»¶å¯¹: {len(filtered_pairs)}")
        return filtered_pairs

    def write_event_cause_edges(self, causality_results):
        rows = []
        for (src_id, dst_id), res in causality_results.items():
            weight = self.causality_weight_map.get(res["causal"], 0.3)
            # print("[CHECK] res: ", res)
            confidence = res.get("confidence", 0.3)
            rows.append({
                "srcId": src_id,
                "dstId": dst_id,
                "weight": weight,
                "confidence": confidence,
                "reason": res["reason"],
                "predicate": "EVENT_CAUSES"
            })
        self.neo4j_utils.write_event_causes(rows)

    
    def check_causality_batch(
        self,
        pairs: List[Tuple[Entity, Entity]]
    ) -> Dict[Tuple[str, str], Dict[str, Any]]:
        """
        æ‰¹é‡æ£€æŸ¥äº‹ä»¶å¯¹çš„å› æœå…³ç³»ï¼ˆå¤šçº¿ç¨‹ç‰ˆï¼‰

        Args:
            pairs: äº‹ä»¶å¯¹åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°

        Returns:
            äº‹ä»¶å¯¹IDåˆ°å› æœå…³ç³»ç»“æœçš„æ˜ å°„
        """
        print(f"ğŸ” å¼€å§‹å¹¶å‘æ£€æŸ¥ {len(pairs)} å¯¹äº‹ä»¶çš„å› æœå…³ç³»...")
        causality_results: Dict[Tuple[str, str], Dict[str, Any]] = {}

        def _process_pair(pair: Tuple[Entity, Entity]):
            src_event, tgt_event = pair
            pair_key = (src_event.id, tgt_event.id)
            try:
                # è·å–äº‹ä»¶ä¿¡æ¯
                info_1 = self.neo4j_utils.get_entity_info(src_event.id, entity_type="äº‹ä»¶", contain_properties=True, contain_relations=True)
                info_2 = self.neo4j_utils.get_entity_info(tgt_event.id, entity_type="äº‹ä»¶", contain_properties=True, contain_relations=True)
                
                chunks = self.neo4j_utils.get_entity_by_id(src_event.id).source_chunks + self.neo4j_utils.get_entity_by_id(tgt_event.id).source_chunks
                chunks = list(set(chunks))
                documents = self.vector_store.search_by_ids(chunks)
                results = {doc.content for doc in documents}
                related_context = "\n".join(list(results))
                
                # è°ƒç”¨ extractor æ£€æŸ¥å› æœå…³ç³»
                result_json = self.graph_analyzer.check_event_causality(
                    info_1, info_2, system_prompt=self.system_prompt_text, related_context=related_context
                )
                result_dict = json.loads(result_json)
                # print("[CHECK] result_dict: ", result_dict)
                return pair_key, {
                    'src_event': src_event,
                    'tgt_event': tgt_event,
                    'causal': result_dict.get('causal', 'Low'),
                    'reason': result_dict.get('reason', ''),
                    'reverse': result_dict.get('reverse', False),
                    'confidence': result_dict.get('confidence', 0.3),
                    'raw_result': result_json
                }
                
            except Exception as e:
                # å‡ºé”™æ—¶è¿”å› Low å¼ºåº¦ä¸”è®°å½•é”™è¯¯
                return pair_key, {
                    'src_event': src_event,
                    'tgt_event': tgt_event,
                    'causal': 'Low',
                    'reason': f'æ£€æŸ¥è¿‡ç¨‹å‡ºé”™: {e}',
                    'reverse': False,
                    'confidence': result_dict.get('confidence', 0),
                    'raw_result': ''
                }

        # å¹¶å‘æ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_pair = {
                executor.submit(_process_pair, pair): pair for pair in pairs
            }
            for fut in tqdm(as_completed(future_to_pair),
                            total=len(future_to_pair),
                            desc="æ£€æŸ¥å› æœå…³ç³»"):
                key, res = fut.result()
                causality_results[key] = res

        print(f"âœ… å› æœå…³ç³»å¹¶å‘æ£€æŸ¥å®Œæˆ")
        return causality_results
        
    def sort_event_pairs_by_section_order(
        self, pairs: List[Tuple[Entity, Entity]]
    ) -> List[Tuple[Entity, Entity]]:
        def get_order(evt: Entity) -> int:
            sec_id = self.event2section_map.get(evt.id)
            if not sec_id:
                return 99999
            sec = self.neo4j_utils.get_entity_by_id(sec_id)
            return int(sec.properties.get("order", 99999))

        ordered = []
        for e1, e2 in pairs:
            ordered.append((e1, e2) if get_order(e1) <= get_order(e2) else (e2, e1))
        return ordered

    def initialize(self):
        # 1. åˆ›å»ºå­å›¾å’Œè®¡ç®—ç¤¾åŒºåˆ’åˆ†
        self.neo4j_utils.delete_relation_type("EVENT_CAUSES")
        self.neo4j_utils.create_subgraph(
            graph_name="event_graph",
            exclude_entity_types=[self.meta["section_label"]],
            exclude_relation_types=[self.meta["contains_pred"], "EVENT_CAUSES"],
            force_refresh=True
        )

        self.neo4j_utils.run_louvain(
            graph_name="event_graph",
            write_property="community",
            force_run=True
        )
    
    def filter_pair_by_distance_and_similarity(self, pairs):
        filtered_pairs = []
        for pair in tqdm(pairs, desc="ç­›é€‰èŠ‚ç‚¹å¯¹"):
            src_id, tgt_id = pair[0].id, pair[1].id
            reachable = self.neo4j_utils.check_nodes_reachable(src_id, tgt_id, excluded_rels=[self.meta["contains_pred"], "EVENT_CAUSES"])
            if reachable: # å¦‚æœèŠ‚ç‚¹é—´è·ç¦»å°äº3ï¼Œä¿ç•™ã€‚
                filtered_pairs.append(pair)
            else:
                score = self.neo4j_utils.compute_semantic_similarity(src_id, tgt_id)
                if score >= 0.7: # å¦‚æœèŠ‚ç‚¹é—´çš„ç›¸ä¼¼åº¦å¤§äºç­‰äº0.7ï¼Œä¿ç•™ã€‚
                    filtered_pairs.append(pair)  
        return filtered_pairs
    
    def build_event_causality_graph(
        self,
        limit_events: Optional[int] = None
    ) -> None:
        """
        å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹
        
        Args:
            limit_events: é™åˆ¶å¤„ç†çš„äº‹ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            
        Returns:
            æ„å»ºå®Œæˆçš„Neo4jæœ‰å‘å›¾
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´çš„äº‹ä»¶å› æœå›¾æ„å»ºæµç¨‹...")
        
        # 2. æ„å»ºäº‹ä»¶åˆ—è¡¨
        print("\nğŸ” æ„å»ºäº‹ä»¶åˆ—è¡¨...")
        event_list = self.build_event_list()
        
        # 3. é™åˆ¶äº‹ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        if limit_events and limit_events < len(event_list):
            event_list = event_list[:limit_events]
            print(f"âš ï¸ é™åˆ¶å¤„ç†äº‹ä»¶æ•°é‡ä¸º: {limit_events}")
        
        # 4. è¿‡æ»¤äº‹ä»¶å¯¹
        print("\nğŸ” è¿‡æ»¤äº‹ä»¶å¯¹...")
        filtered_pairs = self.filter_event_pairs_by_community(event_list)
        filtered_pairs = self.filter_pair_by_distance_and_similarity(filtered_pairs)
        filtered_pairs = self.sort_event_pairs_by_section_order(filtered_pairs)
        print("     æœ€ç»ˆå€™é€‰äº‹ä»¶å¯¹æ•°é‡ï¼š ", len(filtered_pairs))
        # 5. æ£€æŸ¥å› æœå…³ç³»
        print("\nğŸ” æ£€æŸ¥å› æœå…³ç³»...")
        causality_results = self.check_causality_batch(filtered_pairs)
        
        # 6. å†™å› EVENT_CAUSES
        print("\nğŸ”— å†™å› EVENT_CAUSES å…³ç³»...")
        self.write_event_cause_edges(causality_results)
        self.neo4j_utils.create_event_causality_graph("event_causality_graph", force_refresh=True)

    def detect_flattened_causal_patterns(self, edges: List[Dict]) -> List[Dict]:
        """
        ä»è¾¹é›†ä¸­å‘ç°ç±»ä¼¼ Aâ†’B, Aâ†’C, Aâ†’D ä¸”å­˜åœ¨ Bâ†’D çš„å†—ä½™ç»“æ„ï¼Œç”¨äºåç»­å› æœé“¾ç²¾ç‚¼

        Returns:
            List of {
                "source": A,
                "targets": [B, C, D],
                "internal_links": [(B, D), (C, D)]
            }
        """
        # æ„å»ºé‚»æ¥è¡¨å’Œåå‘è¾¹é›†åˆ
        forward_graph = defaultdict(set)
        edge_set = set()

        for edge in edges:
            sid = edge["sid"]
            tid = edge["tid"]
            forward_graph[sid].add(tid)
            edge_set.add((sid, tid))

        patterns = []

        for a, a_children in forward_graph.items():
            a_children = list(a_children)
            if len(a_children) < 2:
                continue  # è‡³å°‘ä¸¤ä¸ªæŒ‡å‘æ‰å¯èƒ½æ„æˆè¯¥æ¨¡å¼

            internal_links = []
            for i in range(len(a_children)):
                for j in range(len(a_children)):
                    if i == j:
                        continue
                    u, v = a_children[i], a_children[j]
                    if (u, v) in edge_set:
                        internal_links.append((u, v))

            if internal_links:
                patterns.append({
                    "source": a,
                    "targets": a_children,
                    "internal_links": internal_links
                })

        # print(f"[+] Detected {len(patterns)} flattened causal patterns")
        return patterns
    
    def filter_weak_edges_in_patterns(
        self,
        patterns: List[Dict],
        edge_map: Dict[Tuple[str, str], Dict],
        weight_threshold: float = 0.3,
        conf_threshold: float = 0.5
    ) -> List[Dict]:
        """
        ä» flattened patterns ä¸­å‰”é™¤ weight å’Œ confidence éƒ½åä½çš„è¾¹
        """
        cleaned_patterns = []
        # print("[CHECK] patterns: ", patterns)
        for pat in patterns:
            src = pat["source"]
            targets = pat["targets"]
            internals = pat["internal_links"]

            # è¿‡æ»¤ source â†’ target è¾¹
            new_targets = []
            for t in targets:
                info = edge_map.get((src, t))
                confidence = info.get("confidence", 0) or 0
                # print("[CHECK] confidence: ", confidence)
                if not info:
                    continue
                if not (info["weight"] <= weight_threshold and confidence  < conf_threshold):
                    new_targets.append(t)

            # è¿‡æ»¤ internal è¾¹
            new_internals = []
            for u, v in internals:
                info = edge_map.get((u, v))
                confidence = info.get("confidence", 0) or 0
                if not info:
                    continue
                if not (info["weight"] <= weight_threshold and confidence < conf_threshold):
                    new_internals.append((u, v))

            # ä¿ç•™ç»“æ„
            if len(new_targets) >= 2 and new_internals:
                cleaned_patterns.append({
                    "source": src,
                    "targets": new_targets,
                    "internal_links": new_internals
                })

        # print(f"[+] Filtered to {len(cleaned_patterns)} refined patterns")
        return cleaned_patterns
    
    def collect_removed_edges(self,
        original_patterns: List[Dict],
        filtered_patterns: List[Dict]
    ) -> Set[Tuple[str, str]]:
        """
        æ¯”è¾ƒä¸¤ç»„ pattern ç»“æ„ï¼Œæ”¶é›†è¢«åˆ é™¤å¯¼è‡´ç»“æ„å˜åŒ–çš„è¾¹

        Returns:
            è¢«æ ‡è®°ä¸ºåˆ é™¤å€™é€‰çš„è¾¹é›†åˆï¼ˆsid, tidï¼‰
        """
        # æŠ½å–åŸå§‹ç»“æ„ä¸­çš„å…¨éƒ¨è¾¹
        def extract_edges(patterns: List[Dict]) -> Set[Tuple[str, str]]:
            edge_set = set()
            for pat in patterns:
                src = pat["source"]
                for tgt in pat["targets"]:
                    edge_set.add((src, tgt))
                edge_set.update(pat["internal_links"])
            return edge_set

        origin_edges = extract_edges(original_patterns)
        filtered_edges = extract_edges(filtered_patterns)

        removed_edges = origin_edges - filtered_edges
        print(f"[+] Found {len(removed_edges)} candidate edges removed due to pattern collapse")
        return list(removed_edges)
    
    def filter_pattern(self, pattern, edge_map):
        source = pattern["source"]
        targets = pattern["targets"]
        internal_links = pattern["internal_links"]
        context_to_check = []
        for link in internal_links:
            mid_tgt_sim = self.neo4j_utils.compute_semantic_similarity(link[0], link[1])
            src_mid_sim = self.neo4j_utils.compute_semantic_similarity(source, link[0])
            src_tgt_sim = self.neo4j_utils.compute_semantic_similarity(source, link[1])
            
            mid_tgt_conf = edge_map.get((link[0], link[1]))["confidence"]
            src_mid_conf = edge_map.get((source, link[0]))["confidence"]
            src_tgt_conf = edge_map.get((source, link[1]))["confidence"]
            
            # print(source_mid_score, internal_score, source_target_score)
            if (src_mid_sim > src_tgt_sim and mid_tgt_sim > src_tgt_sim) or (src_mid_conf > src_tgt_conf and mid_tgt_conf > src_tgt_conf) :
                context_to_check.append({
                    "entities": [source, link[0], link[1]],
                    "details": [
                        {"edge": [source, link[0]], "similarity": src_mid_sim, "confidence": src_mid_conf},
                        {"edge": [source, link[1]], "similarity": src_tgt_sim, "confidence": src_tgt_conf},
                        {"edge": [link[0], link[1]], "similarity": mid_tgt_sim, "confidence": mid_tgt_conf},
                    ]
                })
                
        return context_to_check
    
    
    def prepare_context(self, pattern_detail):
        event_details = self.neo4j_utils.get_event_details(pattern_detail["entities"])
        full_event_details = "ä¸‰ä¸ªäº‹ä»¶å®ä½“çš„æè¿°å¦‚ä¸‹ï¼š\n"
        for i, event_info in enumerate(event_details):
            event_id = event_info["event_id"]
            full_event_details += f"**äº‹ä»¶{i+1}çš„ç›¸å…³æè¿°å¦‚ä¸‹ï¼š**\näº‹ä»¶idï¼š{event_id}\n"
            
            background = self.neo4j_utils.get_entity_info(event_id, "äº‹ä»¶", True, True)
            event_props = json.loads(event_info.get("event_properties"))
            # print(event_props)
            non_empty_props = {k: v for k, v in event_props.items() if isinstance(v, str) and v.strip()}

            if non_empty_props:
                background += "\näº‹ä»¶çš„å±æ€§å¦‚ä¸‹ï¼š\n"
                for k, v in non_empty_props.items():
                    background += f"- {k}ï¼š{v}\n"

            if i+1 !=  len(event_details):
                background += "\n"
            full_event_details += background
        
        full_relation_details = "å®ƒä»¬ä¹‹é—´å·²ç»å­˜åœ¨çš„å› æœå…³ç³»æœ‰ï¼š\n"
        relation_details = pattern_detail["details"]
        for i, relation_info in enumerate(relation_details):
            src, tgt = relation_info["edge"]
            background = f"{i+1}. " + self.neo4j_utils.get_relation_summary(src, tgt, "EVENT_CAUSES")
            background += f"\nå…³ç³»çš„è¯­ä¹‰ç›¸ä¼¼åº¦ä¸ºï¼š{round(relation_info["similarity"], 4)}ï¼Œç½®ä¿¡åº¦ä¸ºï¼š{relation_info["confidence"]}ã€‚"
            if i+1 !=  len(relation_details):
                background += "\n\n"
            full_relation_details += background
        return full_event_details, full_relation_details
    

    def run_SABER(self):
        """
        æ‰§è¡ŒåŸºäºç»“æ„+LLMçš„å› æœè¾¹ç²¾ç®€ä¼˜åŒ–è¿‡ç¨‹
        """
        loop_count = 0
        while True:
            print(f"\n===== [ç¬¬ {loop_count + 1} è½®ä¼˜åŒ–] =====")

            # === è·å–è¿é€šä½“ï¼ˆä¼˜å…ˆ SCCï¼Œå†é€‰ WCCï¼‰ ===
            scc_components = self.neo4j_utils.fetch_scc_components("event_causality_graph", 2)
            wcc_components = []
            if self.check_weakly_connected_components:
                wcc_components = self.neo4j_utils.fetch_wcc_components("event_causality_graph", self.min_component_size)

            connected_components = scc_components + wcc_components
            print(f"ğŸ“Œ å½“å‰è¿é€šä½“æ•°é‡ï¼šSCC={len(scc_components)}ï¼ŒWCC={len(wcc_components)}")

            # === æ„é€ æ‰€æœ‰ triangle å’Œè¾¹ä¿¡æ¯ ===
            all_triangles = []
            edge_map_global = {}

            for cc in connected_components:
                node_map, edges = self.neo4j_utils.load_connected_components_subgraph(cc)
                edge_map = {
                    (e["sid"], e["tid"]): {"weight": e["weight"], "confidence": e.get("confidence", 1.0)}
                    for e in edges
                }
                edge_map_global.update(edge_map)

                old_patterns = self.detect_flattened_causal_patterns(edges)
                new_patterns = self.filter_weak_edges_in_patterns(old_patterns, edge_map)
                for pattern in new_patterns:
                    all_triangles += self.filter_pattern(pattern, edge_map)

            print(f"ğŸ”º æœ¬è½®éœ€åˆ¤æ–­çš„ä¸‰å…ƒå› æœç»“æ„æ•°é‡ï¼š{len(all_triangles)}")

            # === âœ… æå‰é€€å‡ºæ¡ä»¶ ===
            if loop_count >= 1:
                if len(scc_components) == 0 and len(set(removed_edges)) == 0:
                    print("âœ… å›¾ç»“æ„å·²æ— å¼ºè¿é€šä½“ï¼Œä¸”æ— å¾…åˆ¤å®šä¸‰å…ƒç»“æ„ï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
                    break
            elif loop_count >= self.max_iteration:
                break

            # === å¹¶å‘å¤„ç†ä¸‰å…ƒç»“æ„ ===
            removed_edges = []

            def process_triangle(triangle_):
                try:
                    event_details, relation_details = self.prepare_context(triangle_)
                    chunks = [self.neo4j_utils.get_entity_by_id(ent_id).source_chunks[0] for ent_id in triangle_["entities"]]
                    chunks = list(set(chunks))
                    documents = self.vector_store.search_by_ids(chunks)
                    results = {doc.content for doc in documents}
                    related_context = "\n".join(list(results))
                    # related_context = "" # ä¸ºäº†é€Ÿåº¦

                    output = self.graph_analyzer.evaluate_event_redundancy(
                        event_details, relation_details, self.system_prompt_text, related_context
                    )
                    output = json.loads(correct_json_format(output))
                    if output.get("remove_edge", False):
                        return (triangle_["entities"][0], triangle_["entities"][2])
                except Exception as e:
                    print(f"[âš ï¸ é”™è¯¯] Triangle åˆ¤æ–­å¤±è´¥: {triangle_['entities']}, é”™è¯¯ä¿¡æ¯: {str(e)}")
                return None

            print(f"ğŸ§  æ­£åœ¨å¹¶å‘åˆ¤æ–­ä¸‰å…ƒç»“æ„...")
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(process_triangle, tri) for tri in all_triangles]
                for f in tqdm(as_completed(futures), total=len(futures), desc="LLMåˆ¤æ–­"):
                    res = f.result()
                    if res:
                        removed_edges.append(res)

            print(f"âŒ æœ¬è½®å¾…å®šç§»é™¤è¾¹æ•°é‡ï¼š{len(set(removed_edges))}")

            # === åˆ é™¤è¾¹ ===
            for edge in removed_edges:
                self.neo4j_utils.delete_relation_by_ids(edge[0], edge[1], "EVENT_CAUSES")

            # === åˆ·æ–° GDS å›¾ ===
            # self.neo4j_utils.create_event_causality_graph("event_causality_graph", min_confidence=0.5, min_weight=0.5, force_refresh=True)
            self.neo4j_utils.create_event_causality_graph("event_causality_graph", force_refresh=True)
            loop_count += 1

    def get_all_event_chains(self, min_weight: float = 0.0, min_confidence: float = 0.0):
        """
        è·å–æ‰€æœ‰å¯èƒ½çš„äº‹ä»¶é“¾ï¼ˆä»èµ·ç‚¹åˆ°æ²¡æœ‰å‡ºè¾¹çš„ç»ˆç‚¹ï¼‰
        """
        starting_events = self.neo4j_utils.get_starting_events()
        chains = []
        for event in starting_events:
            all_chains = self.neo4j_utils.find_event_chain(event, min_weight, min_confidence)
            chains.extend([chain for chain in all_chains if len(chain) >= 1])
        return chains
    
    def prepare_chain_context(self, chain):
        if len(chain) > 1:
            context = "äº‹ä»¶é“¾ï¼š" + "->".join(chain) +"\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        else:
            context = f"äº‹ä»¶ï¼š{chain[0]}" +"\n\näº‹ä»¶å…·ä½“ä¿¡æ¯å¦‚ä¸‹ï¼š\n"
        for i, event in enumerate(chain):
            context += f"äº‹ä»¶{i+1}ï¼š{event}\n" + self.neo4j_utils.get_entity_info(event, "äº‹ä»¶", False, True) + "\n"
        return context
        

    def generate_plot_relations(self):
        
        self.neo4j_utils.process_all_embeddings(entity_types=["Plot"])
        
        all_plot_pairs = self.neo4j_utils.get_plot_pairs()
        edges_to_add = []

        def process_pair(pair):
            try:
                plot_A_info = self.neo4j_utils.get_entity_info(pair["src"], "æƒ…èŠ‚", False, True)
                plot_B_info = self.neo4j_utils.get_entity_info(pair["tgt"], "æƒ…èŠ‚", False, True)
                result = self.graph_analyzer.extract_plot_relation(plot_A_info, plot_B_info, self.system_prompt_text)
                result = json.loads(correct_json_format(result))

                pair_edges = []
                if result["relation_type"] == "PLOT_CONTRIBUTES_TO":
                    first = result["direction"].split("->")[0]
                    if first == "A":
                        pair_edges.append({
                            "src": pair["src"],
                            "tgt": pair["tgt"],
                            "relation_type": result["relation_type"],
                            "confidence": result["confidence"],
                            "reason": result["reason"]
                        })
                    else:
                        pair_edges.append({
                            "src": pair["tgt"],
                            "tgt": pair["src"],
                            "relation_type": result["relation_type"],
                            "confidence": result["confidence"],
                            "reason": result["reason"]
                        })
                elif result["relation_type"] == "PLOT_CONFLICTS_WITH":
                    pair_edges.append({
                        "src": pair["src"],
                        "tgt": pair["tgt"],
                        "relation_type": result["relation_type"],
                        "confidence": result["confidence"],
                        "reason": result["reason"]
                    })
                    pair_edges.append({
                        "src": pair["tgt"],
                        "tgt": pair["src"],
                        "relation_type": result["relation_type"],
                        "confidence": result["confidence"],
                        "reason": result["reason"]
                    })
                return pair_edges
            except Exception as e:
                print(f"[âš ] å¤„ç†æƒ…èŠ‚å¯¹ {pair} å‡ºé”™: {e}")
                return []

        # å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_pair, pair) for pair in all_plot_pairs]
            # for future in as_completed(futures):
            for future in tqdm(as_completed(futures), total=len(futures), desc="æŠ½å–æƒ…èŠ‚å…³ç³»"):
                edges_to_add.extend(future.result())

        # æ‰¹é‡å†™å…¥ Neo4j
        if edges_to_add:
            self.neo4j_utils.create_plot_relations(edges_to_add)
            print(f"[âœ“] å·²åˆ›å»ºæƒ…èŠ‚å…³ç³» {len(edges_to_add)} æ¡")
        else:
            print("[!] æ²¡æœ‰ç”Ÿæˆä»»ä½•æƒ…èŠ‚å…³ç³»")

    
    def build_event_plot_graph(self):
        all_chains = self.get_all_event_chains(0.5, 0.5)
        print("[âœ“] å½“å‰äº‹ä»¶é“¾æ•°é‡ï¼š", len(all_chains))
        self.neo4j_utils.reset_event_plot_graph()
        def process_chain(chain):
            try:
                event_chain_info = self.prepare_chain_context(chain)
                chunks = [self.neo4j_utils.get_entity_by_id(ent_id).source_chunks[0] for ent_id in chain]
                chunks = list(set(chunks))
                documents = self.vector_store.search_by_ids(chunks)
                results = {doc.content for doc in documents}
                related_context = "\n".join(list(results))

                result = self.graph_analyzer.generate_event_plot(
                    event_chain_info=event_chain_info,
                    system_prompt=self.system_prompt_text,
                    related_context=related_context
                )
                result = json.loads(correct_json_format(result))
                if result["is_plot"]:
                    plot_info = result["plot_info"]
                    plot_title = plot_info["title"]
                    plot_info["id"] = f"plot_{hash(f'{plot_title}') % 1_000_000}"
                    plot_info["event_ids"] = chain
                    plot_info["reason"] = result.get("reason", "")
                    self.neo4j_utils.write_plot_to_neo4j(plot_data=plot_info)
                    return True
                return False
            except Exception as e:
                print(f"[!] å¤„ç†äº‹ä»¶é“¾ {chain} æ—¶å‡ºé”™: {e}")
                return False

        success_count = 0
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(process_chain, chain) for chain in all_chains]
            for future in tqdm(as_completed(futures), total=len(futures), desc="å¹¶å‘ç”Ÿæˆæƒ…èŠ‚å›¾è°±"):
                if future.result():
                    success_count += 1

        print(f"[âœ“] æˆåŠŸç”Ÿæˆæƒ…èŠ‚æ•°é‡ï¼š{success_count}/{len(all_chains)}")

                
                
        
        