# kag/builder/kg_builder_2.py
from __future__ import annotations

import json
import os
import sqlite3
import pickle
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed, TimeoutError, wait
from collections import defaultdict
from copy import deepcopy
from typing import Any, Dict, List, Optional
import asyncio
import pandas as pd
from tqdm import tqdm
from kag.utils.prompt_loader import PromptLoader
from kag.models.data import Entity, KnowledgeGraph, Relation, TextChunk, Document
from ..storage.document_store import DocumentStore
from ..storage.graph_store import GraphStore
from ..storage.vector_store import VectorStore
from ..utils.config import KAGConfig
from ..utils.neo4j_utils import Neo4jUtils
from kag.llm.llm_manager import LLMManager
from kag.agent.kg_extraction_agent import InformationExtractionAgent
from kag.agent.attribute_extraction_agent import AttributeExtractionAgent
from .document_processor import DocumentProcessor
from kag.builder.graph_preprocessor import GraphPreprocessor

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# doc-type â†”ï¸Ž å…ƒå­—æ®µ / æ ‡ç­¾ / è°“è¯æ˜ å°„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DOC_TYPE_META: Dict[str, Dict[str, str]] = {
    "screenplay": {
        "section_label": "Scene",
        "title": "scene_name",
        "subtitle": "sub_scene_name",
        "contains_pred": "SCENE_CONTAINS",
    },
    "novel": {
        "section_label": "Chapter",
        "title": "chapter_name",
        "subtitle": "sub_chapter_name",
        "contains_pred": "CHAPTER_CONTAINS",
    },
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               Builder
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æž„å»ºå™¨ï¼ˆæ”¯æŒå¤šæ–‡æ¡£æ ¼å¼ï¼‰"""
    def __init__(self, config: KAGConfig, doc_type: str = "screenplay", background_path: str = ""):
        if doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {doc_type}")
        self.max_workers = 32
        self.multi_mode = "async"
        
        self.config = config
        self.meta = DOC_TYPE_META[doc_type]
        prompt_dir = (
            config.prompt_dir
            if hasattr(config, "prompt_dir")
            else os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "kag/prompts"
            )
        )
        self.prompt_loader = PromptLoader(prompt_dir)
            
        # LLM & Processor
        self.llm_manager = LLMManager(config)
        self.llm = self.llm_manager.get_llm()
        self.processor = DocumentProcessor(config, self.llm, doc_type, max_worker=self.max_workers)

        # å­˜å‚¨ / æ•°æ®åº“
        self.graph_store = GraphStore(config)
        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, doc_type=doc_type)
        self.vector_store = VectorStore(config)
        self.document_store = DocumentStore(config)

        # è¿è¡Œæ•°æ®
        self.kg = KnowledgeGraph()

        # å¯é€‰ schema / ç¼©å†™
        self._load_schema("kag/schema/graph_schema.json")
        self.background_info = ""
        if background_path:
            print("ðŸ“–åŠ è½½èƒŒæ™¯ä¿¡æ¯")
            # glossary_path = os.path.join("kag/schema", glossary, "settings_schema.json")
            self._load_settings(background_path)
        
        if doc_type == "screenplay":
            system_prompt_id = "agent_prompt_screenplay"
        else:
            system_prompt_id = "agent_prompt_novel"
            
        self.system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": self.background_info})
        
        # æŠ½å– agent
        self.information_extraction_agent = InformationExtractionAgent(config, self.llm, self.system_prompt_text)
        self.attribute_extraction_agent = AttributeExtractionAgent(config, self.llm, self.system_prompt_text)
        self.graph_preprocessor = GraphPreprocessor(config, self.llm, system_prompt=self.system_prompt_text)

    def _load_settings(self, path: str):
        """
        è¯»å– background + abbreviationsï¼Œå¹¶å°†å…¶åˆå¹¶åˆ° self.abbreviation_infoï¼ˆä¸€æ®µ Markdown æ–‡æœ¬ï¼‰ã€‚

        JSON ç»“æž„ç¤ºä¾‹ï¼ˆå­—æ®µå‡å¯é€‰ï¼‰ï¼š
        {
            "background": "â€¦â€¦",
            "abbreviations": [
                { "abbr": "UEG", "full": "United Earth Government", "zh": "è”åˆæ”¿åºœ", "description": "å…¨çƒç»Ÿä¸€æ”¿åºœã€‚" },
                { "symbol": "AI", "meaning": "äººå·¥æ™ºèƒ½", "comment": "å¹¿æ³›åº”ç”¨äºŽâ€¦" }
            ]
        }
        """
        self.background_info = ""

        if not os.path.exists(path):
            return

        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        # ---------- 1) èƒŒæ™¯æ®µè½ï¼ˆå¯é€‰ï¼‰ ----------
        background = data.get("background", "").strip()
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" if background else ""

        # ---------- 2) ç¼©å†™è¡¨ï¼ˆé”®åå®½å®¹ï¼‰ ----------
        def fmt(item: dict) -> str:
            """
            å°†ä¸€ä¸ªç¼©å†™é¡¹è½¬ä¸º Markdown åˆ—è¡¨æ¡ç›®ã€‚ä»»ä½•å­—æ®µéƒ½å¯é€‰ï¼Œæ ‡é¢˜å­—æ®µä¼˜å…ˆçº§ä¸ºï¼š
            abbr > full > å…¶ä»–å­—æ®µ > N/A
            """
            if not isinstance(item, dict):
                return ""

            # æ ‡é¢˜å­—æ®µä¼˜å…ˆçº§
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )

            # å‰©ä¸‹å­—æ®µåŽ»é™¤æ ‡é¢˜å­—æ®µ
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())

            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_list = data.get("abbreviations", [])
        abbr_block = "\n".join(fmt(item) for item in abbr_list if isinstance(item, dict))

        if background and abbr_block:
            self.background_info = f"{bg_block}\n{abbr_block}"
        else:
            self.background_info = bg_block or abbr_block
        print(f"âœ… æˆåŠŸä»Ž{path}åŠ è½½èƒŒæ™¯ä¿¡æ¯")

    def _load_schema(self, path: str):
        if not os.path.exists(path):
            self.entity_types, self.relation_type_groups = [], {}
            return
        sch = json.load(open(path, "r", encoding="utf-8"))
        self.entity_types = sch.get("entities", [])
        self.relation_type_groups = sch.get("relations", {})


    def prepare_chunks(self, json_file_path: str, verbose: bool = True):
        if verbose:
            print(f"ðŸš€ å¼€å§‹æž„å»ºçŸ¥è¯†å›¾è°±: {json_file_path}")

        if verbose:
            print("ðŸ“– åŠ è½½æ–‡æ¡£...")
        
        documents = self.processor.load_from_json(json_file_path, extract_metadata=True)
        
        if verbose:
            print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # å¹¶å‘åˆ‡å—
        all_docs = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as exe:
            futs = [exe.submit(self.processor.prepare_chunk, d) for d in documents]
            for fut in tqdm(as_completed(futs), total=len(futs), desc="å¹¶å‘æ‹†åˆ†ä¸­"):
                grp = fut.result()
                all_docs.extend(grp["document_chunks"])

        # è½ç›˜
        base = self.config.storage.knowledge_graph_path
        os.makedirs(base, exist_ok=True)
        json.dump([c.dict() for c in all_docs],
                  open(os.path.join(base, "all_document_chunks.json"), "w", encoding="utf-8"),
                  ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… ç”Ÿæˆ {len(all_docs)} ä¸ªæ–‡æœ¬å—")
            
    # def prepare_chunks(
    #     self,
    #     json_file_path: str,
    #     verbose: bool = True,
    #     per_task_timeout: int = 120,
    #     max_workers: int = None
    # ):
    #     max_workers = max_workers or self.max_workers
    #     if verbose:
    #         print(f"ðŸš€ å¼€å§‹æž„å»ºçŸ¥è¯†å›¾è°±: {json_file_path}")
    #         print("ðŸ“– åŠ è½½æ–‡æ¡£...")
    #     documents = self.processor.load_from_json(json_file_path, extract_metadata=True)
    #     if verbose:
    #         print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

    #     # 1) å¹¶å‘å°è¯•
    #     all_docs = []
    #     timed_out_docs = []
    #     failed_docs = []

    #     with ThreadPoolExecutor(max_workers=max_workers) as exe:
    #         futures = {exe.submit(self.processor.prepare_chunk, doc): doc for doc in documents}

    #         # ç­‰å¾…å…¨éƒ¨å®Œæˆæˆ–å•ä¸ªè¶…æ—¶ï¼Œä½†ä¸é˜»å¡žåˆ°æ°¸ä¹…
    #         done, not_done = wait(futures, timeout=None)  # ä¸è®¾ç½® overall timeout

    #         # æ”¶é›†å·²å®Œæˆ
    #         for fut in tqdm(done, total=len(done), desc="å¹¶å‘æ‹†åˆ†å¤„ç†ä¸­"):
    #             doc = futures[fut]
    #             try:
    #                 grp = fut.result(timeout=per_task_timeout)
    #                 all_docs.extend(grp["document_chunks"])
    #             except TimeoutError:
    #                 if verbose:
    #                     print(f"âš ï¸ æ–‡æ¡£ {getattr(doc,'id',None)} å¹¶å‘è¶…æ—¶ï¼Œç¨åŽå›žé€€åŒæ­¥åˆ‡åˆ†")
    #                 timed_out_docs.append(doc)
    #             except Exception as e:
    #                 if verbose:
    #                     print(f"âŒ æ–‡æ¡£ {getattr(doc,'id',None)} å¹¶å‘å¤±è´¥ï¼š{e}ï¼Œç¨åŽå›žé€€åŒæ­¥åˆ‡åˆ†")
    #                 failed_docs.append(doc)

    #         # å‰©ä¸‹æ²¡ done çš„ï¼Œä¹Ÿå½“ä½œè¶…æ—¶
    #         for fut in not_done:
    #             doc = futures[fut]
    #             if verbose:
    #                 print(f"âš ï¸ æ–‡æ¡£ {getattr(doc,'id',None)} æœªå®Œæˆï¼Œç¨åŽå›žé€€åŒæ­¥åˆ‡åˆ†")
    #             timed_out_docs.append(doc)

    #     # 2) åŒæ­¥ä¿åº•ï¼šå¯¹æ‰€æœ‰è¶…æ—¶ï¼å¤±è´¥æ–‡æ¡£é€ä¸ªåˆ‡åˆ†ï¼ˆæ— è¶…æ—¶é™åˆ¶ï¼‰
    #     fallback = timed_out_docs + failed_docs
    #     if fallback and verbose:
    #         print(f"ðŸ”„ å¼€å§‹åŒæ­¥ä¿åº•åˆ‡åˆ† {len(fallback)} ä¸ªæ–‡æ¡£ï¼ˆæ— è¶…æ—¶é™åˆ¶ï¼‰")
    #     for doc in tqdm(fallback, desc="åŒæ­¥ä¿åº•åˆ‡åˆ†ä¸­"):
    #         try:
    #             grp = self.processor.prepare_chunk(doc)
    #             all_docs.extend(grp["document_chunks"])
    #         except Exception as e:
    #             # çœŸæ­£å¡ä½æˆ–å…¶ä»–å¼‚å¸¸ï¼Œè¿™é‡ŒæŠ›å‡ºè®©ä½ çœ‹åˆ°å…·ä½“æ˜¯å“ªä¸ªæ–‡æ¡£
    #             raise RuntimeError(f"æ–‡æ¡£ {getattr(doc,'id',None)} åŒæ­¥ä¿åº•åˆ‡åˆ†å¤±è´¥: {e}")

    #     # 3) è½ç›˜
    #     base = self.config.storage.knowledge_graph_path
    #     os.makedirs(base, exist_ok=True)
    #     out_file = os.path.join(base, "all_document_chunks.json")
    #     with open(out_file, "w", encoding="utf-8") as f:
    #         json.dump([c.dict() for c in all_docs], f, ensure_ascii=False, indent=2)

    #     if verbose:
    #         print(f"âœ… å…±ç”Ÿæˆ {len(all_docs)} ä¸ªæ–‡æœ¬å—ï¼Œä¿å­˜åœ¨ {out_file}")
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  2) å­˜å‚¨ Chunkï¼ˆRDB + VDBï¼‰
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def store_chunks(self, verbose: bool = True):
        base = self.config.storage.knowledge_graph_path

        # æè¿°å—
        doc_chunks = [TextChunk(**o) for o in
                       json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        # å†™å…¥ KGï¼ˆDocument + Chunkï¼‰
        for ch in doc_chunks:
            self.kg.add_document(self.processor.prepare_document(ch))
            self.kg.add_chunk(ch)

        # å†™å…¥å‘é‡æ•°æ®åº“
        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        self._store_vectordb(verbose)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  3) å®žä½“ / å…³ç³» æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def extract_entity_and_relation(self, verbose: bool = True):
        if self.multi_mode == "async":
            asyncio.run(self.extract_entity_and_relation_async(verbose=verbose))
        else:
            self.extract_entity_and_relation_threaded(verbose)
            
                
    def extract_entity_and_relation_threaded(self, verbose: bool = True):
   
        base = self.config.storage.knowledge_graph_path
        desc_chunks = [TextChunk(**o) for o in
                    json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        if verbose:
            print("ðŸ§  å®žä½“ä¸Žå…³ç³»ä¿¡æ¯æŠ½å–ä¸­...")

        def _run(ch: TextChunk):
            try:
                if not ch.content.strip():
                    result = {"entities": [], "relations": []}
                else:
                    result = self.information_extraction_agent.run(ch.content)
                result.update(chunk_id=ch.id, chunk_metadata=ch.metadata)
                return result
            except Exception as e:
                return {
                    "chunk_id": ch.id,
                    "chunk_metadata": ch.metadata,
                    "entities": [],
                    "relations": [],
                    "error": str(e)
                }

        results = []
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(_run, ch) for ch in desc_chunks]
            for fut in tqdm(as_completed(futures), total=len(futures), desc="å¹¶å‘æŠ½å–ä¸­"):
                results.append(fut.result())  # è°å…ˆå®Œæˆè°å°±åŠ å…¥åˆ—è¡¨

        # âœ… æœ€åŽç»Ÿä¸€å†™å…¥
        output_path = os.path.join(base, "extraction_results.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å®žä½“ä¸Žå…³ç³»ä¿¡æ¯æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ–‡æœ¬å—")
            print(f"ðŸ’¾ ç»“æžœå·²ä¿å­˜è‡³ {output_path}")
            
    
    async def extract_entity_and_relation_async(self, verbose: bool = True):
        """
        ä½¿ç”¨ asyncio å¹¶å‘æ‰§è¡Œ .arun()ï¼Œå¹¶ç»Ÿä¸€å†™å…¥ç»“æžœåˆ° extraction_results.json
        """
        base = self.config.storage.knowledge_graph_path
        desc_chunks = [TextChunk(**o) for o in
                    json.load(open(os.path.join(base, "all_document_chunks.json"), "r", encoding="utf-8"))]

        if verbose:
            print("ðŸ§  å®žä½“ä¸Žå…³ç³»ä¿¡æ¯å¼‚æ­¥æŠ½å–ä¸­...")

        sem = asyncio.Semaphore(self.max_workers)

        async def _arun(ch: TextChunk):
            async with sem:
                try:
                    if not ch.content.strip():
                        result = {"entities": [], "relations": []}
                    else:
                        result = await self.information_extraction_agent.arun(ch.content)
                    result.update(chunk_id=ch.id, chunk_metadata=ch.metadata)
                    return result
                except Exception as e:
                    return {
                        "chunk_id": ch.id,
                        "chunk_metadata": ch.metadata,
                        "entities": [],
                        "relations": [],
                        "error": str(e)
                    }

        tasks = [_arun(ch) for ch in desc_chunks]
        results = []
        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="å¼‚æ­¥æŠ½å–ä¸­"):
            res = await coro
            results.append(res)

        output_path = os.path.join(base, "extraction_results.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å®žä½“ä¸Žå…³ç³»ä¿¡æ¯æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {len(results)} ä¸ªæ–‡æœ¬å—")
            print(f"ðŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  4) å±žæ€§æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    def extract_entity_attributes(self, verbose: bool = True) -> Dict[str, Entity]:
        if self.multi_mode == "async":
            return asyncio.run(self.extract_entity_attributes_async(verbose=verbose))
        else:
            return self._extract_entity_attributes_threaded(verbose=verbose)

    
    async def extract_entity_attributes_async(self, verbose: bool = True) -> Dict[str, Entity]:
        """
        âš¡ å¼‚æ­¥æ‰¹é‡å±žæ€§æŠ½å–  
        Â· æŒ‰ extract_entity_and_relation_async ç”Ÿæˆçš„ entity_map åŽ»å¹¶å‘  
        Â· æ¯ä¸ªå®žä½“è°ƒç”¨ attribute_extraction_agent.arun()  
        Â· å†…éƒ¨ arun å·²å¸¦è¶…æ—¶ï¼‹é‡è¯•ä¿æŠ¤ï¼Œä¸ä¼šå¡æ­»
        """
        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))
        
        #print(results[0])
        # å°†å®žä½“åˆå¹¶ / åŽ»é‡
        entity_map = self.merge_entities_info(results)            # {name: Entity}

        if verbose:
            print(f"ðŸ”Ž å¼€å§‹å±žæ€§æŠ½å–ï¼ˆå¼‚æ­¥ï¼‰ï¼Œå®žä½“æ•°ï¼š{len(entity_map)}")

        sem = asyncio.Semaphore(self.max_workers)
        updated_entities: Dict[str, Entity] = {}

        async def _arun_attr(name: str, ent: Entity):
            async with sem:
                try:
                    txt = ent.description or ""
                    if not txt.strip():
                        return name, None

                    # AttributeExtractionAgent.arun å·²è‡ªå¸¦ timeout+é‡è¯•
                    res = await self.attribute_extraction_agent.arun(
                        text=txt,
                        entity_name=name,
                        entity_type=ent.type,
                        source_chunks=ent.source_chunks,
                        original_text=""
                    )

                    if res.get("error"):          # è¶…æ—¶æˆ–å¼‚å¸¸
                        return name, None

                    attrs = res.get("attributes", {}) or {}
                    if isinstance(attrs, str):
                        try:
                            attrs = json.loads(attrs)
                        except json.JSONDecodeError:
                            attrs = {}

                    new_ent = deepcopy(ent)
                    new_ent.properties = attrs

                    nd = res.get("new_description", "")
                    if nd:
                        new_ent.description = nd

                    return name, new_ent
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] å±žæ€§æŠ½å–å¤±è´¥ï¼ˆå¼‚æ­¥ï¼‰ï¼š{name}: {e}")
                    return name, None

        # å¹¶å‘æ‰§è¡Œ
        tasks = [_arun_attr(n, e) for n, e in entity_map.items()]
        for coro in tqdm(asyncio.as_completed(tasks),
                               total=len(tasks),
                               desc="å±žæ€§æŠ½å–ä¸­ï¼ˆasyncï¼‰"):
            n, e2 = await coro
            if e2:
                updated_entities[n] = e2

        # å†™æ–‡ä»¶
        output_path = os.path.join(base, "entity_info.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({k: v.dict() for k, v in updated_entities.items()},
                      f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±žæ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®žä½“ {len(updated_entities)} ä¸ª")
            print(f"ðŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

        return updated_entities
    
    
    def _extract_entity_attributes_threaded(self, verbose: bool = True) -> Dict[str, Entity]:
        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))
        
        entity_map = self.merge_entities_info(results)

        if verbose:
            print("ðŸ”Ž å±žæ€§æŠ½å–ä¸­ï¼ˆçº¿ç¨‹ï¼‰...")

        def _run_attr(name: str, ent: Entity):
            txt = ent.description or ""
            if not txt.strip():
                return name, None
            try:
                res = self.attribute_extraction_agent.run(
                    text=txt,
                    entity_name=name,
                    entity_type=ent.type,
                    source_chunks=ent.source_chunks,
                    original_text=""
                )
                return self._postprocess_attribute(name, ent, res)
            except Exception as e:
                if verbose:
                    print(f"[ERROR] å±žæ€§æŠ½å–å¤±è´¥ï¼ˆåŒæ­¥ï¼‰ï¼š{name}: {e}")
                return name, None

        updated: Dict[str, Entity] = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as exe:
            futs = [exe.submit(_run_attr, n, e) for n, e in entity_map.items()]
            for fut in tqdm(as_completed(futs), total=len(futs), desc="å±žæ€§æŠ½å–ä¸­ï¼ˆçº¿ç¨‹ï¼‰"):
                name, ent2 = fut.result()
                if ent2:
                    updated[name] = ent2

        output_path = os.path.join(base, "entity_info.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({k: v.dict() for k, v in updated.items()}, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±žæ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®žä½“ {len(updated)} ä¸ª")
            print(f"ðŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

        return updated

    
    def _extract_entity_attributes_threaded(self, verbose: bool = True) -> Dict[str, Entity]:
        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))

        entity_map = self._merge_entities(results)

        if verbose:
            print("ðŸ”Ž å±žæ€§æŠ½å–ä¸­ï¼ˆçº¿ç¨‹ï¼‰...")

        def _run_attr(name: str, ent: Entity):
            txt = ent.description or ""
            if not txt.strip():
                return name, None
            try:
                res = self.attribute_extraction_agent.run(
                    text=txt,
                    entity_name=name,
                    entity_type=ent.type,
                    source_chunks=ent.source_chunks,
                    original_text=""
                )
                return self._postprocess_attribute(name, ent, res)
            except Exception as e:
                if verbose:
                    print(f"[ERROR] å±žæ€§æŠ½å–å¤±è´¥ï¼ˆåŒæ­¥ï¼‰ï¼š{name}: {e}")
                return name, None

        updated: Dict[str, Entity] = {}
        with ThreadPoolExecutor(max_workers=self.max_workers) as exe:
            futs = [exe.submit(_run_attr, n, e) for n, e in entity_map.items()]
            for fut in tqdm(as_completed(futs), total=len(futs), desc="å±žæ€§æŠ½å–ä¸­ï¼ˆçº¿ç¨‹ï¼‰"):
                name, ent2 = fut.result()
                if ent2:
                    updated[name] = ent2

        output_path = os.path.join(base, "entity_info.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({k: v.dict() for k, v in updated.items()}, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±žæ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®žä½“ {len(updated)} ä¸ª")
            print(f"ðŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

        return updated

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  5) æž„å»ºå¹¶å­˜å‚¨å›¾è°±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def build_graph_from_results(self, verbose: bool = True) -> KnowledgeGraph:
        if verbose:
            print("ðŸ“‚ åŠ è½½å·²æœ‰æŠ½å–ç»“æžœå’Œå®žä½“ä¿¡æ¯...")

        base = self.config.storage.knowledge_graph_path
        results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))
        ent_raw = json.load(open(os.path.join(base, "entity_info.json"), "r", encoding="utf-8"))
        
        with open(os.path.join(base, "section_entities_collection.pkl"), "rb") as f:
            self.section_entities_collection = pickle.load(f)
            
       #  self.section_entities_collection = json.load(open(os.path.join(base, "section_entities_collection.json"), "r", encoding="utf-8"))
        
        # id â†’ Entity
        entity_map = {d["id"]: Entity(**d) for d in ent_raw.values()}
        name2id: Dict[str, str] = {e.name: e.id for e in entity_map.values()}
        
        for e in entity_map.values():
            for al in e.aliases:
                name2id.setdefault(al, e.id)
            self.kg.add_entity(e)

        if verbose:
            print("ðŸ”— æž„å»ºçŸ¥è¯†å›¾è°±...")

        self.section_names = []
        for res in results:
            md = res.get("chunk_metadata", {})
            # Section å®žä½“
            secs = self._create_section_entities(md, res["chunk_id"])
            for se in secs:
                # if se.id not in self.kg.entities:
                if se.name not in self.section_names and se.id not in self.kg.entities:
                    self.kg.add_entity(se)
                    self.section_names.append(se.name)

            # Section contains inner entities
            # inner = [entity_map[name2id[e["name"]]]
            #          for e in res.get("entities", []) if e["name"] in name2id]
            inner = self.section_entities_collection[se.name]
            for se in secs:
                self._link_section_to_entities(se, inner, res["chunk_id"])

            # æ™®é€šå…³ç³»
            for rdata in res.get("relations", []):
                rel = self._create_relation_from_data(rdata, res["chunk_id"], entity_map, name2id)
                if rel:
                    self.kg.add_relation(rel)

        # å†™å…¥æ•°æ®åº“
        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°æ•°æ®åº“...")
        self._store_knowledge_graph(verbose)
        self.neo4j_utils.enrich_event_nodes_with_context()

        if verbose:
            st = self.kg.stats()
            print(f"ðŸŽ‰ çŸ¥è¯†å›¾è°±æž„å»ºå®Œæˆ!")
            # print(f"   - å®žä½“æ•°é‡: {st['entities']}")
            # print(f"   - å…³ç³»æ•°é‡: {st['relations']}")
            graph_stats = self.graph_store.get_stats()
            print(f"   - å®žä½“æ•°é‡: {graph_stats['entities']}")
            print(f"   - å…³ç³»æ•°é‡: {graph_stats['relations']}")
            print(f"   - æ–‡æ¡£æ•°é‡: {st['documents']}")
            print(f"   - æ–‡æœ¬å—æ•°é‡: {st['chunks']}")

        return self.kg

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  å†…éƒ¨å·¥å…·
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # -------- åˆå¹¶å®žä½“ï¼ˆæ ¹æ® doc_type é€‚é…ï¼‰ --------
    def merge_entities_info(self, extraction_results):
        """
        éåŽ†ä¿¡æ¯æŠ½å–ç»“æžœï¼Œåˆå¹¶ / åŽ»é‡å®žä½“ã€‚
        - â€œå±€éƒ¨ä½œç”¨åŸŸâ€å®žä½“ï¼ˆscope == localï¼‰è‹¥å‘½åå†²çªï¼Œä¼šåœ¨å‰é¢åŠ ä¸Š
          â€œåœºæ™¯N â€¦â€ æˆ– â€œç« èŠ‚N â€¦â€ ä½œä¸ºå‰ç¼€ï¼Œé¿å…é‡åã€‚
        - section çš„ç¼–å·ä¼˜å…ˆä½¿ç”¨ chunk_metadata.orderï¼›è‹¥æ— ï¼Œåˆ™é€€åŒ–ä¸º titleã€‚
        """
        extraction_results = self.graph_preprocessor.run_entity_disambiguation(extraction_results)
        
        entity_map: Dict[str, Entity] = {}
        self.chunk2section_map = {result["chunk_id"]: result["chunk_metadata"]["doc_title"] for result in extraction_results}
        self.section_entities_collection = dict()
        
        base = self.config.storage.knowledge_graph_path
        output_path = os.path.join(base, "chunk2section.json")
        # with open(output_path, "w") as f:
        #     json.dump(self.chunk2section_map, f)
    
        # ä¸­æ–‡å‰ç¼€è¯ï¼šScene â†’ åœºæ™¯ï¼›Chapter â†’ ç« èŠ‚
        for result in extraction_results:
            md = result.get("chunk_metadata", {}) or {}
            label = md.get('doc_title', md.get('subtitle', md.get('title', "")))
            
            if label not in self.section_entities_collection:
                self.section_entities_collection[label] = []
                
            # â€”â€” å¤„ç†å½“å‰ chunk æŠ½å–å‡ºçš„å®žä½“ â€”â€”
            for ent_data in result.get("entities", []):
                # å†²çªå¤„ç†ï¼šå±€éƒ¨å®žä½“é‡åå‰åŠ å‰ç¼€
                if (ent_data.get("scope", "").lower() == "local" or ent_data.get("type", "") in ["Action", "Emotion", "Goal"])and ent_data["name"] in entity_map:
                    existing_entity = entity_map[ent_data["name"]]
                    existing_chunk_id = existing_entity.source_chunks[0]
                    existing_section_name = self.chunk2section_map[existing_chunk_id]
                    current_section_name = md["doc_title"]
                    if current_section_name != existing_section_name: # å¦‚æžœä¸å±žäºŽåŒç« èŠ‚çš„localï¼Œéœ€è¦é‡å‘½åã€‚
                        new_name = f"{ent_data['name']}_in_{label}"
                        suffix = 1
                        while new_name in entity_map:        # ä»å†²çªåˆ™è¿½åŠ  _n
                            suffix += 1
                            new_name = f"{ent_data['name']}_in_{label}_{suffix}"
                        ent_data["name"] = new_name

                # åˆ›å»º / åˆå¹¶
                ent_obj = self._create_entity_from_data(ent_data, result["chunk_id"])
                existing = self._find_existing_entity(ent_obj, entity_map)
                if existing:
                    self._merge_entities(existing, ent_obj)
                else:
                    entity_map[ent_obj.name] = ent_obj
                self.section_entities_collection[label].append(ent_obj)

        
        output_path = os.path.join(base, "section_entities_collection.pkl")
        # print("[CHECK] self.section_entities_collection: ", self.section_entities_collection)
        with open(output_path, "wb") as f:
            pickle.dump(self.section_entities_collection, f)
        
        return entity_map

    
    def _find_existing_entity(self, entity: Entity, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„å®žä½“"""
        if entity.type == "Event":
            return None
        if entity.name in entity_map:
            return entity_map[entity.name]
        for existing_entity in entity_map.values():
            if entity.name in existing_entity.aliases:
                return existing_entity
            if any(alias in existing_entity.aliases for alias in entity.aliases):
                return existing_entity
        return None
    
    
    def _merge_entities(self, existing: Entity, new: Entity) -> None:
        """åˆå¹¶å®žä½“ä¿¡æ¯"""
        for alias in new.aliases:
            if alias not in existing.aliases:
                existing.aliases.append(alias)
        existing.properties.update(new.properties)
        for chunk_id in new.source_chunks:
            if chunk_id not in existing.source_chunks:
                existing.source_chunks.append(chunk_id)
        if new.description:
            if not existing.description:
                existing.description = new.description
            elif new.description not in existing.description:
                existing.description = existing.description + "\n" + new.description
    
    def _ensure_entity_exists(self, entity_id: str, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        return entity_map.get(entity_id, None)

    # -------- Section / Contains --------
    def _create_section_entities(self, md: Dict[str, Any], chunk_id: str) -> List[Entity]:
        """
        åˆ›å»ºç« èŠ‚/åœºæ™¯å®žä½“ã€‚
        - title/subtitle æ€»æ˜¯ä»Ž "title"/"subtitle" å­—æ®µè¯»å–
        - Entity.properties å†™å…¥æ˜ å°„å­—æ®µï¼ˆå¦‚ scene_nameï¼‰ + å…¶ä»–æœ‰ç”¨ metadata å­—æ®µ
        """
        raw_title = md.get("title", "").strip()
        raw_subtitle = md.get("subtitle", "").strip()
        order = md.get("order", None)

        if not raw_title:
            return []

        label = self.meta["section_label"]
        full_name = md.get("doc_title", f"{label}{raw_title}-{raw_subtitle}" if raw_subtitle else f"{label}{raw_title}")
        eid = f"{label.lower()}_{order}" if order is not None else f"{label.lower()}_{hash(full_name) % 1_000_000}"

        title_field = self.meta["title"]
        subtitle_field = self.meta["subtitle"]

        # æž„å»º propertiesï¼šå†™å…¥ title/subtitle æ˜ å°„å­—æ®µ + å…¶ä»–æœ‰æ•ˆå­—æ®µ
        excluded = {"chunk_index", "chunk_type", "doc_title", "title", "subtitle", "total_description_chunks", "total_doc_chunks"}
        properties = {
            title_field: raw_title,
            subtitle_field: raw_subtitle,
        }
        
        if order is not None:
            properties["order"] = order

        for k, v in md.items():
            if k not in excluded:
                properties[k] = v

        return [
            Entity(
                id=eid,
                name=full_name,
                type=label,
                description=md.get("summary", ""),  # å¯é€‰ï¼šç”¨ summary ä½œä¸ºç®€è¦æè¿°
                properties=properties,
                source_chunks=[] # è¶…èŠ‚ç‚¹ä¸éœ€è¦chunk_ids
            )
        ]


    def _link_section_to_entities(self, section: Entity, inners: List[Entity], chunk_id: str):
        pred = self.meta["contains_pred"]
        for tgt in inners:
            rid = f"rel_{hash(f'{section.id}_{pred}_{tgt.id}') % 1_000_000}"
            self.kg.add_relation(
                Relation(id=rid, subject_id=section.id, predicate=pred,
                         object_id=tgt.id, properties={}, source_chunks=[chunk_id])
            )

    # -------- Entity / Relation creation --------
    @staticmethod
    def _create_entity_from_data(data: Dict, chunk_id: str) -> Entity:
        return Entity(id=f"ent_{hash(data['name']) % 1_000_000}",
                      name=data["name"],
                      type=data.get("type", "Concept"),
                      scope=data.get("scope", "local"),
                      description=data.get("description", ""),
                      aliases=data.get("aliases", []),
                      source_chunks=[chunk_id])

    @staticmethod
    def _create_relation_from_data(d: Dict, chunk_id: str,
                                   entity_map: Dict[str, Entity],
                                   name2id: Dict[str, str]) -> Optional[Relation]:
        subj = d.get("subject") or d.get("source") or d.get("head") or d.get("head_entity")
        obj = d.get("object") or d.get("target") or d.get("tail") or d.get("tail_entity")
        pred = d.get("predicate") or d.get("relation") or d.get("relation_type")
        if not subj or not obj or not pred:
            return None
        sid, oid = name2id.get(subj), name2id.get(obj)
        if not sid or not oid:
            return None
        rid = f"rel_{hash(f'{sid}_{pred}_{oid}') % 1_000_000}"
        
        return Relation(
            id=rid,
            subject_id=sid,
            predicate=pred,
            object_id=oid,
            properties={
                "description": d.get("description", ""),
                "relation_name": d.get("relation_name", "")
            },
            source_chunks=[chunk_id]
        )
        # return Relation(id=rid, subject_id=sid, predicate=pred,
        #                 object_id=oid, properties={}, source_chunks=[chunk_id])

    # -------- å­˜å‚¨ --------
    # def _build_relational_database(self, dialog_chunks: List[TextChunk]):
    #     rows = [{
    #         "id": c.id,
    #         "content": c.content.split("ï¼š")[-1].strip(),
    #         "character": c.metadata.get("character", ""),
    #         "type": c.metadata.get("type") or "regular",
    #         "remark": "ï¼Œ".join(c.metadata.get("remark", [])),
    #         "title": c.metadata.get("title", ""),
    #         "subtitle": c.metadata.get("subtitle", ""),
    #     } for c in dialog_chunks]

    #     db_dir = self.config.storage.sql_database_path
    #     os.makedirs(db_dir, exist_ok=True)
    #     db_path = os.path.join(db_dir, "conversations.db")
    #     if os.path.exists(db_path):
    #         os.remove(db_path)
    #     df = pd.DataFrame(rows)
    #     df.to_sql("dialogues", sqlite3.connect(db_path), if_exists="replace", index=False)

    def _store_vectordb(self, verbose: bool):
        try:
            self.vector_store.delete_collection()
            self.vector_store._initialize()
            self.vector_store.store_documents(list(self.kg.documents.values()))
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")

    def _store_knowledge_graph(self, verbose: bool):
        try:
            self.graph_store.store_knowledge_graph(self.kg)
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Embedding & Stats
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_emebdding_model(self.config.memory.embedding_model_name)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            exclude_entity_types=[self.meta["section_label"]]
            # exclude_relation_types=[self.meta["contains_pred"]],
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… å›¾å‘é‡æž„å»ºå®Œæˆ")

    #
    def get_stats(self) -> Dict[str, Any]:
        return {
            "knowledge_graph": self.kg.stats(),
            "graph_store": self.graph_store.get_stats(),
            "vector_store": self.vector_store.get_stats(),
        }
