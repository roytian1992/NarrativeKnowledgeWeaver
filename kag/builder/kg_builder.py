# kag/builder/graph_builder.py

"""
çŸ¥è¯†å›¾è°±æž„å»ºå™¨ä¸»æ¨¡å— - é›†æˆæ–°å¢žåŠŸèƒ½

æ•´åˆä¿¡æ¯æŠ½å–ã€æ•°æ®å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ï¼Œæ”¯æŒä¼˜åŒ–çš„å‰§æœ¬å¤„ç†ç­–ç•¥
"""
from copy import deepcopy
import time
from typing import List, Dict, Any, Optional
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import json
from pathlib import Path
from ..models.entities import KnowledgeGraph, Entity, Relation, Document, TextChunk

from ..utils.config import KAGConfig
from ..utils.format import correct_json_format
from .processor import DocumentProcessor
# from .extractor import InformationExtractor
from ..storage.graph_store import GraphStore
from ..storage.document_store import DocumentStore
from ..storage.vector_store import VectorStore
import pandas as pd
import sqlite3
import pickle
from kag.llm.llm_manager import LLMManager
# from kag.builder.reflection import DynamicReflector
from kag.agent.kg_extraction_agent import InformationExtractionAgent
from kag.agent.attribute_extraction_agent import AttributeExtractionAgent
from dataclasses import asdict 
from kag.utils.neo4j_utils import Neo4jUtils
# from ..schema.kg_schema import ENTITY_TYPES, RELATION_TYPE_GROUPS
import os


class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æž„å»ºå™¨ - é›†æˆæ–°å¢žåŠŸèƒ½"""
    
    def __init__(self, config: KAGConfig):
        self.config = config
        # self.reset()
        self.llm_manager = LLMManager(config)
        self.llm = self.llm_manager.get_llm()
        self.graph_store = GraphStore(config)
        self.neo4j_utils = Neo4jUtils(self.graph_store.driver)
        self.vector_store = VectorStore(config)
        self.document_store = DocumentStore(config)
        self.kg = KnowledgeGraph()
        self.max_workers = 32
        self.load_schema("kag/schema/graph_schema.json")
        self.load_abbreviations("kag/schema/settings_schema.json")
        self.processor = DocumentProcessor(config, self.llm)
        self.information_extraction_agent = InformationExtractionAgent(config, self.llm)
        self.attribute_extraction_agent = AttributeExtractionAgent(config, self.llm)
        
    def load_abbreviations(self, path):
        """ä»ŽJSONæ–‡ä»¶åŠ è½½ç¼©å†™åˆ—è¡¨ï¼Œè¿”å›žæ ¼å¼åŒ–åŽçš„æ–‡æœ¬ï¼ˆé€‚åˆæ’å…¥æç¤ºè¯ï¼‰"""
        with open(path, 'r', encoding='utf-8') as f:
            abbr = json.load(f)
        abbr_list = abbr.get("abbreviations", [])

        formatted = []
        for item in abbr_list:
            line = f"- **{item['abbr']}**: {item['full']}ï¼ˆ{item['zh']}ï¼‰ - {item['description']}"
            formatted.append(line)
        self.abbreviation_info = "\n".join(formatted)


    def load_schema(self, path):
        with open(path, 'r', encoding='utf-8') as f:
            schema = json.load(f)
        self.entity_types = schema.get("entities")
        self.relation_type_groups = schema.get("relations")

        self.entity_type_description_text = "\n".join(
            f"- {item['type']}: {item['description']}" for item in self.entity_types
        )

        self.relation_type_description_text = "\n".join(
            f"- {item['type']}: {item['description']}"
            for group in self.relation_type_groups.values()
            for item in group
        )

        RELATION_TYPES = []
        for group in self.relation_type_groups.values():
            RELATION_TYPES.extend(group)

        print("âœ… æˆåŠŸåŠ è½½çŸ¥è¯†å›¾è°±æ¨¡å¼")


    def reset(self):
        path = Path(self.config.storage.knowledge_graph_path)
        for json_file in path.glob("*.json"):
            json_file.unlink()  # åˆ é™¤æ–‡ä»¶


    def prepare_chunks(self, json_file_path: str, verbose: bool = True) -> Dict[str, Any]:
        """ä»ŽJSONæ–‡ä»¶æž„å»ºçŸ¥è¯†å›¾è°±å‰çš„å¤„ç†å’Œä¿¡æ¯æŠ½å–ï¼Œæ‹†åˆ†æž„å›¾é€»è¾‘ä¸ºç‹¬ç«‹æ­¥éª¤"""

        if verbose:
            print(f"ðŸš€ å¼€å§‹æž„å»ºçŸ¥è¯†å›¾è°±: {json_file_path}")

        # 1. åŠ è½½æ–‡æ¡£
        if verbose:
            print("ðŸ“– åŠ è½½æ–‡æ¡£...")
        documents = self.processor.load_from_json(json_file_path)
        if verbose:
            print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")

        # 2. æ‹†åˆ†æ–‡æ¡£
        all_description_chunks = []
        all_conversation_chunks = []
        
        for doc in tqdm(documents, total=len(documents), desc="æ–‡æœ¬æ‹†åˆ†ä¸­"):
            chunk_groups = self.processor.prepare_chunk(doc)
            all_description_chunks.extend(chunk_groups["description_chunks"])
            all_conversation_chunks.extend(chunk_groups["conversation_chunks"])
            
        # 3. å­˜å‚¨æ–‡æœ¬å—
        base_path = self.config.storage.knowledge_graph_path
        with open(os.path.join(base_path, "all_description_chunks.json"), "w", encoding="utf-8") as f:
            json.dump([chunk.dict() for chunk in all_description_chunks], f, ensure_ascii=False, indent=2)
        with open(os.path.join(base_path, "all_conversation_chunks.json"), "w", encoding="utf-8") as f:
            json.dump([chunk.dict() for chunk in all_conversation_chunks], f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… ç”Ÿæˆ {len(all_description_chunks)} ä¸ªå‰§æœ¬æ–‡æœ¬å—")

    def store_chunks(self, verbose: bool = True) -> None:
        # 4. å­˜å‚¨å¯¹è¯ä¿¡æ¯åˆ°å…³ç³»æ•°æ®åº“
        self.vector_store.delete_collection()
        self.vector_store._initialize()
        base_path = self.config.storage.knowledge_graph_path
        with open(os.path.join(base_path, "all_description_chunks.json"), "r", encoding="utf-8") as f:
            description_data = json.load(f)
        with open(os.path.join(base_path, "all_conversation_chunks.json"), "r", encoding="utf-8") as f:
            conversation_data = json.load(f)
            
        all_description_chunks = [TextChunk(**chunk) for chunk in description_data]
        all_conversation_chunks = [TextChunk(**chunk) for chunk in conversation_data]

        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°å…³ç³»æ•°æ®åº“...")
        for chunk in all_description_chunks:
            self.kg.add_document(self.processor.prepare_document(chunk))
            self.kg.add_chunk(chunk)
            
        self._build_relational_database(all_conversation_chunks)

        # 5. å­˜å‚¨æ–‡æ¡£ä¿¡æ¯åˆ°å‘é‡æ•°æ®åº“
        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
        self._store_vectordb(verbose)
    
    def extract_entity_and_relation(self, verbose: bool = True) -> List[Dict[str, Any]]:
        """æŠ½å–å®žä½“ä¸Žå…³ç³»ä¿¡æ¯ï¼Œä¿å­˜ä¸º extraction_results.json"""

        base_path = self.config.storage.knowledge_graph_path
        with open(os.path.join(base_path, "all_description_chunks.json"), "r", encoding="utf-8") as f:
            all_description_chunks = json.load(f)

        all_description_chunks = [TextChunk(**chunk) for chunk in all_description_chunks]

        if verbose:
            print("ðŸ§  å®žä½“ä¸Žå…³ç³»ä¿¡æ¯æŠ½å–ä¸­...")

        extraction_results = self._kg_extraction_multithread(all_description_chunks, self.max_workers)
        extraction_results = [r for r in extraction_results if r is not None]

        with open(os.path.join(base_path, "extraction_results.json"), "w", encoding="utf-8") as f:
            json.dump(extraction_results, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å®žä½“ä¸Žå…³ç³»ä¿¡æ¯æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {len(extraction_results)} ä¸ªæ–‡æœ¬å—")

        return extraction_results
    
    def extract_entity_attributes(self, verbose: bool = True) -> Dict[str, Entity]:
        """åŸºäºŽå·²æœ‰å®žä½“æŠ½å–ç»“æžœï¼ŒæŠ½å–å±žæ€§å¹¶ä¿å­˜ä¸º entity_info.json"""

        base_path = self.config.storage.knowledge_graph_path
        with open(os.path.join(base_path, "extraction_results.json"), "r", encoding="utf-8") as f:
            extraction_results = json.load(f)

        # åˆå¹¶å¹¶åŽ»é‡å®žä½“
        entity_map = self.merge_entities_info(extraction_results)

        if verbose:
            print("ðŸ”Ž å±žæ€§æŠ½å–ä¸­...")

        entity_map = self._attribute_extraction_multithread(entity_map, self.max_workers)

        # ä¿å­˜
        entity_map_ = {k: v.dict() for k, v in entity_map.items()}
        with open(os.path.join(base_path, "entity_info.json"), "w", encoding="utf-8") as f:
            json.dump(entity_map_, f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±žæ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®žä½“ {len(entity_map)} ä¸ª")

        return entity_map


    def build_graph_from_results(self, verbose: bool = True) -> KnowledgeGraph:
        """ä»ŽæŠ½å–ç»“æžœæ–‡ä»¶æž„å»ºçŸ¥è¯†å›¾è°±å¹¶å†™å…¥å›¾æ•°æ®åº“"""

        if verbose:
            print("ðŸ“‚ åŠ è½½å·²æœ‰æŠ½å–ç»“æžœå’Œå®žä½“ä¿¡æ¯...")

        base_path = self.config.storage.knowledge_graph_path

        # åŠ è½½æŠ½å–ç»“æžœ
        extraction_file = os.path.join(base_path, "extraction_results.json")
        with open(extraction_file, "r", encoding="utf-8") as f:
            extraction_results = json.load(f)

        # åŠ è½½å®žä½“ä¿¡æ¯
        entity_file = os.path.join(base_path, "entity_info.json")
        with open(entity_file, "r", encoding="utf-8") as f:
            entity_info_raw = json.load(f)

        # é‡æž„å®žä½“å¯¹è±¡å¹¶åˆ›å»º id->Entity æ˜ å°„
        entity_map = {
            data["id"]: Entity(**data)
            for data in entity_info_raw.values()
        }
        
        # print("***: ", entity_info_raw)

        name_to_id = {}
        for entity in entity_map.values():
            name_to_id[entity.name] = entity.id
            for alias in entity.aliases:
                if alias not in name_to_id:
                    name_to_id[alias] = entity.id


        # æž„å»ºå›¾è°±
        if verbose:
            print("ðŸ”— æž„å»ºçŸ¥è¯†å›¾è°±...")
        self._build_knowledge_graph(entity_map, extraction_results, name_to_id, verbose)

        # å­˜å‚¨å›¾è°±
        if verbose:
            print("ðŸ’¾ å­˜å‚¨åˆ°æ•°æ®åº“...")
        self._store_knowledge_graph(verbose)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        if verbose:
            stats = self.kg.stats()
            print(f"ðŸŽ‰ çŸ¥è¯†å›¾è°±æž„å»ºå®Œæˆ!")
            print(f"   - å®žä½“æ•°é‡: {stats['entities']}")
            print(f"   - å…³ç³»æ•°é‡: {stats['relations']}")
            print(f"   - æ–‡æ¡£æ•°é‡: {stats['documents']}")
            print(f"   - æ–‡æœ¬å—æ•°é‡: {stats['chunks']}")

            num_scene = sum(1 for r in self.kg.relations.values() if r.predicate == "SCENE_CONTAINS")
            num_other = sum(1 for r in self.kg.relations.values() if r.predicate != "SCENE_CONTAINS")
            print(f"   - SCENE_CONTAINS å…³ç³»æ•°: {num_scene}")
            print(f"   - å…¶ä»–å®žä½“å…³ç³»æ•°: {num_other}")

        return self.kg
        

    
    def _kg_extraction(self, chunks: List, verbose: bool) -> List[Dict]:
        """å¹¶è¡Œä¿¡æ¯æŠ½å–ï¼ˆå¢žå¼ºç‰ˆï¼‰ï¼šæ”¯æŒåæ€ä¸Žä½Žåˆ†é‡æŠ½ + å¾—åˆ†æœ€ä¼˜ä¿å­˜"""
        extraction_results = []
    
        for chunk in tqdm(chunks):
            content =   chunk.content
            result = self.information_extraction_agent.run(content)
            result["chunk_id"] = chunk.id
            result["scene_metadata"] = chunk.metadata
            extraction_results.append(result)
            
        return extraction_results

    def _kg_extraction_multithread(self, chunks: List, max_workers: int = 8) -> List[Dict]:
        """å¹¶è¡Œä¿¡æ¯æŠ½å–ï¼ˆå¢žå¼ºç‰ˆï¼‰ï¼šæ”¯æŒåæ€ä¸Žä½Žåˆ†é‡æŠ½ + å¾—åˆ†æœ€ä¼˜ä¿å­˜ + å¹¶å‘åŠ é€Ÿ"""
        extraction_results = []

        def process_chunk(chunk):
            if len(chunk.content.strip()) > 0:
                result = self.information_extraction_agent.run(chunk.content)
            else:
                result = {
                    "entities": [],
                    "relations": [],
                    "suggestions": [],
                    "issues": [],
                    "score": 0,
                }
            result["chunk_id"] = chunk.id
            result["scene_metadata"] = chunk.metadata
            return result

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(process_chunk, chunk) for chunk in chunks]
            for future in tqdm(as_completed(futures), total=len(futures), desc="å¹¶å‘æŠ½å–ä¸­"):
                extraction_results.append(future.result())

        return extraction_results


    def _attribute_extraction(self, entity_map: Dict[str, Entity]) -> Dict[str, Entity]:
        new_entity_map = {}

        for entity_name, entity in tqdm(entity_map.items(), desc="å±žæ€§æŠ½å–ä¸­ï¼ˆä¸²è¡Œï¼‰"):
            entity_type = entity.type.name
            text = entity.description or ""

            if not text.strip():
                continue

            try:
                result = self.attribute_extraction_agent.run(
                    text=text,
                    entity_name=entity_name,
                    entity_type=entity_type,
                    original_text=""
                )
                attributes = result.get("attributes", {})
                if isinstance(attributes, str):
                    try:
                        attributes = json.loads(attributes)
                    except json.JSONDecodeError:
                        print(f"[ERROR] æ— æ³•è§£æž JSON: {attributes}")
                        attributes = {}

                # å¦‚æžœæ˜¯ listï¼Œåˆ™å–ç¬¬ä¸€ä¸ªï¼ˆä¿å®ˆå¤„ç†ï¼‰
                if isinstance(attributes, list):
                    if attributes:
                        attributes = attributes[0]
                    else:
                        attributes = {}
                    
                new_entity = deepcopy(entity)
                new_entity.properties = attributes
                new_entity.description = ""
                new_entity_map[entity_name] = new_entity

            except Exception as e:
                print(f"[ERROR] æŠ½å–å¤±è´¥ï¼š{entity_name} - {e}")

        return new_entity_map


    def _attribute_extraction_multithread(self, entity_map: Dict[str, Entity], max_workers: int = 8) -> Dict[str, Entity]:
        new_entity_map = {}

        def process(entity_name, entity):
            entity_type = entity.type
            # print("[CHECK] entity_typeï¼š", entity_type)
            source_chunks = entity.source_chunks
            text = entity.description or ""
            if not text.strip():
                return entity_name, None  # ç©ºå†…å®¹è·³è¿‡

            try:
                result = self.attribute_extraction_agent.run(
                    text=text,
                    entity_name=entity_name,
                    entity_type=entity_type,
                    source_chunks=source_chunks,
                    original_text=""
                )
                attributes = result.get("attributes", {})
                # description = result.get("description", "")
                # print("[CHECK] description: ", result)
                if "new_description" not in result:
                    print("[CHECK] result: ", result)
                description = result.get("new_description", "")
                
                # print("[CHECK] result: ", result)
                # print("[CHECK] æ–°çš„æè¿°: ", description)
                if isinstance(attributes, str):
                    attributes = json.loads(attributes)

                new_entity = deepcopy(entity)
                new_entity.properties = attributes
                # if new_entity.type == "Event":
                #     print("[CHECK] attributes: ", attributes)
                if description:
                    new_entity.description = description
                return entity_name, new_entity

            except Exception as e:
                print(f"[ERROR] æŠ½å–å¤±è´¥ï¼š{entity_name} - {e}")
                return entity_name, None

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [
                executor.submit(process, name, entity)
                for name, entity in entity_map.items()
            ]
            for future in tqdm(as_completed(futures), total=len(futures), desc="å±žæ€§æŠ½å–ä¸­ï¼ˆå¹¶å‘ï¼‰"):
                entity_name, updated_entity = future.result()
                if updated_entity:
                    new_entity_map[entity_name] = updated_entity

        return new_entity_map

    
    def _build_relational_database(self, conversation_chunks: List):
        conversation_data = []
        for item in conversation_chunks:
            conversation_data.append({
                "id": item.id,
                "content": item.content.split("ï¼š")[-1].strip(),
                "character": item.metadata["character"].strip(),
                "type": item.metadata.get("type") or "regular",
                "remark": "ï¼Œ".join(item.metadata.get("remark", [])),
                "scene_number": item.metadata.get("scene_number"),
                "sub_scene_number": item.metadata.get("sub_scene_number"),
            })

        df = pd.DataFrame(conversation_data)
        db_path = os.path.join(self.config.storage.sql_database_path, "conversations.db")
        if os.path.exists(db_path):
            os.remove(db_path)

        connection = sqlite3.connect(db_path)
        df.to_sql('äººç‰©å¯¹è¯', connection, if_exists='replace', index=False)

    
    def _build_knowledge_graph(
        self,
        entity_map: Dict[str, Entity],
        extraction_results: List[Dict],
        name_to_id: Dict[str, str],
        verbose: bool = True
    ):
        if verbose:
            print("ðŸ”— æ­£åœ¨æž„å»ºçŸ¥è¯†å›¾è°±...")

        # 1. æ·»åŠ å®žä½“
        for ent in entity_map.values():
            self.kg.add_entity(ent)

        # 2. éåŽ†æ¯ä¸ª chunkï¼Œæž„å»ºåœºæ™¯ä¸Žæ™®é€šå…³ç³»
        for result in extraction_results:
            chunk_id = result["chunk_id"]

            # åœºæ™¯å®žä½“
            scene_entities = self._create_scene_entities(result.get("scene_metadata", {}), chunk_id)
            for se in scene_entities:
                if se.name not in name_to_id:
                    name_to_id[se.name] = se.id
                    entity_map[se.id] = se
                self.kg.add_entity(se)

            # Scene â†’ contains â†’ inner entities
            inner_entity_objs = [
                entity_map[name_to_id[e_data["name"]]]
                for e_data in result.get("entities", [])
                if e_data["name"] in name_to_id
            ]
            for scene_ent in scene_entities:
                self._link_scene_to_entities(scene_ent, inner_entity_objs, chunk_id)

            # æ™®é€šå®žä½“å…³ç³»
            for r_data in result.get("relations", []):
                rel = self._create_relation_from_data(r_data, chunk_id, entity_map, name_to_id)
                # if not rel:
                #     print("[CHECK] r_data: ", r_data)
                if rel:
                    self.kg.add_relation(rel)


    def merge_entities_info(self, extraction_results):
        entity_map = {}  # ç”¨äºŽå®žä½“åŽ»é‡å’Œåˆå¹¶
        for result in extraction_results:
            scene_md = result.get("scene_metadata", {})
            if scene_md.get("sub_scene_number"):
                play_name = f"åœºæ™¯{scene_md.get('scene_number')}-{scene_md.get('sub_scene_number')}"
            else:
                play_name = f"åœºæ™¯{scene_md.get('scene_number')}"
            
            # å¤„ç†åŸºç¡€å®žä½“
            for entity_data in result.get("entities", []):
                if entity_data.get("scope").lower()=="local" and entity_data["name"] in entity_map:
                # åœ¨å·²æœ‰åå­—å‰åŠ åœºæ™¯å‰ç¼€ï¼›å¦‚å‰ç¼€å·²å­˜åœ¨åˆ™å†è¿½åŠ è®¡æ•°
                    new_name = f"{play_name}ä¸­çš„{entity_data['name']}"
                    suffix = 1
                    while new_name in entity_map:        # ä»å†²çªå°±åŠ  _n
                        suffix += 1
                        new_name = f"{play_name}ä¸­çš„{entity_data['name']}_{suffix}"
                    entity_data["name"] = new_name
                
                entity = self._create_entity_from_data(entity_data, result["chunk_id"])
                existing_entity = self._find_existing_entity(entity, entity_map)
                if existing_entity:
                    self._merge_entities(existing_entity, entity)
                else:
                    entity_map[entity.name] = entity
        return entity_map
    
    def _create_scene_entities(
            self,
            scene_metadata: Dict[str, Any],
            chunk_id: str
    ) -> List[Entity]:
        """ä»…åˆ›å»ºåœºæ™¯å®žä½“ï¼ˆä¸å†ç”Ÿæˆåœ°ç‚¹å®žä½“ï¼‰"""
        entities = []
        if scene_metadata.get("sub_scene_number", ""):
            play_name = f"åœºæ™¯{scene_metadata.get("scene_number")}-{scene_metadata.get("sub_scene_number")}"
        else:
            play_name = f"åœºæ™¯{scene_metadata.get("scene_number")}"
        
        if play_name:
            scene_entity = Entity(
                id=f"scene_{hash(play_name) % 1_000_000}",
                name=play_name,
                type="Scene",                           # ç›´æŽ¥å­—ç¬¦ä¸²
                description=f"å±žäºŽåœºæ™¯: {scene_metadata.get("scene_name", "")}",
                properties=scene_metadata,             # æŒ‚å…¨éƒ¨å…ƒæ•°æ®
                source_chunks=[chunk_id],
            )
            entities.append(scene_entity)

        return entities

    
    def _create_entity_from_data(self, entity_data: Dict, chunk_id: str) -> Entity:
        """ä»Žæ•°æ®åˆ›å»ºå®žä½“"""
        entity_type = entity_data.get("type", "Concept")

        return Entity(
            id=f"entity_{hash(entity_data['name']) % 1000000}",
            name=entity_data["name"],
            type=entity_type,
            description=entity_data.get("description", ""),
            aliases=entity_data.get("aliases", []),
            source_chunks=[chunk_id]
        )
    
    def _create_relation_from_data(
        self,
        relation_data: Dict,
        chunk_id: str,
        entity_map: Dict[str, Entity],
        name_to_id: Dict[str, str]
    ) -> Optional[Relation]:
        """ä»Žæ•°æ®åˆ›å»ºå…³ç³»"""
        subject_name = (
            relation_data.get("subject")
            or relation_data.get("source")
            or relation_data.get("head")
            or relation_data.get("head_entity")
        )
        object_name = (
            relation_data.get("object")
            or relation_data.get("target")
            or relation_data.get("tail")
            or relation_data.get("tail_entity")
        )
        predicate = (
            relation_data.get("predicate")
            or relation_data.get("relation")
            or relation_data.get("relation_type")
        )
        
        if not subject_name or not object_name or not predicate:
            return None


        subject_id = name_to_id.get(subject_name)
        object_id = name_to_id.get(object_name)

        if not subject_id:
            print("[CHECK] subject: ", subject_name, predicate, object_name)
            
        if not object_id:
            print("[CHECK] object: ", subject_name, predicate, object_name)
            
        if not subject_id or not object_id:
             return None

        relation_id_str = f"{subject_id}_{predicate}_{object_id}"
        return Relation(
            id=f"rel_{hash(relation_id_str) % 1000000}",
            subject_id=subject_id,
            predicate=predicate,
            object_id=object_id,
            properties={
                "description": relation_data.get("description", ""),
                "relation_name": relation_data.get("relation_name", "")
            },
            source_chunks=[chunk_id]
        )

    
    def _find_existing_entity(self, entity: Entity, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„å®žä½“"""
        if entity.type == "Event":
            return None
        if entity.name in entity_map:
            return entity_map[entity.name]
        for existing_entity in entity_map.values():
            if entity.name in existing_entity.aliases:
                return existing_entity
            if any(alias in existing_entity.aliases for alias in entity.aliases):
                return existing_entity
        return None
    
    
    def _merge_entities(self, existing: Entity, new: Entity) -> None:
        """åˆå¹¶å®žä½“ä¿¡æ¯"""
        for alias in new.aliases:
            if alias not in existing.aliases:
                existing.aliases.append(alias)
        existing.properties.update(new.properties)
        for chunk_id in new.source_chunks:
            if chunk_id not in existing.source_chunks:
                existing.source_chunks.append(chunk_id)
        if new.description:
            if not existing.description:
                existing.description = new.description
            elif new.description not in existing.description:
                existing.description = existing.description + "\n" + new.description
    
    def _ensure_entity_exists(self, entity_id: str, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        return entity_map.get(entity_id, None)

    
    def _link_scene_to_entities(
            self,
            scene_entity: Entity,
            inner_entities: List[Entity],
            chunk_id: str
    ) -> None:
        """
        ä¸ºå½“å‰åœºæ™¯å®žä½“ scene_entity ä¸Žå…¶å†…éƒ¨å®žä½“ inner_entities
        åˆ›å»º "SCENE_CONTAINS" å…³ç³»å¹¶å†™å…¥ self.kg
        """
        for target in inner_entities:
            rel_id = f"{scene_entity.id}_scene_contains_{target.id}"
            relation = Relation(
                id=f"rel_{hash(rel_id) % 1_000_000}",
                subject_id=scene_entity.id,
                object_id=target.id,
                predicate="SCENE_CONTAINS",
                properties={},
                source_chunks=[chunk_id],
                # confidence=1.0,
            )
            self.kg.add_relation(relation)


    def _store_vectordb(self, verbose: bool):
        try:
            if verbose:
                print("   - å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
            self.vector_store.delete_collection()
            self.vector_store._initialize()
            self.vector_store.store_documents(list(self.kg.documents.values()))
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {str(e)}")



    def _store_knowledge_graph(self, verbose: bool) -> None:
        """å­˜å‚¨çŸ¥è¯†å›¾è°±åˆ°æ•°æ®åº“"""
        try:
            if verbose:
                print("   - å­˜å‚¨åˆ°Neo4j...")
            self.graph_store.store_knowledge_graph(self.kg)
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {str(e)}")
                
    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_emebdding_model(self.config.memory.embedding_model_name)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(exclude_node_types=["Scene"], exclude_rel_types=["SCENE_CONTAINS"])
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… å›¾å‘é‡æž„å»ºå®Œæˆ")

    def search_entities(self, query: str, limit: int = 10) -> List[Entity]:
        return self.graph_store.search_entities(query, limit)
    
    def search_relations(self, entity_name: str, limit: int = 10) -> List[Relation]:
        return self.graph_store.search_relations(entity_name, limit)
    
    def semantic_search(self, query: str, limit: int = 5) -> List[Document]:
        return self.vector_store.search(query, limit)
    
    def get_stats(self) -> Dict[str, Any]:
        return {
            "knowledge_graph": self.kg.stats(),
            "graph_store": self.graph_store.get_stats(),
            "vector_store": self.vector_store.get_stats()
        }

