# core/builder/graph_builder.py
from __future__ import annotations

import json
import os
import sqlite3
import pickle
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeoutError, wait, FIRST_COMPLETED
import time
from collections import defaultdict
from copy import deepcopy
from typing import Any, Dict, List, Optional
import asyncio
import random
import re, hashlib
import glob
from tqdm import tqdm
from core.utils.prompt_loader import PromptLoader
from core.utils.format import correct_json_format
from core.models.data import Entity, KnowledgeGraph, Relation, TextChunk, Document
from core.storage.graph_store import GraphStore
from core.utils.config import KAGConfig
from core.utils.neo4j_utils import Neo4jUtils
from core.model_providers.openai_llm import OpenAILLM
from core.agent.knowledge_extraction_agent import InformationExtractionAgent
from core.agent.attribute_extraction_agent import AttributeExtractionAgent
from core.utils.format import DOC_TYPE_META
from plug_in.builder.reflection import DynamicReflector
from collections import defaultdict

def _norm_name(c) -> str:
    if c.scope == "local" and "_" in c.name:
        return c.name.split("_")[0]
    else:
        return c.name
   
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#                               Builder
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class CMPKnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ï¼ˆæ”¯æŒå¤šæ–‡æ¡£æ ¼å¼ï¼‰"""
    def __init__(self, config: KAGConfig):
        self.doc_type = config.knowledge_graph_builder.doc_type
        if self.doc_type not in DOC_TYPE_META:
            raise ValueError(f"Unsupported doc_type: {self.doc_type}")
        
        self.config = config
        self.max_workers = config.knowledge_graph_builder.max_workers
        self.meta = DOC_TYPE_META[self.doc_type]
        self.section_chunk_ids = defaultdict(set)
        self.plug_in_path = "./plug_in"
        prompt_dir = os.path.join(self.plug_in_path, "prompts")
        self.prompt_loader = PromptLoader(prompt_dir)
        self.llm = OpenAILLM(config)

        # å­˜å‚¨ / æ•°æ®åº“
        self.graph_store = GraphStore(config)
        self.neo4j_utils = Neo4jUtils(self.graph_store.driver, doc_type=self.doc_type)
        # åˆå§‹åŒ–è®°å¿†æ¨¡å—
        self.reflector = DynamicReflector(config)
        self.reflector.clear()
        # è¿è¡Œæ•°æ®
        self.kg = KnowledgeGraph()
        self.item_name2object_id: Dict[str, str] = {}
        self.preload_characters_and_objects()


    def preload_characters_and_objects(self):
        characters = self.neo4j_utils.search_entities_by_type("Character", "")
        objects = self.neo4j_utils.search_entities_by_type("Object", "")
        self.character_name2id: Dict[str, str] = {}   # è§’è‰²å/åˆ«å -> ç°æœ‰ Character.id
        self.object_name2id: Dict[str, str] = {}      # ç‰©å“å/åˆ«å -> ç°æœ‰ Object.id
        for chracter in characters:
            if chracter.scope == "local" and "_" in chracter.name:
                character_name = chracter.name.split("_")[0]
                self.character_name2id[chracter.name] = chracter.id
            else:
                character_name = chracter.name
            self.character_name2id[character_name] = chracter.id
        for obj in objects:
            if obj.scope == "local" and "_" in obj.name:
                object_name = obj.name.split("_")[0]
                self.object_name2id[object_name] = obj.id 
            else:
                object_name = obj.name
            self.object_name2id[object_name] = obj.id  

    def clear_directory(self, path):
        for file in glob.glob(os.path.join(path, "*.json")):
            try:
                os.remove(file)
            except Exception as e:
                print(f"åˆ é™¤å¤±è´¥: {file} -> {e}")

    def construct_system_prompt(self, background, abbreviations):
        background_info = self.get_background_info(background, abbreviations)
        system_prompt_id = "agent_prompt"
        system_prompt_text = self.prompt_loader.render_prompt(system_prompt_id, {"background_info": background_info})
        return system_prompt_text
    

    def get_related_content(self, section_name: str) -> str:
        scenes = self.neo4j_utils.search_entities_by_type(self.meta["section_label"], section_name) or []
        if not scenes:
            return ""  # æ‰¾ä¸åˆ°åœºæ™¯å°±ä¸åŠ æç¤ºï¼Œé¿å…å¼‚å¸¸

        scene_id = scenes[0].id
        characters = self.neo4j_utils.search_related_entities(
            scene_id, predicate=self.meta["contains_pred"], entity_types=["Character"]
        )
        objects = self.neo4j_utils.search_related_entities(
            scene_id, predicate=self.meta["contains_pred"], entity_types=["Object"]
        )
        
        character_info = "ã€".join([_norm_name(c) for c in characters])
        object_info = "ã€".join([_norm_name(o) for o in objects])

        parts = []
        if character_info:
            parts.append(f"å½“å‰åœºæ™¯åŒ…å«è§’è‰²æœ‰ï¼š{character_info}ã€‚")
        if object_info:
            parts.append(
                "å½“å‰åœºæ™¯åŒ…å«ç‰©å“æœ‰ï¼š" + object_info +
                "ã€‚åœ¨æŠ½å–æ—¶ä¼˜å…ˆå¯¹ç…§å·²æœ‰ç‰©å“æ¸…å•ï¼Œè‹¥å…¶ä¸­åŒ…å«æœåŒ–é“ç›¸å…³é¡¹åˆ™ç›´æ¥æŠ½å–ï¼›è‹¥æ¸…å•ä¸­æ²¡æœ‰ï¼Œä½†ä¸Šä¸‹æ–‡å¦æœ‰æœåŒ–é“å…ƒç´ ï¼Œåˆ™è¡¥å……æŠ½å–ã€‚"
            )
        return "\n".join(parts)

    
    def get_background_info(self, background, abbreviations):
        bg_block = f"**èƒŒæ™¯è®¾å®š**ï¼š{background}\n" 

        # ---------- 2) ç¼©å†™è¡¨ï¼ˆé”®åå®½å®¹ï¼‰ ----------
        def fmt(item: dict) -> str:
            """
            å°†ä¸€ä¸ªç¼©å†™é¡¹è½¬ä¸º Markdown åˆ—è¡¨æ¡ç›®ã€‚ä»»ä½•å­—æ®µéƒ½å¯é€‰ï¼Œæ ‡é¢˜å­—æ®µä¼˜å…ˆçº§ä¸ºï¼š
            abbr > full > å…¶ä»–å­—æ®µ > N/A
            """
            if not isinstance(item, dict):
                return ""

            # æ ‡é¢˜å­—æ®µä¼˜å…ˆçº§
            abbr = (
                item.get("abbr")
                or item.get("full")
                or next((v for k, v in item.items() if isinstance(v, str) and v.strip()), "N/A")
            )

            # å‰©ä¸‹å­—æ®µå»é™¤æ ‡é¢˜å­—æ®µ
            parts = []
            for k, v in item.items():
                if k in ("abbr", "full"):
                    continue
                if isinstance(v, str) and v.strip():
                    parts.append(v.strip())

            return f"- **{abbr}**: " + " - ".join(parts) if parts else f"- **{abbr}**"

        abbr_block = "\n".join(fmt(item) for item in abbreviations if isinstance(item, dict))

        if background and abbr_block:
            background_info = f"{bg_block}\n{abbr_block}"
        else:
            background_info = bg_block or abbr_block
            
        return background_info
        

            
    def initialize_agents(self):
        
        schema_path = os.path.join(self.plug_in_path, "schema", "graph_schema.json")
        settings_path = os.path.join(self.config.storage.graph_schema_path, "settings.json")
        if os.path.exists(schema_path):
            schema = json.load(open(schema_path, "r", encoding="utf-8"))
        else:
            raise FileNotFoundError("æ²¡æœ‰plug_inçš„graph schema")
        
        self.entity_white_list = set([item["type"] for item in schema.get("entities", [])])
        relations = []
        for k, v in schema.get("relations", {}).items():
            relations.extend(v)
        self.relation_white_list = set([item["type"] for item in relations])
        
        if os.path.exists(settings_path):
            settings = json.load(open(settings_path, "r", encoding="utf-8"))
        else:
            settings = {"background": "", "abbreviations": []}
 
        self.system_prompt_text = self.construct_system_prompt(
            background=settings["background"],
            abbreviations=settings["abbreviations"]
        )
        for entity_type in self.entity_white_list:
            if entity_type != "Character":
                self.neo4j_utils.delete_entity_type(entity_type, exclude_labels=["Object"])
                
        # æŠ½å– agent
        self.information_extraction_agent = InformationExtractionAgent(self.config, self.llm, self.system_prompt_text, schema, self.reflector, prompt_loader=self.prompt_loader)
        self.attribute_extraction_agent = AttributeExtractionAgent(self.config, self.llm, self.system_prompt_text, schema, prompt_loader=self.prompt_loader)
        
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  3) å®ä½“ / å…³ç³» æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def extract_entity_and_relation(self, verbose: bool = True):
        asyncio.run(self.extract_entity_and_relation_async(verbose=verbose))
            
    async def extract_entity_and_relation_async(self, verbose: bool = True):
        """
        å¹¶å‘æŠ½å– â†’ è®°å½•å¤±è´¥çš„ chunk â†’ ç»Ÿä¸€é‡è¯•ä¸€è½® â†’ å†™ç›˜
        """

        desc_chunks = [TextChunk(**o) for o in
                    json.load(open(os.path.join(self.config.storage.knowledge_graph_path, "all_document_chunks.json"), "r", encoding="utf-8"))]

        if verbose:
            print("ğŸ§  å®ä½“ä¸å…³ç³»ä¿¡æ¯å¼‚æ­¥æŠ½å–ä¸­...")

        sem = asyncio.Semaphore(self.max_workers)

        async def _arun_once(ch: TextChunk):
            async with sem:
                try:
                    if not ch.content.strip():
                        result = {"entities": [], "relations": []}
                    else:
                        raw_content = ch.content.strip()
                        section_name = ch.metadata.get("doc_title", "")
                        related_content = self.get_related_content(section_name) 
                        content = raw_content + "\n" + related_content
                        result = await self.information_extraction_agent.arun(
                            content,
                            timeout=self.config.agent.async_timeout,
                            max_attempts=self.config.agent.async_max_attempts,
                            backoff_seconds=self.config.agent.async_backoff_seconds
                        )
                    result.update(chunk_id=ch.id, chunk_metadata=ch.metadata)
                    return result
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] æŠ½å–å¤±è´¥ chunk_id={ch.id} | {e.__class__.__name__}: {e}")
                    return {
                        "chunk_id": ch.id,
                        "chunk_metadata": ch.metadata,
                        "entities": [],
                        "relations": [],
                        "error": f"{e.__class__.__name__}: {e}"
                    }

        async def _arun_with_ch(ch: TextChunk):
            """è¿”å› (chunk, result) æ–¹ä¾¿ç›´æ¥è®°å½•å¤±è´¥çš„ chunkã€‚"""
            res = await _arun_once(ch)
            return ch, res

        # ====== é¦–è½®å¹¶å‘ ======
        tasks = [_arun_with_ch(ch) for ch in desc_chunks]
        first_round_pairs = []   # [(ch, res), ...]
        failed_chs = []          # [ch, ...]

        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="å¼‚æ­¥æŠ½å–ä¸­"):
            ch, res = await coro
            first_round_pairs.append((ch, res))
            if res.get("error"):
                failed_chs.append(ch)

        # ====== ç»Ÿä¸€é‡è¯•ï¼ˆåªä¸€è½®ï¼‰======
        retry_pairs = []
        if failed_chs:
            if verbose:
                print(f"ğŸ”„ å¼€å§‹é‡è¯•å¤±è´¥çš„ {len(failed_chs)} ä¸ªæ–‡æœ¬å—...")
            retry_tasks = [_arun_with_ch(ch) for ch in failed_chs]
            for coro in tqdm(asyncio.as_completed(retry_tasks), total=len(retry_tasks), desc="é‡è¯•æŠ½å–ä¸­"):
                ch, res = await coro
                retry_pairs.append((ch, res))

        # ====== åˆå¹¶ç»“æœï¼ˆç”¨å¤±è´¥çš„ ch è¿‡æ»¤é¦–è½®å¯¹åº”ç»“æœï¼Œå†è¿½åŠ é‡è¯•ç»“æœï¼‰======
        failed_ids = {ch.id for ch in failed_chs}
        final_results = [res for ch, res in first_round_pairs if ch.id not in failed_ids]
        final_results += [res for _, res in retry_pairs]

        # ====== è½ç›˜ ======
        base = os.path.join(self.config.storage.knowledge_graph_path, "plug_in")
        os.makedirs(base, exist_ok=True)
        output_path = os.path.join(base, "extraction_results.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)

        if verbose:
            still_failed = sum(1 for r in final_results if r.get("error"))
            print(f"âœ… å®ä½“ä¸å…³ç³»ä¿¡æ¯æŠ½å–å®Œæˆï¼Œå…±å¤„ç† {len(final_results)} ä¸ªæ–‡æœ¬å—")
            if still_failed:
                print(f"âš ï¸ ä»æœ‰ {still_failed} ä¸ªæ–‡æœ¬å—åœ¨é‡è¯•åå¤±è´¥ï¼ˆä¿ç•™ error å­—æ®µä»¥ä¾¿æ’æŸ¥ï¼‰")
            print(f"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  4) å±æ€§æŠ½å–
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def extract_entity_attributes(self, verbose: bool = True) -> Dict[str, Entity]:
        asyncio.run(self.extract_entity_attributes_async(verbose=verbose))
    
    async def extract_entity_attributes_async(self, verbose: bool = True) -> Dict[str, Entity]:
        """
        âš¡ å¼‚æ­¥æ‰¹é‡å±æ€§æŠ½å–  
        Â· æŒ‰ extract_entity_and_relation_async ç”Ÿæˆçš„ entity_map å»å¹¶å‘  
        Â· æ¯ä¸ªå®ä½“è°ƒç”¨ attribute_extraction_agent.arun()  
        Â· å†…éƒ¨ arun å·²å¸¦è¶…æ—¶ï¼‹é‡è¯•ä¿æŠ¤ï¼Œä¸ä¼šå¡æ­»
        """
        base = os.path.join(self.config.storage.knowledge_graph_path, "plug_in")
        os.makedirs(base, exist_ok=True)
        results = json.load(open(os.path.join(base, "extraction_results.json"), "r", encoding="utf-8"))
        
        # å°†å®ä½“åˆå¹¶ / å»é‡
        entity_map = self.merge_entities_info(results)            # {name: Entity}

        if verbose:
            print(f"ğŸ” å¼€å§‹å±æ€§æŠ½å–ï¼ˆå¼‚æ­¥ï¼‰ï¼Œå®ä½“æ•°ï¼š{len(entity_map)}")

        sem = asyncio.Semaphore(self.max_workers)
        updated_entities: Dict[str, Entity] = {}

        async def _arun_attr(name: str, ent: Entity):
            async with sem:
                try:
                    txt = ent.description or ""
                    if not txt.strip():
                        return name, None

                    res = await self.attribute_extraction_agent.arun(
                        text=txt,
                        entity_name=name,
                        entity_type=ent.type,
                        source_chunks=ent.source_chunks,
                        original_text="",
                        timeout=self.config.agent.async_timeout,
                        max_attempts=self.config.agent.async_max_attempts,
                        backoff_seconds=self.config.agent.async_backoff_seconds
                    )

                    if res.get("error"):  # è¶…æ—¶æˆ–å¼‚å¸¸
                        return name, None

                    attrs = res.get("attributes", {}) or {}
                    if isinstance(attrs, str):
                        try:
                            attrs = json.loads(attrs)
                        except json.JSONDecodeError:
                            attrs = {}

                    new_ent = deepcopy(ent)
                    new_ent.properties = attrs

                    nd = res.get("new_description", "")
                    if nd:
                        new_ent.description = nd

                    return name, new_ent
                except Exception as e:
                    if verbose:
                        print(f"[ERROR] å±æ€§æŠ½å–å¤±è´¥ï¼ˆå¼‚æ­¥ï¼‰ï¼š{name}: {e}")
                    return name, None

        # å¹¶å‘æ‰§è¡Œ
        tasks = []
        for n, e in entity_map.items():
            # åªå¤„ç† CMP ç™½åå•é‡Œçš„ç±»å‹ï¼Œä¸”è·³è¿‡ Character
            if e.type[0] not in self.entity_white_list or e.type[0] == "Character":
                continue
            tasks.append(_arun_attr(n, e))

        for coro in tqdm(asyncio.as_completed(tasks),
                            total=len(tasks),
                            desc="å±æ€§æŠ½å–ä¸­ï¼ˆasyncï¼‰"):
            n, e2 = await coro
            if e2:
                updated_entities[n] = e2
                target_id = None
                if n in self.item_name2object_id:
                    # â€”â€” å·²å­˜åœ¨çš„ Object â€”â€” å¢åŠ  CMP æ ‡ç­¾
                    target_id = self.item_name2object_id[n]
                    try:
                        self.neo4j_utils.add_labels(target_id, self._to_type_list(e2.type))
                    except Exception as ex:
                        if verbose:
                            print(f"[WARN] Neo4j æ·»åŠ æ ‡ç­¾å¤±è´¥: {n} ({target_id}): {ex}")
                else:
                    # â€”â€” çº¯ CMP èŠ‚ç‚¹ â€”â€” ç›´æ¥ç”¨è‡ªèº« ID
                    target_id = e2.id

                if target_id and e2.properties:
                    try:
                        self.neo4j_utils.update_entity_properties(target_id, e2.properties)
                    except Exception as ex:
                        if verbose:
                            print(f"[WARN] Neo4j å±æ€§å†™å›å¤±è´¥: {n} ({target_id}): {ex}")


        # å†™æ–‡ä»¶
        output_path = os.path.join(base, "entity_info.json")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({k: v.dict() for k, v in updated_entities.items()},
                    f, ensure_ascii=False, indent=2)

        if verbose:
            print(f"âœ… å±æ€§æŠ½å–å®Œæˆï¼Œå…±å¤„ç†å®ä½“ {len(updated_entities)} ä¸ª")
            print(f"ğŸ’¾ å·²ä¿å­˜è‡³ï¼š{output_path}")

        return updated_entities


    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  5) æ„å»ºå¹¶å­˜å‚¨å›¾è°±
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    @staticmethod
    def _to_type_list(t) -> List[str]:
        """
        å°†ä¼ å…¥çš„ t ç»Ÿä¸€è§„èŒƒä¸º List[str]ï¼š
        - None         -> []
        - 'Styling'    -> ['Styling']
        - ['A','B',''] -> ['A','B']
        - å…¶å®ƒç±»å‹     -> []
        """
        # print("[CHECK] t", t)
        if t is None:
            return []
        if isinstance(t, list):
            return [x for x in t if isinstance(x, str) and x.strip()]
        if isinstance(t, str):
            s = t.strip()
            return [s] if s else []
        return []

    
    def _ensure_entity_exists(
            self,
            entity_payload: Any,
            ent_by_name: Dict[str, "Entity"],
            ent_by_id: Dict[str, "Entity"],
        ) -> Optional[str]:
            """
            ç¡®ä¿å®ä½“åœ¨ self.kg.entities é‡Œå­˜åœ¨ï¼š
            - å‘½ä¸­ KG/ç¼“å­˜ï¼šåˆå¹¶ properties
            - å‘½ä¸­ ent_by_id / ent_by_nameï¼šåŠ å…¥ KG
            - å¦åˆ™åˆ›å»ºæœ€å°å®ä½“ï¼ˆç¨³å®š ent_cmp_*ï¼‰ï¼Œä¿ç•™å¤šç±»å‹ï¼ˆList[str]ï¼‰
            è¿”å›å®ä½“ id æˆ– None
            """
            ep = self._normalize_entity_payload(entity_payload, ent_by_name, ent_by_id)
            if not ep:
                return None

            props_in = self._to_props_dict(ep.get("properties", {}))
            e_name = ep.get("name")
            e_id   = ep.get("id")
            e_type = self._to_type_list(ep.get("type"))

            # 1) KG å†…å·²æœ‰
            if e_id and e_id in self.kg.entities:
                old = self.kg.entities[e_id]
                old.properties = {**self._to_props_dict(getattr(old, "properties", {})), **props_in}
                return e_id

            # 2) ent_by_id ä¸­å·²æœ‰
            if e_id and e_id in ent_by_id:
                e = ent_by_id[e_id]
                e.properties = {**self._to_props_dict(getattr(e, "properties", {})), **props_in}
                if e.id not in self.kg.entities:
                    self.kg.add_entity(e)
                return e.id

            # 3) é€šè¿‡ name å‘½ä¸­
            if not e_id and e_name and e_name in ent_by_name:
                e = ent_by_name[e_name]
                e.properties = {**self._to_props_dict(getattr(e, "properties", {})), **props_in}
                if e.id not in self.kg.entities:
                    self.kg.add_entity(e)
                return e.id

            # 4) å…¨æ–°æœ€å°å®ä½“ï¼ˆç¨³å®š CMP idï¼‰
            if not e_id:
                base = f"{e_name or ''}|{','.join(e_type) or ''}"
                e_id = "ent_cmp_" + hashlib.md5(base.encode("utf-8")).hexdigest()[:12]

            payload = {
                "id": e_id,
                "name": e_name or e_id,
                "type": e_type or ["Concept"],
                "description": "",
                "aliases": ep.get("aliases", []),
                "properties": props_in,
                "source_chunks": [],
            }
            try:
                new_e = Entity(**payload)
            except TypeError:
                minimal = {k: payload[k] for k in ("id", "name", "type", "properties") if k in payload}
                new_e = Entity(**minimal)

            if new_e.id not in self.kg.entities:
                self.kg.add_entity(new_e)
            return new_e.id
    

    def _normalize_entity_payload(
            self,
            raw: Any,
            ent_by_name: Dict[str, "Entity"],
            ent_by_id: Dict[str, "Entity"],
        ) -> Optional[Dict[str, Any]]:
            """
            æŠŠ subject/object çš„ä»»æ„å½¢æ€ï¼ˆdict / id å­—ç¬¦ä¸² / name å­—ç¬¦ä¸²ï¼‰ç»Ÿä¸€æˆè§„èŒƒ payloadï¼š
            è¿”å›å­é›†ï¼š{"id": str|None, "name": str|None, "type": List[str], "properties": dict, "aliases": list}
            ä¸ä¿®æ”¹ç« èŠ‚é€»è¾‘ã€‚
            """
            if not raw:
                return None

            def _from_entity(e: "Entity") -> Dict[str, Any]:
                return {
                    "id": e.id,
                    "name": e.name,
                    "type": self._to_type_list(getattr(e, "type", [])),
                    "properties": self._to_props_dict(getattr(e, "properties", {})),
                    "aliases": list(getattr(e, "aliases", []) or []),
                }

            # dictï¼šä¼˜å…ˆ idï¼Œå…¶æ¬¡ nameï¼›å¦åˆ™æŒ‰å­—æ®µåŸæ ·è§„æ•´
            if isinstance(raw, dict):
                rid = (raw.get("id") or "").strip()
                rname = (raw.get("name") or "").strip()
                if rid and rid in ent_by_id:
                    return _from_entity(ent_by_id[rid])
                if rname:
                    if rname in ent_by_name:
                        return _from_entity(ent_by_name[rname])
                    if rname in self.character_name2id:
                        return {"id": self.character_name2id[rname], "name": rname, "type": ["Character"], "properties": {}, "aliases": []}
                    if rname in self.object_name2id:
                        return {"id": self.object_name2id[rname], "name": rname, "type": ["Object"], "properties": {}, "aliases": []}
                return {
                    "id": rid or None,
                    "name": rname or None,
                    "type": self._to_type_list(raw.get("type")),
                    "properties": self._to_props_dict(raw.get("properties", {})),
                    "aliases": list(raw.get("aliases", []) or []),
                }

            # å­—ç¬¦ä¸²ï¼šå¯èƒ½æ˜¯ id æˆ– name
            if isinstance(raw, str):
                s = raw.strip()
                if not s:
                    return None
                # å…ˆæŒ‰ id å‘½ä¸­
                if s in ent_by_id:
                    return _from_entity(ent_by_id[s])
                # å†æŒ‰ name å‘½ä¸­
                if s in ent_by_name:
                    return _from_entity(ent_by_name[s])
                # å…œåº•ï¼šå·²æœ‰è§’è‰²/ç‰©å“æ˜ å°„
                if s in self.character_name2id:
                    return {"id": self.character_name2id[s], "name": s, "type": ["Character"], "properties": {}, "aliases": []}
                if s in self.object_name2id:
                    return {"id": self.object_name2id[s], "name": s, "type": ["Object"], "properties": {}, "aliases": []}
                # åƒ id çš„æ¨¡å¼
                if s.startswith("ent_") or re.match(r"^[A-Za-z]{2,5}_[A-Za-z0-9\-]+$", s):
                    return {"id": s, "name": None, "type": [], "properties": {}, "aliases": []}
                # ä½œä¸ºåç§°è¿”å›
                return {"id": None, "name": s, "type": [], "properties": {}, "aliases": []}

            return None
    
    def build_graph_from_results(self, verbose: bool = True) -> KnowledgeGraph:
        """
        ä» plug_in/extraction_results.json ä¸ entity_info.json æ„å»º KGï¼Œå¹¶å†™å…¥ Neo4jï¼š
        1) è§„èŒƒåŒ–/åŠ è½½å®ä½“ï¼ˆå…ˆè½½å…¥ Neo4j æ—¢æœ‰ Character/Object é˜²æ­¢å…³ç³»è§¦å‘ç©ºèŠ‚ç‚¹ï¼‰
        2) æ‰«ææŠ½å–ç»“æœè¡¥å®ä½“ã€åˆ›å»º Section èŠ‚ç‚¹å¹¶å»ºç«‹åŒ…å«å…³ç³»
        3) ç”Ÿæˆå¹¶å†™å…¥å…³ç³»
        4) æœ€åç»Ÿä¸€å†™åº“ä¸åå¤„ç†ï¼ˆä¸Šä¸‹æ–‡å¯Œé›†ä¸ä¸­å¿ƒæ€§è®¡ç®—ï¼‰
        """
        # ä¸ºæœ¬å‡½æ•°å–åˆ«åï¼Œä¾¿äºå±€éƒ¨è°ƒç”¨
        _to_props_dict = self._to_props_dict

        if verbose:
            print("ğŸ“‚ åŠ è½½å·²æœ‰æŠ½å–ç»“æœå’Œå®ä½“ä¿¡æ¯...")

        base = os.path.join(self.config.storage.knowledge_graph_path, "plug_in")
        os.makedirs(base, exist_ok=True)

        # ---- è¯»å–æŠ½å–/å±æ€§/åˆ†ç»„æ–‡ä»¶ ----
        results_path = os.path.join(base, "extraction_results.json")
        entinfo_path = os.path.join(base, "entity_info.json")
        secs_pkl_path = os.path.join(base, "section_entities_collection.pkl")

        # å¿…è¦æ–‡ä»¶æ£€æŸ¥
        if not os.path.exists(results_path):
            raise FileNotFoundError(f"ç¼ºå°‘æŠ½å–ç»“æœæ–‡ä»¶ï¼š{results_path}")
        if not os.path.exists(entinfo_path):
            # æ²¡åšå±æ€§æŠ½å–ä¹Ÿå…è®¸ç»§ç»­ï¼ˆç»™ä¸ªç©ºï¼‰
            if verbose:
                print(f"âš ï¸ æœªå‘ç°å±æ€§ä¿¡æ¯æ–‡ä»¶ï¼š{entinfo_path}ï¼Œå°†ä»…ä¾æ®æŠ½å–ç»“æœæ„å»ºå›¾ã€‚")
            ent_raw = {}
        else:
            ent_raw = json.load(open(entinfo_path, "r", encoding="utf-8"))

        results = json.load(open(results_path, "r", encoding="utf-8"))

        # section_entities_collection å¦‚å­˜åœ¨å¯å¤ç”¨ï¼ˆä¸å¼ºä¾èµ–ï¼‰
        if os.path.exists(secs_pkl_path):
            with open(secs_pkl_path, "rb") as f:
                self.section_entities_collection = pickle.load(f)
        else:
            self.section_entities_collection = {}

        # ---- 1) è§„èŒƒåŒ– entity_info.json â†’ Entity æ˜ å°„ ----
        ent_by_name: Dict[str, Entity] = {}
        for name, payload in ent_raw.items():
            payload = dict(payload)
            payload["properties"] = _to_props_dict(payload.get("properties", {}))
            # print("[CHECK] payload: ", payload)
            try:
                ent_by_name[name] = Entity(**payload)
            except TypeError:
                # æœ€å°åŒ–å›é€€ï¼ˆé˜²å¾¡ï¼‰
                minimal = {
                    "id": payload.get("id") or f"ent_cmp_{hashlib.md5(name.encode('utf-8')).hexdigest()[:12]}",
                    "name": name,
                    "type": payload.get("type") or "Concept",
                    "properties": payload.get("properties", {}),
                }
                ent_by_name[name] = Entity(**minimal)

        ent_by_id: Dict[str, Entity] = {e.id: e for e in ent_by_name.values()}

        # ---- 2) é¢„åŠ è½½ Neo4j ä¸­å·²æœ‰è§’è‰²/ç‰©å“ï¼Œå…ˆå…¥ KGï¼Œé¿å…åç»­å…³ç³» MERGE å‡ºâ€œç©ºèŠ‚ç‚¹â€ ----
        def _safe_add(e: Entity):
            if not e:
                return
            e.properties = _to_props_dict(getattr(e, "properties", {}))
            # print("[CHECK] é¢„åŠ è½½å®ä½“: ", e.properties)
            if e.id not in self.kg.entities:
                self.kg.add_entity(e)

        pre_chars = self.neo4j_utils.search_entities_by_type("Character", "")
        pre_objs = self.neo4j_utils.search_entities_by_type("Object", "")

        for e in pre_chars or []:
            _safe_add(e)
        for e in pre_objs or []:
            _safe_add(e)

        # ---- 3) æŠŠ ent_raw çš„å®ä½“ä¹Ÿå†™å…¥ KGï¼ˆä»¥ id å»é‡ï¼‰----
        # ---- 3) æŠŠ ent_raw çš„å®ä½“ä¹Ÿå†™å…¥ KGï¼ˆä»¥ id å»é‡ï¼‰ï¼Œå¹¶åˆå¹¶å±æ€§ ----
        for e in ent_by_name.values():
            if e.id in self.kg.entities:
                exist = self.kg.entities[e.id]
                # åˆå¹¶å±æ€§ï¼ˆentity_info ä¼˜å…ˆï¼‰
                exist.properties = {**(getattr(exist, "properties", {}) or {}), **(getattr(e, "properties", {}) or {})}

                print("[CHECK] åˆå¹¶å®ä½“: ", exist.properties)

                # åˆå¹¶åˆ«å/æè¿°/æ¥æºï¼ˆæŒ‰éœ€ï¼‰
                for a in getattr(e, "aliases", []) or []:
                    if a and a not in getattr(exist, "aliases", []):
                        exist.aliases.append(a)
                if getattr(e, "description", "") and getattr(e, "description", "") not in (getattr(exist, "description", "") or ""):
                    exist.description = (exist.description + "\n" if exist.description else "") + e.description
                # ä¸é‡æ–° add_entity
            else:
                self.kg.add_entity(e)


        if verbose:
            print("ğŸ”— æ„å»ºçŸ¥è¯†å›¾è°±...")

        self.section_names = []


        # ==== å…ˆæ‰«ä¸€éï¼Œè¡¥â€œåªåœ¨å…³ç³»é‡Œå‡ºç°â€çš„å®ä½“ ====
        for res in results:
            for ed in res.get("entities", []) or []:
                self._ensure_entity_exists(ed, ent_by_name, ent_by_id)

        # ==== Section èŠ‚ç‚¹ä¸åŒ…å«å…³ç³» + æ™®é€šå…³ç³» ====
        for res in results:
            md = res.get("chunk_metadata", {}) or {}

            # å…ˆåˆ›å»º/åˆå¹¶ Section èŠ‚ç‚¹
            secs = self._create_section_entities(md, res["chunk_id"])
            for se in secs:
                if se.name not in self.section_names and se.id not in self.kg.entities:
                    self.kg.add_entity(se)
                    self.section_names.append(se.name)
                else:
                    # åˆå¹¶ source_chunks
                    exist = self.kg.entities.get(se.id)
                    if exist:
                        merged = list(dict.fromkeys(list(getattr(exist, "source_chunks", [])) + list(getattr(se, "source_chunks", []))))
                        exist.source_chunks = merged

            # æŠŠå½“å‰ section ä¸è¯¥ section å·²æ”¶é›†çš„å®ä½“é“¾æ¥
            for se in secs:
                inner = self.section_entities_collection.get(se.name, [])
                self._link_section_to_entities(se, inner, res["chunk_id"])

            # å†å¤„ç†æ™®é€šå…³ç³»
            for rdata in res.get("relations", []) or []:
                # å…œåº•ä¿è¯ç«¯ç‚¹å­˜åœ¨
                self._ensure_entity_exists(rdata.get("subject"), ent_by_name, ent_by_id)
                self._ensure_entity_exists(rdata.get("object"), ent_by_name, ent_by_id)

                rel = self._create_relation_from_data(rdata, res["chunk_id"], ent_by_name)
                if rel:
                    self.kg.add_relation(rel)

        # ---- 5) å†™åº“ï¼ˆå…ˆå®ä½“åå…³ç³»ï¼‰å¹¶åå¤„ç† ----
        if verbose:
            print("ğŸ’¾ å­˜å‚¨åˆ°æ•°æ®åº“...")

        # 5.1 å®ä½“ MERGEï¼ˆå«å±æ€§ï¼‰
        for e in self.kg.entities.values():
            props = _to_props_dict(getattr(e, "properties", {}))
            try:
                self.neo4j_utils.merge_entity_with_properties(
                    node_id=e.id,
                    name=e.name,
                    etypes=e.type,
                    aliases=getattr(e, "aliases", []),
                    props=props,                 # ä¿æŒåŸæ ·ä¼ å…¥ï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
                    store_mode="both"
                )
            except Exception as ex:
                if verbose:
                    print(f"[WARN] å†™å…¥å®ä½“å¤±è´¥ï¼š{e.id} / {e.name} -> {ex}")

        # 5.2 å…³ç³»å†™å…¥
        self._store_knowledge_graph(verbose)

        # 5.3 å…¶å®ƒåå¤„ç†
        try:
            self.neo4j_utils.enrich_event_nodes_with_context()
        except Exception as ex:
            if verbose:
                print(f"[WARN] enrich_event_nodes_with_context å¤±è´¥ï¼š{ex}")

        try:
            self.neo4j_utils.compute_centrality(exclude_rel_types=[self.meta['contains_pred']])
        except Exception as ex:
            if verbose:
                print(f"[WARN] compute_centrality å¤±è´¥ï¼š{ex}")

        if verbose:
            st = self.kg.stats()
            print("ğŸ‰ é“å…·å›¾è°±æ„å»ºå®Œæˆ!")
            try:
                graph_stats = self.graph_store.get_stats()
                print(f" - å®ä½“æ•°é‡: {graph_stats.get('entities')}")
                print(f" - å…³ç³»æ•°é‡: {graph_stats.get('relations')}")
            except Exception:
                pass
            print(f" - æ–‡æ¡£æ•°é‡: {st['documents']}")
            print(f" - æ–‡æœ¬å—æ•°é‡: {st['chunks']}")

        return self.kg

    @staticmethod
    def _to_props_dict(props):
        if props is None:
            return {}
        if isinstance(props, dict):
            return props
        if isinstance(props, str):
            s = props.strip()
            if not s:
                return {}
            try:
                return json.loads(s)
            except Exception:
                return {}
        return {}

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  å†…éƒ¨å·¥å…·
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # -------- åˆå¹¶å®ä½“ï¼ˆæ ¹æ® doc_type é€‚é…ï¼‰ --------
    def merge_entities_info(self, extraction_results):
        """
        éå†ä¿¡æ¯æŠ½å–ç»“æœï¼Œåˆå¹¶ / å»é‡å®ä½“ã€‚
        - ä»…ä¿ç•™ç™½åå•å†…çš„å®ä½“ç±»å‹ã€‚
        - Character ä¸æ–°å»ºèŠ‚ç‚¹ï¼Œåªå°è¯•æ˜ å°„åˆ°å·²æœ‰ IDã€‚
        - Object / WardrobeItem / PropItem ä¼˜å…ˆæ˜ å°„åˆ°å·²æœ‰ Objectï¼Œå¦åˆ™æ–°å»ºã€‚
        - å…¶å®ƒç™½åå•ç±»å‹æ­£å¸¸æ–°å»º/åˆå¹¶ã€‚
        """
        entity_map: Dict[str, Entity] = {}
        self.chunk2section_map = {r["chunk_id"]: r["chunk_metadata"]["doc_title"] for r in extraction_results}
        self.section_entities_collection = {}

        base = os.path.join(self.config.storage.knowledge_graph_path, "plug_in")
        os.makedirs(base, exist_ok=True)

        for result in extraction_results:
            md = result.get("chunk_metadata", {}) or {}
            label = md.get("doc_title", md.get("subtitle", md.get("title", "")))
            if label not in self.section_entities_collection:
                self.section_entities_collection[label] = []

            # éå†å½“å‰ chunk çš„å®ä½“
            for ent_data in result.get("entities", []):
                t = ent_data.get("type", "")
                name = ent_data.get("name")
                if not name or t not in self.entity_white_list:
                    continue

                # ---- è§’è‰²å¤„ç†ï¼šä¸å»ºèŠ‚ç‚¹ï¼Œåªæ˜ å°„ ----
                if t == "Character":
                    char_id = self.character_name2id.get(name)
                    if not char_id:
                        # å¦‚æœéœ€è¦å¯ä»¥æ‰“å° warn
                        # print(f"[WARN] Character {name} æœªåœ¨Neo4jæ‰¾åˆ°ï¼Œè·³è¿‡åˆ›å»º")
                        pass
                    continue

                # ---- å†²çªå¤„ç†ï¼šlocalå®ä½“æˆ– action-like å®ä½“é‡å‘½å ----
                is_action_like = t in {"Action", "Emotion", "Goal"}
                if (ent_data.get("scope", "").lower() == "local" or is_action_like) and name in entity_map:
                    existing_entity = entity_map[name]
                    existing_chunk_id = existing_entity.source_chunks[0]
                    existing_section_name = self.chunk2section_map[existing_chunk_id]
                    current_section_name = md.get("doc_title", "")
                    if current_section_name != existing_section_name:
                        new_name = f"{name}_in_{label}"
                        suffix = 1
                        while new_name in entity_map:
                            suffix += 1
                            new_name = f"{name}_in_{label}_{suffix}"
                        ent_data["name"] = new_name
                        name = new_name

                # ---- ç‰©å“å¤„ç†ï¼šå°è¯•å¯¹é½å·²æœ‰ Object ----
                if t in self.entity_white_list and t != "Character":
                    obj_id = self.object_name2id.get(name)
                    if obj_id:
                        self.item_name2object_id[name] = obj_id
                        continue  # å·²å¯¹é½ï¼Œä¸éœ€è¦æ–°å»ºèŠ‚ç‚¹

                # ---- åˆ›å»º / åˆå¹¶ ----
                ent_obj = self._create_entity_from_data(ent_data, result["chunk_id"])
                # print("[CHECK] åˆ›å»ºå®ä½“: ", ent_obj)
                if not ent_obj:
                    continue
                existing = self._find_existing_entity(ent_obj, entity_map)
                if existing:
                    self._merge_entities(existing, ent_obj)
                else:
                    entity_map[ent_obj.name] = ent_obj

                self.section_entities_collection[label].append(ent_obj)

        # å­˜ä¸€ä»½ section_entities_collection
        output_path = os.path.join(base, "section_entities_collection.pkl")
        with open(output_path, "wb") as f:
            pickle.dump(self.section_entities_collection, f)

        return entity_map


    
    def _find_existing_entity(self, entity: Entity, entity_map: Dict[str, Entity]) -> Optional[Entity]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„å®ä½“"""
        if (entity.type == "Event") or (isinstance(entity.type, list) and "Event" in entity.type):
            return None
        if entity.name in entity_map:
            return entity_map[entity.name]
        for existing_entity in entity_map.values():
            if entity.name in existing_entity.aliases:
                return existing_entity
            if any(alias in existing_entity.aliases for alias in entity.aliases):
                return existing_entity
        return None
    
    def _merge_types(self, a, b) -> List[str]:
        """
        æŠŠ a å’Œ b çš„ç±»å‹å¹¶é›†ååšè§„èŒƒåŒ–ï¼ˆEvent ä¼˜å…ˆ + å»é‡ä¿åºï¼‰
        å§‹ç»ˆè¿”å› List[str]ï¼›è‹¥ä¸ºç©ºåˆ™è¿”å› ['Concept']ã€‚
        """
        a_list = self._to_type_list(a)
        b_list = self._to_type_list(b)

        merged, seen = [], set()
        for x in a_list + b_list:
            if x and x not in seen:
                seen.add(x)
                merged.append(x)

        if "Event" in seen:
            merged = ["Event"] + [x for x in merged if x != "Event"]

        return merged or ["Concept"]



    def _merge_entities(self, existing: Entity, new: Entity) -> None:
        """åˆå¹¶å®ä½“ä¿¡æ¯"""
        for alias in new.aliases:
            if alias not in existing.aliases:
                existing.aliases.append(alias)
        existing.properties.update(new.properties)
        for chunk_id in new.source_chunks:
            if chunk_id not in existing.source_chunks:
                existing.source_chunks.append(chunk_id)
        if new.description:
            if not existing.description:
                existing.description = new.description
            elif new.description not in existing.description:
                existing.description = existing.description + "\n" + new.description
                
        existing.type = self._merge_types(existing.type, new.type)
        

    # -------- Section / Contains --------
    def _create_section_entities(self, md: Dict[str, Any], chunk_id: str) -> List[Entity]:
        """
        åˆ›å»ºç« èŠ‚/åœºæ™¯å®ä½“ã€‚
        - title/subtitle æ€»æ˜¯ä» "title"/"subtitle" å­—æ®µè¯»å–
        - Entity.properties å†™å…¥æ˜ å°„å­—æ®µï¼ˆå¦‚ scene_nameï¼‰ + å…¶ä»–æœ‰ç”¨ metadata å­—æ®µ
        """
        raw_title = md.get("title", "").strip()
        raw_subtitle = md.get("subtitle", "").strip()
        order = md.get("order", None)

        if not raw_title:
            return []

        label = self.meta["section_label"]
        full_name = md.get("doc_title", f"{label}{raw_title}-{raw_subtitle}" if raw_subtitle else f"{label}{raw_title}")
        eid = f"{label.lower()}_{order}" if order is not None else f"{label.lower()}_{hash(full_name) % 1_000_000}"

        title_field = self.meta["title"]
        subtitle_field = self.meta["subtitle"]

        # æ„å»º propertiesï¼šå†™å…¥ title/subtitle æ˜ å°„å­—æ®µ + å…¶ä»–æœ‰æ•ˆå­—æ®µ
        excluded = {"chunk_index", "chunk_type", "doc_title", "title", "subtitle", "total_description_chunks", "total_doc_chunks"}
        properties = {
            title_field: raw_title,
            subtitle_field: raw_subtitle,
        }
        
        if order is not None:
            properties["order"] = order

        for k, v in md.items():
            if k not in excluded:
                properties[k] = v

        self.section_chunk_ids[eid].add(chunk_id)
        agg_chunks = sorted(self.section_chunk_ids[eid])

        return [
            Entity(
                id=eid,
                name=full_name,
                type=label,
                scope="local",  
                description=md.get("summary", ""),  # å¯é€‰ï¼šç”¨ summary ä½œä¸ºç®€è¦æè¿°
                properties=properties,
                source_chunks=agg_chunks 
            )
        ]


    def _link_section_to_entities(self, section: Entity, inners: List[Entity], chunk_id: str):
        pred = self.meta["contains_pred"]
        for tgt in inners:
            rid = f"rel_{hash(f'{section.id}_{pred}_{tgt.id}') % 1_000_000}"
            self.kg.add_relation(
                Relation(id=rid, subject_id=section.id, predicate=pred,
                         object_id=tgt.id, properties={}, source_chunks=[chunk_id])
            )

    # -------- Entity / Relation creation --------
    
    def _create_entity_from_data(self, data: Dict, chunk_id: str) -> Optional[Entity]:
        """
        - å…è®¸çš„ç±»å‹ï¼šself.entity_white_listï¼ˆå·²ä» schema æ³¨å…¥ï¼‰
        - è‹¥æ‰€æœ‰ç±»å‹éƒ½ä¸åœ¨ç™½åå•ï¼šè¿”å› None
        - **ä¿ç•™å¤šç±»å‹ï¼ˆList[str]ï¼‰**
        """
        name = (data.get("name") or "").strip()
        if not name:
            return None

        types_raw = data.get("type")
        types_all = self._to_type_list(types_raw)
        types_in  = [x for x in types_all if x in self.entity_white_list]
        if not types_in:
            return None

        return Entity(
            id=f"ent_{hash(name) % 1_000_000}",   # ä½ åŸæ¥çš„ç¨³å®š id æ–¹æ¡ˆï¼Œä¿ç•™
            name=name,
            type=types_in,                         # â† ä¿ç•™å¤šç±»å‹
            scope=data.get("scope", "local"),
            description=data.get("description", ""),
            aliases=data.get("aliases", []),
            source_chunks=[chunk_id],
            properties=self._to_props_dict(data.get("properties", {})),  # è‹¥æŠ½å–é‡Œå·²å¸¦å±æ€§ä¹Ÿä¸ä¸¢
        )




    def _create_relation_from_data(
        self,
        d: Dict,
        chunk_id: str,
        ent_by_name: Dict[str, Entity]
    ) -> Optional[Relation]:
        # å…¼å®¹å¤šç§é”®å
        subj = d.get("subject") or d.get("source") or d.get("head") or d.get("from_entity")
        obj  = d.get("object")  or d.get("target") or d.get("tail") or d.get("to_entity")
        pred = d.get("predicate") or d.get("relation") or d.get("relation_type") or d.get("type")

        if not subj or not obj or not pred:
            return None
        if pred not in self.relation_white_list:
            return None

        # æº/ç›®æ ‡IDè§£æé¡ºåºï¼šæŠ½å–æ‰¹å†…æ–°å®ä½“ > æ—¢æœ‰è§’è‰² > å·²å¯¹é½çš„Object > æ—¢æœ‰Object
        def _resolve_id(name: str) -> Optional[str]:
            if name in ent_by_name:
                return ent_by_name[name].id
            if name in self.character_name2id:
                return self.character_name2id[name]
            if name in self.item_name2object_id:
                return self.item_name2object_id[name]
            if name in self.object_name2id:
                return self.object_name2id[name]
            return None

        sid = _resolve_id(subj)
        oid = _resolve_id(obj)
        if not sid or not oid:
            return None

        rid = f"rel_{hash(f'{sid}_{pred}_{oid}') % 1_000_000}"

        return Relation(
            id=rid,
            subject_id=sid,
            predicate=pred,
            object_id=oid,
            properties={
                "description": d.get("description", ""),
                "relation_name": d.get("relation_name", "")
            },
            source_chunks=[chunk_id],     # â† å›ºå®šç”¨å½“å‰ chunk_id
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  Embedding & Stats
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def prepare_graph_embeddings(self):
        self.neo4j_utils.load_embedding_model(self.config.graph_embedding)
        self.neo4j_utils.create_vector_index()
        self.neo4j_utils.process_all_embeddings(
            # exclude_entity_types=[self.meta["section_label"]]
            # exclude_relation_types=[self.meta["contains_pred"]],
        )
        self.neo4j_utils.ensure_entity_superlabel()
        print("âœ… å›¾å‘é‡æ„å»ºå®Œæˆ")

    def _store_knowledge_graph(self, verbose: bool):
        try:
            # self.graph_store.reset_knowledge_graph()
            self.graph_store.store_knowledge_graph(self.kg)
        except Exception as e:
            if verbose:
                print(f"âš ï¸ å­˜å‚¨å¤±è´¥: {e}")

